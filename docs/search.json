[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Pankaj Chejara is a junior researcher cum web developer at the Center of Education Technology at Tallinn University, Estonia. His work is primarily focused on building AI tools to support teachers in the classroom with group work monitoring. He enjoys diving into data to extract insights and is always ready to learn new things to solve the problem at hand. When not working on data analytics projects, Pankaj enjoys spending time with his son and watching animated movies.\nResume"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nMaster in Computer Engg. (MTech) | Aug 2010 - June 2012  Malviya National Institute of Technology | Jaipur, India\nMaster in Computer Applications (MCA) | Aug 2007 - May 2010  Malviya National Institute of Technology | Jaipur, India\nBachelor in Science (Mathematics) | Aug 2004 - May 2007  University of Rajasthan | Jaipur, India"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nData Researcher | Tallinn University, Estonia | Sep 2019 - present\n\nData cleaning, Data Pre-processing\nBuilding interactive data visualization Applications\nBuilding machine learning models for detecting collaboration levels"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pankaj Chejara",
    "section": "",
    "text": "About this blog\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/data_r.html",
    "href": "posts/post-with-code/data_r.html",
    "title": "R tutorial on accessing, filtering, aggregating and plotting data",
    "section": "",
    "text": "This post offers a tutorial on how to access, filter, aggregate and plot the data in R. It will mainly covers following topics\n\nHow to access a particular column or row from a dataframe?\nHow to access rows with conditions?\nHow to generate group wise statistics (e.g., salary statistics for male and female groups)?\nHow to create new column in a dataframe?\nHow to plot distribution of data?\nHow to plot group-wise stats?\n\n\n\nThis tutorial requires you to have R-Studio installed on your system. In case if you don’t have R-Studio on your system, you can access it online by going to rstudio-cloud.\n\n\nGo to your R-Studio and execute following command\ninstall.packages('rio')\ninstall.packages('dplyr')\ninstall.packages('lattice')\nThe above commands will install three pacakages (rio)[https://cran.r-project.org/web/packages/rio/vignettes/rio.html], (dplyr)[https://www.rdocumentation.org/packages/dplyr/versions/0.7.8], and (ggplot2)[https://ggplot2.tidyverse.org/]. We will use these packages in our tutorials.\n\n\n\n\nIn our first tutorial, we will learn how to load dataset into R-studio. For this tutorial, we will use a very simple dataset available here. This dataset contains four attributes (name, age, salary, expenses) (given below).\n\n\n\nname\nage\nsalary\nexpenses\n\n\n\n\nmac\n21\n15000\n600\n\n\nravi\n25\n18000\n800\n\n\ndavid\n27\n17000\n600\n\n\nmoorey\n43\n33000\n1200\n\n\nnolan\n33\n24000\n900\n\n\n\nWe can either use R-studio GUI or we can write code to load the dataset. So let’s say you have downloaded the dataset and saved it. You can write following codes to load the dataset into R.\n# load the package\nlibrary(rio)\n\n# open dataset file\n# Specify your dataset filename with complete path\ndata &lt;– import('filename')\nFollowing is the demonstration of both ways for loading the dataset.\n\n\n\n\n\nSo we have loaded the dataset into R and now we want to access two attributes name and salary using dplyr package.\nWe will use following syntax\nselect(data_object,attr_name,attr_name,..)\nOR\ndata_object %&gt;% select(attr_name,attr_name,..)\nHere, data_object is the object name you have used to load your dataset. You then specify the attributes name which you want to access.\nIn our case, we want to access name and salary. Therefore, we will specify these attributes in select().\nlibrary(dplyr)\nlibrary(rio)\n# load dataset\ndataset &lt;- import('sample_data.csv')\n\n# access name and salary attributes\ndataset %&gt;% select(name,salary)\nFollowing is our code from R-Studio.\n\n\n\nScreenshot 2021-02-26 at 11.23.52\n\n\nSimilarly you can specify other attributes name which you want to access from your data. You can refer this link to get more information on usage of select().\nNow, we will look at rows selection. Let’s say we want to fetch data of peple whose salary is higher than 20000.\nWe will use following syntax\nfilter(data_object,condition)\nOR\ndata_object %&gt;% filter(condition)\nTo fetch our data, we will use the condition salary &gt; 20000. The snapshow below shows the result.\n\n\n\nScreenshot 2021-02-26 at 11.36.23\n\n\nYou can refer here for more information on filter() function. Following are some other examples\n\nAccessing the last row\nslice_tail(data_object)\nAccessing the first row\nslice_head(data_object)\nAccessing row with minimum value of an attribute (e.g., who has the minimum salary)\nslice_min(data_object, attr_name)\nAccessing row with maximum value of an attribute (e.g., who has the maximum salary)\nslice_max(data_object, attr_name)\n\n\n\n\nWe will take an example to understand this scenario. Let’s say we want to calculate the saving for each of the employee (from our salary dataset). So we want to have another attribute (saving) which will simply contains the amount of salary left after subtracting the expenses.\nWe will use mutate() function for this task. Following is the syntax\ndata_object %&gt;% mutate(expr)\nOR\nmutate(data_object,expr)\nHere expr means the expression that will be used to generate new attribute. In our case, the saving can be compute by subtracting expenses from salary. Therefore, expr is saving = salary - expenses.\n\n\n\nScreenshot 2021-02-26 at 11.51.01\n\n\nRefer here for more detailed information on mutate() function.\n\n\n\nWhiile analyzing the dataset, we are often interested in group-wise statistics. For example, what is average salary for younger and older people or what is difference in sleeping hours in male and female.\nTo answer these question, we need to way to first group the dataset and then compute required statistics.\nFor our dataset, we can have this question: what is average salary of people who are younger and older than 30 years. So this we will use group_by() and summarise() functions from dplyr package.\nFollowing is the representation of how the processing needs to be done.\nIn R, following is the syntax\ndata_object %&gt;% group_by(attribute_name_used_for_grouping or condition) %&gt;% summarize(statistics_you_need)\nhere first we need to specify attribute which will be used for grouping and then we will specify what summary stats we need. Following are the options for summary stats\n\nmean(), median()\nsd(), IQR()\nmin(), max()\nn()\n\nFor more details refer this\nNow, let’s findout average salary of people who are younger and older than 30 years. So, here we need a condition for grouping. For example, if age &lt; 30 then employee belongs to group-1 otherwise he/she belongs to group-2. Then we need to specify the attribute name for computing average.\nFollowing is the illustration of finding group-wise average.\n\n\n\nScreenshot 2021-02-26 at 12.25.21\n\n\nNow, we will do the same using R. For grouping we will use group_by() function. Here we need to specify the condition. Then we will summarise using summarise() function. Here we need to specify the statistics (e.g., mean, standard deviation, count) and the attribute/s name which will be used to compute statistics\n\n\n\nScreenshot 2021-02-26 at 12.29.02\n\n\n\n\n\nIn this section, we will use lattice package from R to produce graphs. We will cover create following graphs\n\nBar chart\nBox plot\nDensity plot\nHistogram\nScatter plot\n\nYou can use following syntax to create graphs using lattice package\ngraph_type(formula,data=)\nHere, graph_type is the name of graph which you want to plot. In formula, you need to specify what you want to plot. For example, if you want to plot a salary attribute from our dataset, you can write it as ~salary. If you want to plot two attributes (e.g., age and salary) then you can either write age~salary or salary~age.\nFollowing table shows the graph_type for each of aforementioned graph\n\n\n\ngraph_type\nWhat it plots\n\n\n\n\nbarchart\nbar chart\n\n\nbwplot\nboxplot\n\n\nDensityplot\nkernel density plot\n\n\nhistogram\nhistogram\n\n\nxyplot\nscatterplot matrix\n\n\n\nWe will learn more about how to write these formuls in our examples.\n\nLet’s plot a histogram for salary attribute from our dataset. For this, we will use histogram graph type. We will specify the name of the attribute ~salary.\ndataset &lt;- import(your_file_name_path)\nhistogram(~salary,data=dataset)   \n\n\n\n\nScreenshot 2021-02-26 at 17.57.38\n\n\n\n\n\nimage-20210226175840188\n\n\n\nNow, we will plot the scatterplot for age and salary. We will use xyplot.\n# You already have your dataset loaded in object dataset\nxyplot(age~salary,data=dataset)\n\n\n\nimage-20210226180034354\n\n\nLet’s plot a boxplot for salary attribute.\nbwplot(~salary,data=dataset   )\n\n\n\n\nimage-20210226180350357"
  },
  {
    "objectID": "posts/post-with-code/data_r.html#plotting-the-data",
    "href": "posts/post-with-code/data_r.html#plotting-the-data",
    "title": "R tutorial on accessing, filtering, aggregating and plotting data",
    "section": "",
    "text": "In this section, we will use lattice package from R to produce graphs. We will cover create following graphs\n\nBar chart\nBox plot\nDensity plot\nHistogram\nScatter plot\n\nYou can use following syntax to create graphs using lattice package\ngraph_type(formula,data=)\nHere, graph_type is the name of graph which you want to plot. In formula, you need to specify what you want to plot. For example, if you want to plot a salary attribute from our dataset, you can write it as ~salary. If you want to plot two attributes (e.g., age and salary) then you can either write age~salary or salary~age.\nFollowing table shows the graph_type for each of aforementioned graph\n\n\n\ngraph_type\nWhat it plots\n\n\n\n\nbarchart\nbar chart\n\n\nbwplot\nboxplot\n\n\nDensityplot\nkernel density plot\n\n\nhistogram\nhistogram\n\n\nxyplot\nscatterplot matrix\n\n\n\nWe will learn more about how to write these formuls in our examples.\n\nLet’s plot a histogram for salary attribute from our dataset. For this, we will use histogram graph type. We will specify the name of the attribute ~salary.\ndataset &lt;- import(your_file_name_path)\nhistogram(~salary,data=dataset)   \n\n\n\n\nScreenshot 2021-02-26 at 17.57.38\n\n\n\n\n\nimage-20210226175840188\n\n\n\nNow, we will plot the scatterplot for age and salary. We will use xyplot.\n# You already have your dataset loaded in object dataset\nxyplot(age~salary,data=dataset)\n\n\n\nimage-20210226180034354\n\n\nLet’s plot a boxplot for salary attribute.\nbwplot(~salary,data=dataset   )\n\n\n\n\nimage-20210226180350357"
  },
  {
    "objectID": "posts/post-with-code/mqtt.html",
    "href": "posts/post-with-code/mqtt.html",
    "title": "Setting up MQTT server and Client",
    "section": "",
    "text": "This post offers an introduction to the MQTT (Message Queuing Telemetry Transport) protocol [1] and also demonstrates its usage with an example in Python (Just for info: telemetry means the collection of measurement data from a remote location and its transmission wiki-link."
  },
  {
    "objectID": "posts/post-with-code/mqtt.html#what-is-mqtt",
    "href": "posts/post-with-code/mqtt.html#what-is-mqtt",
    "title": "Setting up MQTT server and Client",
    "section": "What is MQTT?",
    "text": "What is MQTT?\n\nMQTT is a communication protocol which is actually developed for low-energy devices to transmit data with the low-bandwidth network. It is a lightweight protocol built on the top of TCP/IP.\n\n\n\n\n\n\nTip\n\n\n\nLightweight: In software/programs, lightweight specify the characteristic of low memory and CPU usage.\n\n\nMQTT allows simple and efficient data transmission from sensors to IoT (Internet of Things) networks. Additionally, it also enables the easy integration of new sensors to the IoT network.\n\n\nMQTT Terminology\n\nIn MQTT, there are five main terminology\n\n\n\nBroker: It is the middleware between data sender (publisher) and data receiver (subscriber). You can consider it as a server which receives data from sender and forward it to the receiver.\n\n\nPublisher: In simple terms, you can think of a device which needs to send data to other parties. This device could be a sensor, laptop, and Raspberry pi board.\n\n\nSubscriber: It is the receiver of data sent by the publisher.\n\n\nTopic: Publisher and Subscriber do not know each other directly. When the publisher sends some data to Broker, it associates that data with a topic . The topic is a string in the format of hierarchy e.g. sensor/room-1/temprature (it is just an example) where ‘/’ is level separator and each string represents a level. You can define it as per your requirement. When the subscriber connects to the Broker, it specifies the topic in which it is interested in. Then, broker forward published messages to their corresponding subscribers on the basis of the topic. Here, you need to understand two operators + and #. Before jumping to these operators, let’s assume a scenario where we have four temperature sensors in four rooms. These sensors are sending their data with the following topics\n\n\n\n\n\n\nhome/bedroom/temperature\n\n\nhome/kitchen/temperature\n\n\nhome/dining-room/temperature\n\n\nhome/study-room/temperature\nNow, in order to receive data from all four rooms, subscribers need to subscribe to the above topics. One way of doing that is to subscribe to each topic separately. Another way is to use + operator and simply subscribe to the topic home\\+\\temprature . + operator here matches any single level ( + is known as a single-level wildcard). Therefore, home+will match any topic with three-level hierarchy starting with home  and ending with . Coming to second operator # which matches multiple levels in the topic. For instance, home\\# will match all the aforementioned four topics.\nSome examples Let’s say we have multiple sensors which are transmitting data with following topics\n\n\n\n\n\nschool\\class-1\\group-1\\audio\nschool\\class-1\\group-2\\audio\nschool\\class-2\\group-1\\audio\nschool\\class-2\\group-2\\audio\n\n\n\n\n\n\n\nTip\n\n\n\nWhich topic to subscribe to receive all data from class-1 ? school\\class-1\\#\nWhich topic to subscribe to receive data from group-1? school\\+\\group-1\\audio\nWhich topic to subscribe to receive data from entire school? school\\#\n\n\n\n# operator can appear only once in a topic expression e.g. school## is invalid. A topic expression # matches every topic hence subscriber to this topic will receive every message.\n\n\n\n\nMessage: It is simply the data which needs to be sent.\n\n\n\n\n\n\nDemonstration of MQTT in Python\n\nNow, we will see a python example of using MQTT protocol for transmitting data. We will use a python package [paho-mqtt](https://pypi.org/project/paho-mqtt/ “” target = ““_blank) for creating publisher and subcriber. You can install it using following command\n pip install paho-mqtt \nNext, we need to set up a Broker. There are numerous option of this. You can either use a cloud-based broker or you can install a broker on a server in your network. There are multiple cloud services are available with can be used ([Complete list of broker servers](https://github.com/mqtt/mqtt.github.io/wiki/servers “” target = ““_blank)). I would like to mention [MaqiaTTo](https://www.maqiatto.com “” target = ““_blank) which is a free cloud-based MQTT broker. It can be used for testing purpose. In this tutorial, however, we are going to set-up a broker in the network. We will use [Mosquitto](https://mosquitto.org/ “” target = ““_blank) broker server.\n\n\nSetting up Mosquitto Broker\n\nMosquitto is an open-source MQTT message broker. Following are the instructions for installing it on your systems.\n\n\n\nWindows users: Download and install 64bit-version, 32bit-version(If you are not sure then install 32bit-version)\n\n\nUbuntu Users: Run following commands\nsudo apt-add-repository ppa:mosquitto- dev/mosquitto-ppa\nsudo apt-get update \n\n\n\n\n\nMac Users: First install brew package manager using following command\n/usr/bin/ruby \\\n -e \"\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\" \nNext, install Mosquitto\nbrew install mosquitto \n\n\n\n\nStarting Mosquitto Server\nNow, we are going to start our MQTT broker. Run the following command [Mac users]\n/usr/local/sbin/mosquitto -c /usr/local/etc/mosquitto/mosquitto.conf \n\n\nOn ubuntu following command can be used to start or stop Mosquitto\nsudo systemctl (start|stop) mosquitto\n\n\n\n\n\nWriting Publisher and Subscriber codes\n\nNow, we will develop our publisher and subscriber. Following are the steps for developing publisher and subscriber in Python.\n\n\n\nCreate client\n\n\nConnect to broker\n\n\nPublish/subscribe message\n\n\nConnect callback function\n\n\nRun the loop\n\n\nDisconnect\n\n\n\nLet’s understand these steps in details. The first step is to create a client. For this, we will create a Client object from paho-MQTT python package.\nThe second step connects to the broker. In our case, we are running the broker on the same machine, therefore, we specify the broker’s IP as 127.0.0.1 and port as 1883 (default port for Mosquitto broker).\nIn third step, you can publish or subcribe using publish() and subscribe() function.\nNext, we need to write callback functions. It needs a bit of explanation. Callback functions are functions which are executed on the occurrence of particular events. Followings are the table showing the event and their corresponding callback function\n\n\n\n\n\n\nEvent\nCallback Function\n\n\nConnection ACK\non_connect\n\n\nDis-connection ACK\non_disconnect\n\n\nPublish ACK\non_publish\n\n\nSubcription ACK\non_subcribe\n\n\nUn-subcription ACK\non_unsubcribe\n\n\nMessage received\non_message\n\n\n\n\n\n\nLet’s take one example to understand it. When a client connects to the broker, the broker sends an ACK (acknowledgment) to the client. This event triggers the execution of a callback function on_connect .\n\n\nRunning a loop: Why we need to start a loop?\nWhen a publisher sends messages to the broker or subscriber receive messages from the broker, these messages are first stored in the buffer. Now, in order to process all messages (either for sending or receiving), we need to write a loop manually. Thanks to paho-MQTT , it provides three functions for the same purpose, therefore, we don’t need to write message processing loop. These functions are as following\n\n\n\n\n\n\nTip\n\n\n\n\n\nloop(): When you call this function, it will process any pending message sending or receiving action. This function waits for a particular time (you can specify using timeout parameter) for processing buffer for reading or sending a message. After, that its execution completes. Therefore, if you plan to use this function you need to call it regularly.\n\n\nloop_forever(): This function call results in indefinite execution of your program. This function automatically reconnects to the broker in case of disconnection. This function is blocking type function (you can understand it as an infinite for loop) and it returns when you disconnect with the broker.\n\n\nloop_start() & loop_stop(): loop_start() function starts a new background thread and that thread regularly execute loop() function. You can stop this background thread using loop_stop() function.\n\n\n\n\n\n\n\nCoding Publisher\n\nAs we have setup our broker server, now we move towards writing publisher code.\n\nimport paho.mqtt.client as MQTT\n\n#  function\ndef connect_msg():\n    print('Connected to Broker')\n\n# function\ndef publish_msg():\n    print('Message Published')\n\n# Creating client\nclient = mqtt.Client(client_id='publisher-1')\n\n# Connecting callback functions\nclient.on_connect = connect_msg\nclient.on_publish = publish_msg\n\n# Connect to broker\nclient.connect(\"\"127.0.0.1\"\",1883)\n\n# if you experience Socket error then replace above statement with following one\n# client.connect(\"\"127.0.0.1\"\",1883,60)\n\n# Publish a message with topic\nret= client.publish(\"\"house/light\"\",\"\"on\"\")\n\n# Run a loop\nclient.loop()\nThe above code publishes a message on with topic house/light. In this code, we have written two functions connect_msg and publish_msg (you can use any names for functions). We connected these functions to callback functions using client.on_connect = connect_msg . We specified that connect_msg function is callback function and it will be called when the Connection ACK event occurs (events and their callbacks are given in the above table). In this program, we used the loop function which process pending action (sending or receiving) and then returns.\n\n\nCoding Subscriber\n\n\nAs we have our publisher with topic housetopic, we now develop our subscriber for the same topic.\nimport paho.mqtt.client as MQTT #import the client\n\n# Function to process recieved message\ndef process_message(client, userdata, message):\n    print(\"\"message received \"\" ,str(message.payload.decode(\"\"utf-8\"\")))\n    print(\"\"message topic=\"\",message.topic)\n    print(\"\"message qos=\"\",message.qos)\n    print(\"\"message retain flag=\"\",message.retain)\n\n\n\n# Create client\nclient = mqtt.Client(client_id=\"\"subscriber-1\"\")\n\n# Assign callback function\nclient.on_message = process_message\n\n# Connect to broker\nclient.connect(broker_address,1883,60)\n\n# Subscriber to topic\nclient.subscribe(\"\"house/light\"\")\n\n# Run loop\nclient.loop_forever() \n\n\nExecution Results\n\n\n\n\n\n\n\nReferences\n\n\n\nMQTT Version 5.0. Edited by Andrew Banks, Ed Briggs, Ken Borgendale, and Rahul Gupta. 07 March 2019. OASIS Standard. https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html. Latest version: https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html.\n\n\n\n”"
  },
  {
    "objectID": "posts/post-with-code/basics_nlp.html",
    "href": "posts/post-with-code/basics_nlp.html",
    "title": "Basics of Natural Language Processing in Python",
    "section": "",
    "text": "Natural language processing is an interdisciplinary field combining computer science, artificial intelligence, and linguistics. Natural language processing focuses on the processing of natural languages (i.e., human languages such as English, and Hindi). This processing aims at comprehending natural languages and generating text in those languages.\nNatural language processing, thus, enables computers to understand and process human languages and presents the enormous potential of building intelligent machines capable of understanding human input in their own languages.\nIn this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial. The post is targeted at beginners who are just starting to gain first-hand experience with natural language processing. # Basics of Natural Language Processing in Python Natural langauge processing enables computer to human languages. It combines linguistic with statistical modeling. In this post, we will become familiar with basics of nlp with Python. We will use NLTK library for the tutorial.\nFront image by Andrea De Santis on Unsplash."
  },
  {
    "objectID": "posts/post-with-code/basics_nlp.html#basic-concepts",
    "href": "posts/post-with-code/basics_nlp.html#basic-concepts",
    "title": "Basics of Natural Language Processing in Python",
    "section": "Basic concepts",
    "text": "Basic concepts\nNow, we will get familiar with the basic concepts of natural language processing. This processing takes place through multiple steps. With each step, a higher level of abstraction is achieved. Let’s understand some basic steps first.\nWhen a text is preseted to a computer, it only sees the text as a sequence of characters; So, the first step focuses on breaking the character’s sequence into sentences, and then each sentence into words. This step is known as Tokenization.\nNext, the sentence structure is understood from a grammatical point of view. This involves identifying nouns, verbs, objects, etc. This step is known as Parts-of-Speech Tagging.\nOnce the grammatical relavant tags are identified for each word in the sentence, the next step tries to apply a transformation which replaces words with their base form. For example, transforming running into run. This step is known as Stemming/Lemmatization. We will later discuss the differences between these two.\nThe final step converts the text data into numbers for the computer’s usage. This step is known as Vectorization.\nNow we will cover these topics one by one. The list of topics is provided below as well.\n\nTokenization\nParts of Speech Tagging\nStemming/Lemmatization\nVectorization\n\n\nTokenization\nLet’s start with tokenization, the first step in the process. The tokenization step simply breaks down text data into smaller units for analysis purposes such as sentences, words, numbers, etc. These units are also known as tokens.\nThe following program performs tokenization, first breaking the text into a group of sentences, and second, breaking each sentence into a group of words.\n\nfrom nltk import word_tokenize, sent_tokenize\n\ntext = \"\"\"This post offers basics of natural langauge processing (NLP) in Python. \n    NLP enables computer to human languages. It combines linguistic with statistical modeling.\n    \"\"\"\n\n# Breaking the text data into sentences\nsentences = sent_tokenize(text)\nprint('Sentences:\\n',sentences)\n\n# Breaking each sentece into words\nwords = [word_tokenize(sentence) for sentence in sentences]\nprint('Words:\\n',words)\n\nSentences:\n ['This post offers basics of natural langauge processing (NLP) in Python.', 'NLP enables computer to human languages.', 'It combines linguistic with statistical modeling.']\nWords:\n [['This', 'post', 'offers', 'basics', 'of', 'natural', 'langauge', 'processing', '(', 'NLP', ')', 'in', 'Python', '.'], ['NLP', 'enables', 'computer', 'to', 'human', 'languages', '.'], ['It', 'combines', 'linguistic', 'with', 'statistical', 'modeling', '.']]\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou need to run nltk.download('punkt') only once.\n\n\n\n\nPOS tagging\nLet’s move to now understanding grammatical structure of sentences. The step involves identifying whether a word in the sentence is noun, verb, adverb, etc. It is known as Parts-of-Speech or POS tagging.\nLet’s see our first example of tagging.\n\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk import pos_tag\nimport nltk\n\n# Uncomment the below statement if you get an error of tagger not found\n# nltk.download('averaged_perceptron_tagger')\n\n# using only word tokenization because there is only one sentence in the text data.\nwords = word_tokenize('Estonia is a leading country in digital space.')\n\n# applying parts-of-speech tagging\ntags = pos_tag(words)\n\nprint(tags)\n\n[('Estonia', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'JJ'), ('country', 'NN'), ('in', 'IN'), ('digital', 'JJ'), ('space', 'NN'), ('.', '.')]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can check the meaning of each tag using ntlk.help.upenn_tagset() function.\n\n\n\n# checking the meaning of NN\nnltk.help.upenn_tagset('NN')\n\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\n\n\nThe result is provided in a form of list of tuples. Each tuple contains a word and corresponding word category (e.g., VBZ for verb). This tuple is also known as tagged token. You can read about it in more details here.\nThe complete list is given below.\n\n\n\n\n\n\n\nTag\nDescription\n\n\n\n\nCC\ncoordinating conjunction\n\n\nCD\ncardinal digit\n\n\nDT\ndeterminer\n\n\nEX\nexistential there (like: “there is” . think of it like “there exists”)\n\n\nFW\nforeign word\n\n\nIN\npreposition/subordinating conjunction\n\n\nJJ\nadjective ‘big’\n\n\nJJR\nadjective, comparative ‘bigger’\n\n\nJJS\nadjective, superlative ‘biggest’\n\n\nLS\nlist marker\n\n\nMD\nmodal could, will\n\n\nNN\nnoun, singular ‘desk’\n\n\nNNS\nnoun plural ‘desks’\n\n\nNNP\nproper noun, singular ‘Harrison’\n\n\nNNPS\nproper noun, plural ‘Americans’\n\n\nPDT\npredeterminer ‘all the kids’\n\n\nPOS\npossessive ending parent\n\n\nPRP\npersonal pronoun I, he, she\n\n\nPRP$\npossessive pronoun my, his, hers\n\n\nRB\nadverb very, silently,\n\n\nRBR\nadverb, comparative better\n\n\nRBS\nadverb, superlative best\n\n\nRP\nparticle give up\n\n\nTO,\nto go ‘to’ the store.\n\n\nUH\ninterjection, errrrrrrrm\n\n\nVB\nverb, base form take\n\n\nVBD\nverb, past tense took\n\n\nVBG\nverb, gerund/present participle taking\n\n\nVBN\nverb, past participle taken\n\n\nVBP\nverb, sing. present\n\n\nVBZ\nverb, third person sing. present takes\n\n\nWDT\nwh-determiner which\n\n\nWP\nwh-pronoun who, what\n\n\nWP$\npossessive wh-pronoun whose\n\n\nWRB\nwh-abverb where, when\n\n\n\n\n\nStemming and Lemmatization\nThis step transforms a word into its root word or base form. For example, car, cars, car’s all share a common root word car. In the linguistic field, such words are known as words with inflection endings or derivationally related words. There is a nice post which you can refer to understand more about it and also about morphological analysis.\nHere we are briefly discussing inflection and derivational forms of words.\nInflection forms These word forms are used to distinct tenses, person, gender, etc. For example, words like go, going, gone, goes. If you notice these words have different endings. These all are called inflection endings in linguistic field.\n\n\n\n\n\n\nTip\n\n\n\nThe words with inflection endings do not have a separate entry in the dictionary. You will find all words with inflection endings under a single entry i.e., go.\n\n\nDerivational form\nThese word forms are derived from the rood words and create a new meaning. For example, react and actor both are derived from the word act.\n\n\n\n\n\n\nTip\n\n\n\nThe words with derivational forms have a separate entry in the dictionary. You will find a separate entry in the dictionary for act, react, and actor.\n\n\nNow, as we have a preliminary understanding of different forms of words, we next move to the stemming and lemmatization. These are two techniques to transform a word from its inflection form (and sometimes derivational form) to the base form.\n\n\nStemming\nStemming is the technique which simply chops off the ending of a word to obtain its base form. For example, removing ing from eating to obain the base form i.e., eat. By default NLTK uses a rule-based stemmer (i.e., Porter Stemmer). There are other stemmer as well. You can check this page for more information on different stemmers.\nLet’s take a look at the following code which perform stemming.\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n# Uncomment the following statement if running it first time\n#nltk.download('wordnet')\n\n\n# Initialize Python porter stemmer\nps = PorterStemmer()\n\n# Tokenize the text data\nwords = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n\n# Printing the data with its base form after stemming operation\nfor word in words:\n    print('Word:{:10} Stem:{}'.format(word,ps.stem(word)))\n\nWord:Estonia    Stem:estonia\nWord:is         Stem:is\nWord:a          Stem:a\nWord:leading    Stem:lead\nWord:country    Stem:countri\nWord:in         Stem:in\nWord:digital    Stem:digit\nWord:space      Stem:space\nWord:,          Stem:,\nWord:and        Stem:and\nWord:on         Stem:on\nWord:its        Stem:it\nWord:way        Stem:way\nWord:to         Stem:to\nWord:become     Stem:becom\nWord:the        Stem:the\nWord:leader     Stem:leader\nWord:.          Stem:.\n\n\nIn the output, we can see that words leading, country, digital are transformed into lead, countri, and digit, respectively. Now, we will move to Lemmatization.\n\n\nLemmatization\nLemmatization is an another technique which also perform a similar task as Stemming i.e., transforming words into its base forms. However, it differs in its approach. Lemmatization uses morphological analysis to achieve the goal. The morphological analysis means understanding of words and their parts.\nYou can refer to this post to gain more information on different lemmatization approaches in python.\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Lemmatizer \nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Tokenization\nwords = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n\nfor word in words:\n    print('Word:{:10} Lemma:{} Tag:{}'.format(word,wordnet_lemmatizer.lemmatize(word),wordnet_lemmatizer.lemmatize(word)))\n\nWord:Estonia    Lemma:Estonia\nWord:is         Lemma:is\nWord:a          Lemma:a\nWord:leading    Lemma:leading\nWord:country    Lemma:country\nWord:in         Lemma:in\nWord:digital    Lemma:digital\nWord:space      Lemma:space\nWord:,          Lemma:,\nWord:and        Lemma:and\nWord:on         Lemma:on\nWord:its        Lemma:it\nWord:way        Lemma:way\nWord:to         Lemma:to\nWord:become     Lemma:become\nWord:the        Lemma:the\nWord:leader     Lemma:leader\nWord:.          Lemma:.\n\n\n[nltk_data] Downloading package wordnet to /Users/htk/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\n\n\n\n\n\nNo change after lemmatization\n\n\n\nIf you notice in the results there are no changes after applying lemmatization. The reason is that if the word can not be found in WordNet (publicly awailable English dataset) then the word remain unchanged. It can be corrected by providing the pos tag of the word when calling lemmatize function.\n\n\nNow, we will supply pos tag of each word when calling lemmatize function. However, the function only takes a single character for the pos tag. For example, n for nouns, v for verbs, a for adjectives, and r for adverbs.\nSo, we need to prepare a mapping which translates pos tags, obtained from nltk.pos_tag() function, into a, r, n, v (depending on the tag).\nWe know from the POS tags table above that tags for adjectives starts from ‘J’. So what we can do is, we can take the first character of pos-tag and determine which tag to supply in lemmatize() function.\n\nfrom nltk import pos_tag\n\ndef get_pos(word):\n    # the function returns a list with one tagged tuple, e.g., [('riding','VBD')]\n    tagged_tuple_list = pos_tag([word])\n    \n    # fetching the item in the list and then the tag in tuple\n    tagged_tuple = tagged_tuple_list[0]  # the first index will fetch the fist item in the list\n    \n    # fetching the tag from the tagged tuple\n    tag = tagged_tuple[1]   # the index will fetch the tag (e.g., 'VBD')\n    \n    # extracting the first character\n    tag_char = tag[0]\n    \n    # all these three statement can be combined into a single statement given below\n    # tag_char = pos_tag([word])[0][1][0]\n    \n    # Now we will create a mapping\n    pos_to_lemma_tag = {\n        'J': 'a',\n        'N': 'n',\n        'R': 'r',\n        'V': 'v'\n    }\n    \n    # we will return the tag for usage in lemmatize function\n    return pos_to_lemma_tag.get(tag_char,'n')   # get function will return n if the tag is something else\n\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Lemmatizer \nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Tokenization\nwords = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n\nfor word in words:\n    print('Word:{:10} Lemma:{}'.format(word,\n                                       wordnet_lemmatizer.lemmatize(word,get_pos(word))))\n    \n\nWord:Estonia    Lemma:Estonia\nWord:is         Lemma:be\nWord:a          Lemma:a\nWord:leading    Lemma:lead\nWord:country    Lemma:country\nWord:in         Lemma:in\nWord:digital    Lemma:digital\nWord:space      Lemma:space\nWord:,          Lemma:,\nWord:and        Lemma:and\nWord:on         Lemma:on\nWord:its        Lemma:it\nWord:way        Lemma:way\nWord:to         Lemma:to\nWord:become     Lemma:become\nWord:the        Lemma:the\nWord:leader     Lemma:leader\nWord:.          Lemma:.\n\n\n\nIt worked now like a charm :-)\n\n\n\nVectorization\nNow, we will move to the final step in the aforementioned list of preprocessing steps in natural language processing. This final step is vectorization step. This step translates the text into numbers so that computers can use them for further anlaysis.\nThere are multiple techinques of vectorization. In this post, we are going to discuss two basics techniques techniques: count vectorization, and tf-idf vectorization.\n\nCount Vectorization\nIn this technique, the input text is first broken down into a set of unique words, and next, each word is assigned a number which represents the frequency of that word.\nLet’s see a working example. For the example, we will use CountVectorizer from scikit-learn.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# input data\ninput_text = [\"In this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial.\"] \n\n# initialization\nvect = CountVectorizer()\n\n# applying count vectorization on the text\nresult = vect.fit_transform(input_text)\n\n# printing vocubulary with frequency\nprint('Shape:',result.shape, '\\n Vector:',result.toarray())\n\nShape: (1, 20) \n Vector: [[1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 2 2 2]]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe result is a single vector of length 20. In out input, we only had a single sentence, therefore, one got a single vector. In case of multiple sentences, we get one vector for each sentence.\n\n\n\n\nTF-IDF\nThe next vectorization technique is TF-IDF (Term Frequency- Inverse Document Frequency). Let’s understand their meaning. \nTerm Frequency (TF) This frequency counts the number of times a word occurs in the document.\nInverse Document Frequency This is the inverse of how many documents contains the specified term.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# input data\ninput_text = [\"This is an amazing field with huge potential of building intelligent machine.\",\n              \"Those machine would be able to transform people's life.\",\n              \"This transformation would significantly improve the quality of life.\"]\n# TF-IDF intialization\ntf = TfidfVectorizer()\n\n# applying vectorizer\nresult = tf.fit_transform(input_text)\n\n# print results\nprint('Shape:',result.shape, '\\n Vector:',result.toarray())\n\nShape: (3, 25) \n Vector: [[0.         0.30520733 0.30520733 0.         0.30520733 0.30520733\n  0.30520733 0.         0.30520733 0.30520733 0.         0.23211804\n  0.23211804 0.         0.30520733 0.         0.         0.\n  0.23211804 0.         0.         0.         0.         0.30520733\n  0.        ]\n [0.35955412 0.         0.         0.35955412 0.         0.\n  0.         0.         0.         0.         0.27345018 0.27345018\n  0.         0.35955412 0.         0.         0.         0.\n  0.         0.35955412 0.35955412 0.35955412 0.         0.\n  0.27345018]\n [0.         0.         0.         0.         0.         0.\n  0.         0.36977238 0.         0.         0.28122142 0.\n  0.28122142 0.         0.         0.36977238 0.36977238 0.36977238\n  0.28122142 0.         0.         0.         0.36977238 0.\n  0.28122142]]\n\n\n\nYou can check [this blog post](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp) on further information on vectorization.\n\n\nReferences\n1. https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n2. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n3. https://www.nltk.org/book/\n4. Morphological analysis: https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx\n5. https://www.datacamp.com/tutorial/stemming-lemmatization-python\n6. https://pianalytix.com/countvectorizer-in-nlp/#:~:text=CountVectorizer%20means%20breaking%20down%20a,data%20needs%20to%20be%20vectorized."
  },
  {
    "objectID": "posts/post-with-code/pandas.html",
    "href": "posts/post-with-code/pandas.html",
    "title": "Introduction to Python’s Pandas API",
    "section": "",
    "text": "Pandas is a Python API for processing data in a easy and efficient way. This post offers an introduction to this amazing API , especially for beginners.\nThe post starts with installation instructions of the API and then introduces its functionality with the help of examples."
  },
  {
    "objectID": "posts/post-with-code/pandas.html#installation",
    "href": "posts/post-with-code/pandas.html#installation",
    "title": "Introduction to Python’s Pandas API",
    "section": "Installation",
    "text": "Installation\n\n\nThere are two options to install the Pandas API on your system. The first option is to install it through Anaconda Distribution which comes with all essential Python packages. While the second option is to install Python on the system and then install Pandas package using the following command. \n\npip install pandas"
  },
  {
    "objectID": "posts/post-with-code/pandas.html#introduction",
    "href": "posts/post-with-code/pandas.html#introduction",
    "title": "Introduction to Python’s Pandas API",
    "section": "Introduction",
    "text": "Introduction\nNow we will explore the API basics. Basically, we will cover the following topics which I think will be good enough to start using Pandas API.\n\n\n\n\n\nPandas DataFrame and Series objects\n\n\nReading data CSV files and accessing basic information\n\n\nQuerying data using loc() and iloc() function\n\n\nHandling missing data\n\n\nAdding, deleting columns or rows\n\n\n\n\n\nPandas DataFrame and Series objects\nPandas’ Series is a one-dimensional array representation of values. we can understand it as an attribute in a dataset. For instance, consider a dataset with three attributes sid (student-id), name, and marks. Now, each of these attributes in pandas is represented as a Series object.\n\n\nLet’s write a program to create these three Series objects sid, name, and marks.\n\nimport pandas as pd\n\n# Creating Series using list\nid = pd.Series([101,102,103,104,105])\n\nname = pd.Series(['pradeep','manoj','kiran','pushpendra','sambit']\n\nmarks = pd.Series([20,30,40,32,28])&lt;/code&gt;&lt;/pre&gt;\nThe first line import pandas as pd imports the pandas package in wer program. Next, three lines create three Series objects with a given list.\n\nPandas’ DataFrame object is a two-dimensional data structure where each attribute is a Series object. we can create a DataFrame using a dictionary of key:value pair where the key represents attribute name and the value represents the values of that attribute.\nLet’s create a dataframe using the Series objects we created in the above program.\n\ndf = pd.DataFrame({'sid':id,'name':name,'marks':marks})\n\n\nReading data CSV files and accessing basic information\nPandas have a function read__csv() to read CSV format data files. This function comes with multiple useful options which we will learn in this section. The data file used in this tutorial can be downloaded from the link. The name of the downloaded data file is iris_csv.csv. \n\nOpen and read a CSV data file\nimport pandas as pd\ndf=pd.read_csv('iris_csv.csv')\ndf.head()\n\ndf=pd.read_csv(‘iris_csv.csv’) opens and reads the specified CSV file (here we can specify the name of wer data file). The third line df.head() shows first five records (we can specify the number of records) from wer data file.\n\n\n\nAssign/Rename column names\n\nIn case, if wer data file does not have column names or we want to assign a different column name then we can use the names option of read_csv() function. Example:\n\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv',names=['sep-len','sep-wid','pet-len','pet-wid','class'])\n\n\nReading data file with different seperator\n\nSometimes the data files have columns seperated by other characters (e.g. spaces, colon). In such cases, in order to read the CSV file we need to specify the sep option in the read_csv() function.\n\n# reading file having data seperated by :\ndf = pd.read_csv('data_file',sep=':')\n\n\nSkipping rows while reading data\n\nIn case, if wer data file does not have data records from the first line (let’s say it contains some summary or description and data records begins from line 4), we can skip those lines by specifying skip rows option.\n\ndf = pd.read_csv('data_file',skiprows=3)\n\n\nAccessing sizes of data\nwe can check the size of wer data set (e.g. number of rows, number of columns) using shape property of the DataFrame.\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv')\nprint(df.shape)\n# output\n# (150, 5)\n\nHere, 150 is the number of rows and 5 is number of columns.\n\n\n\nChecking data types of columns\n\nTo check the data types of columns in the data file, we can use &lt;a href=““https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html”“&gt;dtypes property.\n\n\n\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv')\ndf.dtypes&lt;/code&gt;&lt;/pre&gt;\n\nOutput:\n\nsep-len    float64\nsep-wid    float64\npet-len    float64\npet-wid    float64\nclass       object\ndtype: object\n\n\n\n\n\n\n\nTip\n\n\n\nAs the data processing modules requires wer data to be in numeric data types (e.g. int, float) it is best practice to check the data types before processing it.\n\n\n\n\nBasic stats of data\nIf we want to learn about our data in more depth, we can use describe() function. This function provides information about count, minimum, maximum, mean, standard deviation, quartiles for each column. An example is given below.\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv',skiprows=1,names=['sep-len','sep-wid','pet-len','pet-wid','class'])\ndf.describe()&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\nQuerying data using loc() and iloc() function\nPandas offers two different functions (there is one more ix() which is actually deprecated) for accessing data from the dataframe- .loc() and .iloc(). In these functions, we specify the labels or positions of rows and columns to access data. However, if we do not specify columns selector then by default all columns are accessed.\n\n\n\n\n\n\n\nTip\n\n\n\n: operator used for slicing purpose. It works differently in case of label and position. When applied with labels (start:end), it include end element in the result. However, in case of positions (start:end), it does not include end in the result.\n\n\n\nLet’s understand the difference between labels and positions. In the following code, we are creating a simple dataframe with two columns sid and name.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\ndf.head()\nAs shown in the figure below, the row labels are (0,1,2,3,4) and column labels are (sid, name). The position for rows and columns begins with 0 that means the first row has position 0, second row has position 1 and so on. \nIn the above example, the rows position and labels are same. To make the difference clear, let’s try to change the index of our dataframe and then see it. we can change the index of dataframe using set_index() function. In the following example, we are setting the first column sid as the index of our dataset. This function create a copy of dataframe, apply changes it it and then return the updated copy. In order to make changes to dataframe inplace=True parameter needs to be passed.\ndf.set_index('sid',inplace=True)\ndf.head()\n\nAs we can see in the figure below, rows’ indices are (101,102,103,104,105) whereas rows’ positions are the same as previous.\n\n\n\nSome Examples\nThe following figure shows some examples for accessing data with label and position-based selections.\n\n\nTo access a particular row, we need to specify its label or position in the row-selector (for example, we have to specify label 0 to access first row). In case, if we want to access multiple rows, we need to specify their corresponding labels or positions in a list or we can use : operator for slicing (for example, row selector for accessing first three rows can be [0,1,2] or 0:3).\n\n# import pandas package\nimport pandas as pd\n\n# create the dataframe\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# set sid as index\ndf.set_index('sid',inplace=True)\n\n# Access first row\ndf.loc[101]  # lable-based\n\ndf.iloc[0]  #  Position-based\n\n\n# Access first three rows\ndf.loc[101:103]  # label-based\n\ndf.iloc[0:3]  # position-based\n\n\nCondition-based data access\n\n\n\nFunction loc() and iloc() both support condition-based data access using boolean array. Let’s say we want to access the first row. To do that we need to specify a boolean array for rows selection. This array will contain a boolean value for each row and only one True value.\nIf we want to show a particular set-of rows, we can do that by specifying a boolean array with True values on corresponding location of those rows. Same applies for column selection.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\ndf.loc[[True,False,False,False,False],[True,True]]\n\n#Output\n#      sid     name\n#0     101     pradeep\n\n\nIn the above example, we spcified a boolean array for rows selection and another for columns selection. In the first array, True is specified at the first index (which corresponds to the first row). The second array contains all True(which corresponds to all columns). Hence, we get the values from the first row and both columns.\n\n\n\n\nHandling missing data\nPandas offers a great support to handle missing values. If the dataset has some values missing, Pandas automatically marks them as NaN values. To demonstrate it, I have prepared a CSV file with three columns- eid, name, salary.\n\nIn this file, I intentionally kept the salary field for the third record empty for the following exercises on the missing values.\nNow let’s read this file using pandas.\ndf=pd.read_csv('emp.csv')\ndf.head()\n\nOutput:\n\n\n\nWe can handle missing values in two ways: delete or replace. The following sections discusses these both ways.\n\n\nDeleting missing values\n\n\nWe can use the dropna() function to delete missing records.\nIn this function, we need to specify axis=0 if we want to delete the row/rows having NaN and for deleting column/columns having NaN specify axis=1. \n\ndf=pd.read_csv('emp.csv')\n\n#delete row\ndf1 = df.dropna(axis=0)\ndf1.head()\n\n#delete column\ndf1 = df.dropna(axis=1)\ndf1.head()&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\n\n\n\nTip\n\n\n\nTo change the original dataframe, specify inplace=True in the dropna() function.\n\n\n\n\n\nFilling missing values\n\n\n\nFunction fillna is useful in filling missing values in the dataframe.\n\ndf=pd.read_csv('emp.csv')\n\n#fill with 0\ndf1 = df.fillna(0)\ndf1.head()\n\n#fill using forward fill method\ndf2 = df.fillna(method='ffill')\ndf2.head()\n\n# fill using backward fill method\ndf3 = df.fillna(method='bfill')\ndf3.head()\n\n# fill using mean value of column\ndf4 = df.fillna(df['salary'].mean())\ndf4.head()\n\n\n\n\n\n\n\nTip\n\n\n\nffill replaces NaN with the previous value in the same column. While, bfill replaces NaN with the next value in the same column (order of values top to bottom).\n\n\n\n\n\nAdd or delete row/column\n\nThis section will show we how to add a new row or column to an already existing dataframe.\n\n\nAdding row/column\nWe can simply add a row using append() or loc()/iloc() function. We can use key:value pair in the append function, where the key is the attribute name and the value represents the value we want to add. Pandas automatically puts NaN if some attributes values are not provided.\nNow, let’s add a record with sid as 106 and name as ‘gaurav’.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# using the append function\ndf = df.append(append({'sid':106,'name':'gaurav'},ignore_index=True)\nprint(df)\n\n# Adding a column\ndf['marks'] = [20,30,40,32,28,50]\nThe same record can also be added using df.loc[5]=[106,‘gaurav’].\n\n\nDeleting row/column\n\n\n\nTo delete some columns or rows, the drop function can be used. In this function, we need to specify the label of row or column we want to delete. In case, it is a column then we also need to pass a parameter axis=1.\nThe following example illustrates the use of drop function.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# delete sid column\ndf.drop('sid',axis=1)"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html",
    "href": "posts/post-with-code/Bike sharing map visualization.html",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "This tutorial will show you how to work with geospatial data using Python with library geoPandas. For this exercise, I have used Boston Bike Sharing dataset which is available here. I have recently started learning and using Map library with Python and this tutorial is the reflection of things I have learned so far. I hope it will be helpful for those learning the same first time.\n\n\nWe start first loading our dataset. There are two CSV files, hubway_trips.csv and hubway_station.csv. The first data file contains information all bike trips (e.g., bike number, start and end time of the trip, start and end station of the trip, user information, etc). The second data file hubway_station.csv contains information about the bike stations (e.g., longitude, lattitude, municipality).\n\nimport pandas as pd\n\n# Bike trip data\ntrips = pd.read_csv('hubway_trips.csv')\ntrips.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n\n\n\n\n\n\n\n\n# Bike station data [seperator in this file is ';']\nstations = pd.read_csv('hubway_stations.csv',sep=';')\nstations.head()\n\n\n\n\n\n\n\n\nid\nterminal\nstation\nmunicipal\nlat\nlng\nstatus\n\n\n\n\n0\n3\nB32006\nColleges of the Fenway\nBoston\n42.340021\n-71.100812\nExisting\n\n\n1\n4\nC32000\nTremont St. at Berkeley St.\nBoston\n42.345392\n-71.069616\nExisting\n\n\n2\n5\nB32012\nNortheastern U / North Parking Lot\nBoston\n42.341814\n-71.090179\nExisting\n\n\n3\n6\nD32000\nCambridge St. at Joy St.\nBoston\n42.361285\n-71.065140\nExisting\n\n\n4\n7\nA32000\nFan Pier\nBoston\n42.353412\n-71.044624\nExisting\n\n\n\n\n\n\n\n\n\n\nWe will first join the data to have a single file with lattitude and longitude information for start station of bike trip. For that we will perform join operation on start_statn in trips and id in stations.\n\n# we exclude status column because it is present in both data file and will cause an error on join operation.\nstations_non_status = stations[['id','lat','lng','station','municipal']]\n\n# combined data with start station geo spatial information\ntrips_stations = trips.join(stations_non_status.set_index('id'),on='strt_statn')\n\ntrips_stations = trips_stations.loc[trips_stations['municipal'] == 'Boston',:]\n\n\ntrips_stations.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\nlat\nlng\nstation\nmunicipal\ngeometry\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n\n\n\n\n\n\n\n\nNow we will move towards setting up our dataset to have geospatial data required by GeoPandas library. GeoPandas is a python library with extends capability of Pandas by including GeoSpatial data processing and plotting functionality.\nFirst install GeoPandas library if you don’t have it on your computer.\n\n# Installing Geopandas\n#! pip3 install geopandas\n\n\nimport geopandas as gpd\nfrom shapely import Point, Polygon\n\n\ngeo_column = [Point(lng,lat) for lng, lat in zip(trips_stations['lng'],trips_stations['lat'])]\n\nWhat we did in above code was created a list with a Point object for each record in our joined data file. This Point object will provide geospatial data information to GeoPandas to work with.\nNow we will create our GeoPandas dataframe which will have our data with geospatial information.\n\ncrs={'init':'epsg:4326'}\ngdf = gpd.GeoDataFrame(trips_stations,crs=crs,geometry=geo_column)\n\nNow, we created a geopandas dataframe which have all our records from trips_stations dataframe and all those records are associated with a geometric point (e.g., geospatial data).\nEach geopandas dataframe requires one geometry column which has information of geospatial data.\n\n\n\n\ngdf.plot('strt_statn')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nThe above plot does not show Boston’s map. In order to do that we would need Map files for Boston.\n\nboston = gpd.read_file('./City_of_Boston_Boundary/')\nboston = boston.to_crs({'init': 'epsg:4326'})\nboston.plot()\n\n/Users/htk/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\n\nNow we will plot the data of start station of bike trip on the map of Boston.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,8))\nboston.plot(ax=ax)\ngdf.plot(ax=ax,color='red',marker='+',alpha=.5)\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#loading-the-dataset",
    "href": "posts/post-with-code/Bike sharing map visualization.html#loading-the-dataset",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "We start first loading our dataset. There are two CSV files, hubway_trips.csv and hubway_station.csv. The first data file contains information all bike trips (e.g., bike number, start and end time of the trip, start and end station of the trip, user information, etc). The second data file hubway_station.csv contains information about the bike stations (e.g., longitude, lattitude, municipality).\n\nimport pandas as pd\n\n# Bike trip data\ntrips = pd.read_csv('hubway_trips.csv')\ntrips.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n\n\n\n\n\n\n\n\n# Bike station data [seperator in this file is ';']\nstations = pd.read_csv('hubway_stations.csv',sep=';')\nstations.head()\n\n\n\n\n\n\n\n\nid\nterminal\nstation\nmunicipal\nlat\nlng\nstatus\n\n\n\n\n0\n3\nB32006\nColleges of the Fenway\nBoston\n42.340021\n-71.100812\nExisting\n\n\n1\n4\nC32000\nTremont St. at Berkeley St.\nBoston\n42.345392\n-71.069616\nExisting\n\n\n2\n5\nB32012\nNortheastern U / North Parking Lot\nBoston\n42.341814\n-71.090179\nExisting\n\n\n3\n6\nD32000\nCambridge St. at Joy St.\nBoston\n42.361285\n-71.065140\nExisting\n\n\n4\n7\nA32000\nFan Pier\nBoston\n42.353412\n-71.044624\nExisting"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#joining-the-data",
    "href": "posts/post-with-code/Bike sharing map visualization.html#joining-the-data",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "We will first join the data to have a single file with lattitude and longitude information for start station of bike trip. For that we will perform join operation on start_statn in trips and id in stations.\n\n# we exclude status column because it is present in both data file and will cause an error on join operation.\nstations_non_status = stations[['id','lat','lng','station','municipal']]\n\n# combined data with start station geo spatial information\ntrips_stations = trips.join(stations_non_status.set_index('id'),on='strt_statn')\n\ntrips_stations = trips_stations.loc[trips_stations['municipal'] == 'Boston',:]\n\n\ntrips_stations.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\nlat\nlng\nstation\nmunicipal\ngeometry\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#working-with-geopandas-library",
    "href": "posts/post-with-code/Bike sharing map visualization.html#working-with-geopandas-library",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "Now we will move towards setting up our dataset to have geospatial data required by GeoPandas library. GeoPandas is a python library with extends capability of Pandas by including GeoSpatial data processing and plotting functionality.\nFirst install GeoPandas library if you don’t have it on your computer.\n\n# Installing Geopandas\n#! pip3 install geopandas\n\n\nimport geopandas as gpd\nfrom shapely import Point, Polygon\n\n\ngeo_column = [Point(lng,lat) for lng, lat in zip(trips_stations['lng'],trips_stations['lat'])]\n\nWhat we did in above code was created a list with a Point object for each record in our joined data file. This Point object will provide geospatial data information to GeoPandas to work with.\nNow we will create our GeoPandas dataframe which will have our data with geospatial information.\n\ncrs={'init':'epsg:4326'}\ngdf = gpd.GeoDataFrame(trips_stations,crs=crs,geometry=geo_column)\n\nNow, we created a geopandas dataframe which have all our records from trips_stations dataframe and all those records are associated with a geometric point (e.g., geospatial data).\nEach geopandas dataframe requires one geometry column which has information of geospatial data."
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#plot-geospatial-data",
    "href": "posts/post-with-code/Bike sharing map visualization.html#plot-geospatial-data",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "gdf.plot('strt_statn')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nThe above plot does not show Boston’s map. In order to do that we would need Map files for Boston.\n\nboston = gpd.read_file('./City_of_Boston_Boundary/')\nboston = boston.to_crs({'init': 'epsg:4326'})\nboston.plot()\n\n/Users/htk/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#plotting-geospatial-data-over-bostons-map",
    "href": "posts/post-with-code/Bike sharing map visualization.html#plotting-geospatial-data-over-bostons-map",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "Now we will plot the data of start station of bike trip on the map of Boston.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,8))\nboston.plot(ax=ax)\ngdf.plot(ax=ax,color='red',marker='+',alpha=.5)\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/post-with-code/llama.html",
    "href": "posts/post-with-code/llama.html",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "This post shares the step-by-step process of running the LLAMA2 model locally on MAC. The post is a reflection of my learning of Large Language Models and their practical applications.\n Image by Freepik\nThis post assumes your system already has Python installed.\n\n\n\n\ngit clone https://github.com/ggerganov/llama.cpp.git\nYou can create a virtual environment for setting up llama. I have used here conda commant to create a new environment for my setup.\nconda create -n llama\nconda activate llama\nNext, go to the repository and run the make command.\ncd llama.cpp\nmake\nInstall python packages\npip3 install llama-cpp-python\n\n\n\nWe need llama2 model files. To access these files, a request has to be made by filling out the form given here. On submission, you will get an email providing instructions to download llama2 models.\n\n\n\n\n\n\nTip\n\n\n\nThe email will have an URL which is asked while downloading the models.\n\n\n\n\n\nNow, we have llama2 models and installed llama CPP binding for Python. We will now convert the downloaded model files.\nThe first step in converting the model file is to run the following command while being in the llama.cpp directory. To run this command, we need the path of the directory containing the downloaded models.\npython3 convert.py &lt;directory_containing_llama_model&gt;\nThis command will generate a file with the name ggml-model-f16.gguf and save it in the repository of llama_model which has the downloaded models.\nThe second step will run the quantize command.\n./quantize &lt;directory_containing_llama_model&gt;/ggml-model-f16.gguf &lt;directory_containing_llama_model&gt;/ggml-model-q4_0.gguf q4_0\n\n\n\n\n\n\nImportant\n\n\n\nYou can also use the already converted file available here if you get any error while running above two steps.\n\n\n\n\n\nLangChain makes it easier to develop applications using language models. You can learn about it more here\npip3 install langchain \n\n\n\n\nNow, we will see the fun part of asking a question to llama2 and getting its answer.\nThe following python script has been used from the tutorial available here.\nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nllm = LlamaCpp(model_path='../ggml-model-q4_0.gguf',\n    temperature=0.0,\n    top_p=1,\n    n_ctx=6000,\n    callback_manager=callback_manager, \n    verbose=True,\n)\n\nquestion = \"who is mr. narendra modi?\"\nanswer = llm(question)\n\nprint('Q:',question)\nprint('A:',answer)\nOutput:\nMr. Narendra Modi is the current Prime Minister of India, serving since May 2014. He is known ...\n\n\n\n\nhttps://medium.com/@karankakwani/build-and-run-llama2-llm-locally-a3b393c1570e\nhttps://github.com/facebookresearch/llama-recipes/blob/main/demo_apps/HelloLlamaLocal.ipynb"
  },
  {
    "objectID": "posts/post-with-code/llama.html#installing-required-libraries",
    "href": "posts/post-with-code/llama.html#installing-required-libraries",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "git clone https://github.com/ggerganov/llama.cpp.git\nYou can create a virtual environment for setting up llama. I have used here conda commant to create a new environment for my setup.\nconda create -n llama\nconda activate llama\nNext, go to the repository and run the make command.\ncd llama.cpp\nmake\nInstall python packages\npip3 install llama-cpp-python\n\n\n\nWe need llama2 model files. To access these files, a request has to be made by filling out the form given here. On submission, you will get an email providing instructions to download llama2 models.\n\n\n\n\n\n\nTip\n\n\n\nThe email will have an URL which is asked while downloading the models.\n\n\n\n\n\nNow, we have llama2 models and installed llama CPP binding for Python. We will now convert the downloaded model files.\nThe first step in converting the model file is to run the following command while being in the llama.cpp directory. To run this command, we need the path of the directory containing the downloaded models.\npython3 convert.py &lt;directory_containing_llama_model&gt;\nThis command will generate a file with the name ggml-model-f16.gguf and save it in the repository of llama_model which has the downloaded models.\nThe second step will run the quantize command.\n./quantize &lt;directory_containing_llama_model&gt;/ggml-model-f16.gguf &lt;directory_containing_llama_model&gt;/ggml-model-q4_0.gguf q4_0\n\n\n\n\n\n\nImportant\n\n\n\nYou can also use the already converted file available here if you get any error while running above two steps.\n\n\n\n\n\nLangChain makes it easier to develop applications using language models. You can learn about it more here\npip3 install langchain"
  },
  {
    "objectID": "posts/post-with-code/llama.html#building-applications-on-the-top-of-a-language-model",
    "href": "posts/post-with-code/llama.html#building-applications-on-the-top-of-a-language-model",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "Now, we will see the fun part of asking a question to llama2 and getting its answer.\nThe following python script has been used from the tutorial available here.\nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nllm = LlamaCpp(model_path='../ggml-model-q4_0.gguf',\n    temperature=0.0,\n    top_p=1,\n    n_ctx=6000,\n    callback_manager=callback_manager, \n    verbose=True,\n)\n\nquestion = \"who is mr. narendra modi?\"\nanswer = llm(question)\n\nprint('Q:',question)\nprint('A:',answer)\nOutput:\nMr. Narendra Modi is the current Prime Minister of India, serving since May 2014. He is known ..."
  },
  {
    "objectID": "posts/post-with-code/llama.html#references",
    "href": "posts/post-with-code/llama.html#references",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "https://medium.com/@karankakwani/build-and-run-llama2-llm-locally-a3b393c1570e\nhttps://github.com/facebookresearch/llama-recipes/blob/main/demo_apps/HelloLlamaLocal.ipynb"
  },
  {
    "objectID": "Awards.html",
    "href": "Awards.html",
    "title": "Awards",
    "section": "",
    "text": "Special Research Award for Combining Teaching Methodology and Technology\nEstonian Youth Board | Estonia  Link Research, Education\n\nThe award was given under the ITL Ustus Agur Scholarship for my research activities.\n\n\n\nBest Educational Technology Demonstration Award\nInternational Society of Learning Analytics Research | USA Link Research, Technology, Education\n\nThe award was given at the International Conference of Learning Analytics and Knowledge, Texas, Usa (2023) (paper acceptance rate: ~30%).\nThe award signifies the importance of the research project (i.e., CoTrack) for the education technology community.\n\n\n\nBest Paper Nominee\nInternational Society of Learning Analytics Research | USA  Paper-link  Research\n\nRecognized in Top 5 research papers in short paper category at the International Conference of Learning Analytics and Knowledge, Texas, Usa (2023).\n\n\n\nEC-TEL Scholarship\nEuropean Association of Technology-Enhanced Learning | EU  Research\n\nAwarded by European Association of Technology-Enhanced Learning for attending EC-TEL summer school in Greece in 2022.\n\n\n\nDora Plus Scholarship for Foreign Doctoral Students\nEuropean Regional Development Fund | Estonia  Research\n\nAwarded to Ph.D. students based on their academic performance.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/gramian.html",
    "href": "posts/post-with-code/gramian.html",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "In this post, I will share my learning on this topic. I have been searching for a way to use a convolutional neural network (CNN) for my time-series data and this search led me to Gramian Angular Summation/Difference Fields (GASF/GADF). This post will give you a detailed introduction to Gramian Angular Field and show you ‘how to convert time-series data into image’ with a help of an example.\n\n\nBefore starting the introduction, I think first we should get our self familiar with the basic concepts of GASF/GADF. If you are already aware then feel free to skip this section. Cartesian coordinates: You may have most likely seen this in your earlier mathematics classes. In this scheme, the position of a point (or other geometrical shapes) is determined by one or more numbers. For example, if we take two-dimensional coordinate systems then a position is determined with a pair of numbers, e.g., (2,3). The position is then presented with a point on distance from two reference lines, known as the x-axis and y-axis.\n\n\n\nImagine a circle with origin as center drawn on the image above shown image in a way that it crosses point (2,3). Now, we will take the radius of this circle and the angle between a line connecting (0,0) to (2,3) and X-axis. These two numbers now represent the same position but using a polar coordinate system. Taking the above example, the point will not be represented as (3.6, 56.3)\n\n\n\n\nLet’s say we have a set of vectors V. The Gramian (Gram) matrix is a matrix of inner products of every pair of vectors from V. As you can see in the below image, each element in the matrix &lt;vi,vj&gt; is vector production between vectors vi and vj.\n\n\n\n\n\nNow we will move towards the main goal of this post that is understanding the process of representing a time-series is represented in an image. In short, you can understand the process in three following steps. \n\n\n\nAggregate the time series by taking the mean of every M points to reduce the size. This step uses Piecewise Aggregation Approximation (PAA).\nScaling values in the interval [0,1].\nGenerating polar coordinates by taking timestamp as radius and inverse cosine (or arccosine) of scaled value. This will give us the value of the angle. Generate Gramian summation/difference angular fields. In this step, every pair values are summed (subtracted) and then cosine is taken on the summed value.\n\nIn case, if you couldn’t understand some part of the process no worries, we will see each step in detail below.\n\n\n\nI am providing here an example in python to demonstrate the state by step process of converting a time series into an image using gramian angular field.\n\n\nfrom pyts.approximation import PiecewiseAggregateApproximation\nfrom pyts.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nX = [[1,2,3,4,5,6,7,8],[23,56,52,46,34,67,70,60]]\nplt.plot(X[0],X[1])\nplt.title('Time series')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\n# PAA\ntransformer = PiecewiseAggregateApproximation(window_size=2)\nresult = transformer.transform(X)\n\n# Scaling in interval [0,1]\nscaler = MinMaxScaler()\nscaled_X = scaler.transform(result)\nplt.plot(scaled_X[0,:],scaled_X[1,:])\nplt.title('After scaling')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\narccos_X = np.arccos(scaled_X[1,:])\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(result[0,:], arccos_X)\nax.set_rmax(2)\nax.set_rticks([0.5, 1, 1.5, 2]) # Less radial ticks\nax.set_rlabel_position(-22.5) # Move radial labels away from plotted line\nax.grid(True)\nax.set_title(\"Polar coordinates\", va='bottom')\nplt.show()\n# Gramian angular summation fields\nfield = [a+b for a in arccos_X for b in arccos_X]\ngram = np.cos(field).reshape(-1,4)\nplt.imshow(gram)\n\n\n\n\n\n\n\nSome additional explanation\n\n\n\nThe aforementioned steps are for illustrating the process of converting time series into an image using Gramian Angular Summation/Difference Field. In practice, we don’t need to compute polar coordinates thanks to trigonometry (following rules). \\[\ncos(A+B) = cos(A)cos(B) + sin(A)sin(B)\n\\] \\[\n1 = sin^2(A) + cos^2(B)\n\\] For computing Cos(A+B) in Gramian Angular Field computation, we expand it as following\n\\[\ncos(A+B) = cos(A)cos(B) + sin(A)sin(B)\n\\] \\[\n= cos(A)cos(B) + \\sqrt(1- cos^2(A)) \\sqrt(1- cos^2(B))\n\\] \\[\n= x_a * x_b + \\sqrt(1- x_a^2) \\sqrt(1- x_b^2)\n\\] Because we computed A and B by taking cosine inverse of time series value (actually on values after PAA and scaling).\nYou can checkout this python library pyts.\n\n\n\n\n\n\n\n\nWang, Z., & Oates, T. (2015). Imaging time-series to improve classification and imputation. IJCAI International Joint Conference on Artificial Intelligence, 2015-January, 3939–3945.\nEamonn J Keogh and Michael J Paz- zani. Scaling up dynamic time warping for datamining applications. In Proceedings ofthe sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 285– 289. ACM, 2000.\nhttps://pyts.readthedocs.io/en/stable/_modules/pyts/image/gaf.html#GramianAngularField\nhttps://pyts.readthedocs.io/en/stable/index.html"
  },
  {
    "objectID": "posts/post-with-code/gramian.html#basic-concepts",
    "href": "posts/post-with-code/gramian.html#basic-concepts",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "Before starting the introduction, I think first we should get our self familiar with the basic concepts of GASF/GADF. If you are already aware then feel free to skip this section. Cartesian coordinates: You may have most likely seen this in your earlier mathematics classes. In this scheme, the position of a point (or other geometrical shapes) is determined by one or more numbers. For example, if we take two-dimensional coordinate systems then a position is determined with a pair of numbers, e.g., (2,3). The position is then presented with a point on distance from two reference lines, known as the x-axis and y-axis.\n\n\n\nImagine a circle with origin as center drawn on the image above shown image in a way that it crosses point (2,3). Now, we will take the radius of this circle and the angle between a line connecting (0,0) to (2,3) and X-axis. These two numbers now represent the same position but using a polar coordinate system. Taking the above example, the point will not be represented as (3.6, 56.3)\n\n\n\n\nLet’s say we have a set of vectors V. The Gramian (Gram) matrix is a matrix of inner products of every pair of vectors from V. As you can see in the below image, each element in the matrix &lt;vi,vj&gt; is vector production between vectors vi and vj."
  },
  {
    "objectID": "posts/post-with-code/gramian.html#gramian-angular-fields",
    "href": "posts/post-with-code/gramian.html#gramian-angular-fields",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "Now we will move towards the main goal of this post that is understanding the process of representing a time-series is represented in an image. In short, you can understand the process in three following steps. \n\n\n\nAggregate the time series by taking the mean of every M points to reduce the size. This step uses Piecewise Aggregation Approximation (PAA).\nScaling values in the interval [0,1].\nGenerating polar coordinates by taking timestamp as radius and inverse cosine (or arccosine) of scaled value. This will give us the value of the angle. Generate Gramian summation/difference angular fields. In this step, every pair values are summed (subtracted) and then cosine is taken on the summed value.\n\nIn case, if you couldn’t understand some part of the process no worries, we will see each step in detail below.\n\n\n\nI am providing here an example in python to demonstrate the state by step process of converting a time series into an image using gramian angular field.\n\n\nfrom pyts.approximation import PiecewiseAggregateApproximation\nfrom pyts.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nX = [[1,2,3,4,5,6,7,8],[23,56,52,46,34,67,70,60]]\nplt.plot(X[0],X[1])\nplt.title('Time series')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\n# PAA\ntransformer = PiecewiseAggregateApproximation(window_size=2)\nresult = transformer.transform(X)\n\n# Scaling in interval [0,1]\nscaler = MinMaxScaler()\nscaled_X = scaler.transform(result)\nplt.plot(scaled_X[0,:],scaled_X[1,:])\nplt.title('After scaling')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\narccos_X = np.arccos(scaled_X[1,:])\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(result[0,:], arccos_X)\nax.set_rmax(2)\nax.set_rticks([0.5, 1, 1.5, 2]) # Less radial ticks\nax.set_rlabel_position(-22.5) # Move radial labels away from plotted line\nax.grid(True)\nax.set_title(\"Polar coordinates\", va='bottom')\nplt.show()\n# Gramian angular summation fields\nfield = [a+b for a in arccos_X for b in arccos_X]\ngram = np.cos(field).reshape(-1,4)\nplt.imshow(gram)\n\n\n\n\n\n\n\nSome additional explanation\n\n\n\nThe aforementioned steps are for illustrating the process of converting time series into an image using Gramian Angular Summation/Difference Field. In practice, we don’t need to compute polar coordinates thanks to trigonometry (following rules). \\[\ncos(A+B) = cos(A)cos(B) + sin(A)sin(B)\n\\] \\[\n1 = sin^2(A) + cos^2(B)\n\\] For computing Cos(A+B) in Gramian Angular Field computation, we expand it as following\n\\[\ncos(A+B) = cos(A)cos(B) + sin(A)sin(B)\n\\] \\[\n= cos(A)cos(B) + \\sqrt(1- cos^2(A)) \\sqrt(1- cos^2(B))\n\\] \\[\n= x_a * x_b + \\sqrt(1- x_a^2) \\sqrt(1- x_b^2)\n\\] Because we computed A and B by taking cosine inverse of time series value (actually on values after PAA and scaling).\nYou can checkout this python library pyts."
  },
  {
    "objectID": "posts/post-with-code/gramian.html#references",
    "href": "posts/post-with-code/gramian.html#references",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "Wang, Z., & Oates, T. (2015). Imaging time-series to improve classification and imputation. IJCAI International Joint Conference on Artificial Intelligence, 2015-January, 3939–3945.\nEamonn J Keogh and Michael J Paz- zani. Scaling up dynamic time warping for datamining applications. In Proceedings ofthe sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 285– 289. ACM, 2000.\nhttps://pyts.readthedocs.io/en/stable/_modules/pyts/image/gaf.html#GramianAngularField\nhttps://pyts.readthedocs.io/en/stable/index.html"
  },
  {
    "objectID": "posts/post-with-code/statistical_test.html",
    "href": "posts/post-with-code/statistical_test.html",
    "title": "Statistical tests in R",
    "section": "",
    "text": "This post delves into inferential statistics and explains two different statistical test. Additionally, this post also talks about the need of performing these tests along with their suitability based on available data.\nIn particular, this post tackles the following three questions.\n\nWhy we do the statistical test?\nHow to do it?\nWhen to do which test?\n\n\nWhy we do the statistical test?\nAs we have already discussed that we often do not have access to the entire population. Instead, we have access to a small portion of the population known as a sample. We use this sample to infer knowledge about the population.\nFollowing are a few examples to explain it further.\n\nWe collected a sample of height data from students studying in university X. We computed the average of our sample and obtained the average height of students in our sample. Now we want to test whether the average height found in our sample is the same for the population.\nWe collected two samples from two universities’ students’ salaries. We want to see is there any difference between the average salary between two universities or not.\nWe collected two samples of test scores from the same classroom at the start of the semester and end of semester. We want to see students whether the students’ test scores’ were improved significantly or not.\nWe collected a salary sample of people working in Tallinn with their education levels (e.g., primary, secondary, bachelors, master, doctorate). We want to test is there any significant difference in salary among these various levels of education levels.\n\nAll these aforementioned examples illustrate some of the cases where we can apply the statistical test to test our assumption or guess (aka. hypothesis). One thing to note here is we are not just interested in finding knowledge for our sample, we want to rather use it to gain knowledge about the population.\n\nThe above diagram gives a pictorial representation of what we have just talked about. There are two classes- A and B. Let’s say there are 100 and 80 students in class A and B, respectively. We collected a sample of student’s test scores data from both classes. In our sample, we have test scores for only a few students (let’s say 35-40). We computed the average for both the samples and found that the sample-1 mean is higher than sample-2 mean. This is what we learned from our sample. Now, we want to see is it also the same for the population (classroom-A average is higher than classroom-B average test scores or not). Here comes the statistical test. We use them to infer knowledge about the population based on collected sample data.\n\n\nHow to do it?\nWe will cover one sample t-test, independent sample t-test, paired t-test, and ANOVA in R.\n\nIndependent sample t-test\nWe do this test when we have two independent samples and we want to compare a statistic for these two groups. For example, we want to compare the average salary of students working part-time from two different universities. In this case, we have two samples collected from different universities. These samples are independent because they contain data of different people.\nMake sure the following assumption in your dataset when you do a t-test\n\nContinuous dependent variable.\nCategorical independent variable for grouping.\nIndependent data samples\nDependent variable has a normal distribution.\nEach group has the same variance.\n\nWe will look at the above assumptions with the help of an example. Let’s say we have test scores from two classes: A and B. Now we want to test whether the difference between these two classes’ test scores is significant or not.\n\n\n\nClass\nTestScore\n\n\n\n\nA\n99\n\n\nA\n93\n\n\nA\n81\n\n\nA\n92\n\n\nA\n89\n\n\nA\n80\n\n\nA\n88\n\n\nA\n81\n\n\nA\n100\n\n\nA\n82\n\n\nB\n98\n\n\nB\n66\n\n\nB\n86\n\n\nB\n62\n\n\nB\n78\n\n\nB\n87\n\n\nB\n77\n\n\nB\n60\n\n\nB\n60\n\n\nB\n79\n\n\n\nYou can see the dataset has one continuous variable (TestScore: Numeric type) and one class variable as the grouping variable. This class variable tells the classroom from which the scores are collected.\nAssumptions\n\nContinuous dependent variable: the dataset has continuous dependent variable (testScore).\nCategorical independent variable: the dataset has a class variable which is a categorical variable(a variable that has categories of value).\nThe independence assumption can only ensure while data collection. We assume here that the data were collected independently.\nNormal distribution: We can plot the test scores for each class and check for bell shape.\nlibrary(lattice)\nlibrary(rio)\n\n# load dataset\ndata &lt;- import('data-file-name')\n\n# plot the distribution\ndensityplot(~ data$TestScore|data$Class)\n\nSame variance in both groups (class A and B): We will compute the variance in both groups. We can use the dplyr package for computing variance class-wise.\ndata %&gt;% group_by(Class) %&gt;% summarise(var(TestScore))\nThe variance for Class A and B are following\n\n\n\nClass\nVariance\n\n\n\n\nA\n55.8\n\n\nB\n169\n\n\n\nAs we can see the variance is not the same and the difference is not small.\n\nIf the variance is the same, we will use Student’s T-test.\nIf the variance is not the same then we will use Welsh T-test.\n\nIn our dataset, the variance is not the same therefore we will apply Welsh t-test.\n\n\nPerforming the test\nWe will first set up our hypothesis. We want to test that is there any difference between the test scores of class A and B. We will follow the below steps\n\nState Null and alternative hypothesis\nDecide significance level \\(\\alpha\\)\nPerform test\nCheck the p-value and decide ‘whether to reject the null hypothesis or not’\n\nNow, let’s talk about each step in detail.\nThe first step is to formulate the null and alternative hypotheses. What are those? We specify what we want to test. For example, in our class’s test scores dataset, we want to test that the average test scores from two groups are different. It will become our alternative hypothesis. The null hypothesis is a hypothesis of no difference. In other words, the opposite of what we want to test. The below diagram is showing our null and alternative hypotheses.\n\n\n\nhyp\n\n\nThe second step to decide the level of significance (\\(\\alpha\\)). You can choose one of the following: 1%, 1%, 5%. For our example, let’s select as 5% or .05.\nThe third step is to perform the test and obtain the p-value.\n\np-value =&lt; \\(\\alpha\\) : \\(H_1\\) hypothesis\np-value &gt; \\(\\alpha\\) : \\(H_0\\) hypothesis\n\nWe will perform the t-test on our dataset in R.\nt.test(data$TestScore ~ data$Class, var.equal = FALSE)\nWe use t.test() function from R. We have specified the test score and class variable. Here, the test score is the dependent variable and class is an independent variable. We have also specified that the variance of groups is not equal. It automatically selects Welsh t-test when we tell that the variance among groups is not the same.\n\n\n\nScreenshot 2021-03-09 at 12.27.02\n\n\nIn the results, we can see p-value. We will compare it with \\(\\alpha\\). We can see that the p-value is less than .05 (or our selected level of significance). Hence, we say that null hypothesis is rejected and the alternative hypothesis can be accepted. From the results, we can also see that group-A mean is higher than group-B. We can say that the average test score from class A is significanlty higher than the average test score from class B.\n\n\n\nOne sample t-test\nThis is the most basic scenario for a t-test. In our list of examples, the first example is where we use one-sample t-test. So if you have a single sample and you want to test whether the statistic (e.g., mean, variance, standard deviation) obtained from the sample is population statistic or not.\nFor this test, we will use women dataset from the R datasets package. This dataset has two attributes- height and weight of American women. We want to see whether the average height of American women is 65 (this is what we got from our sample) or not.\nSo here we set up the following hypothesis\n\nH_0 : Average height is same as 65 (hypothesis of no difference)\nH_1: Average height is not the same as 65.\n\nDon’t forget to test the assumptions for the t-test. In this case, we will go for testing the distribution is normal or not. You can plot the histogram or density plot and check for bell shape.\nWe then set the level of significance as 5% (or .05)\nLet’s perform the test now.\n# load datasets package\nlibrary(datasets)\n\nt.test(women$height,mu=65)\n\n\n\nScreenshot 2021-03-09 at 14.03.46\n\n\nFrom our results from the t-test, we can see that the p-value is not less than .05 which means we can not reject our null hypothesis.\n\n\nPaired t-test\nLet’s think about the third example which we have discussed in the start.\n\nWe collected two samples of test scores of same classroom at the start of the semester and end of the semester. We wanted to test whether the students’ test scores’ were improved significantly or not.\n\nFor this exercise, we have the following dataset of test scores of the same students at the start of the lecture and end of the lecture.\n\n\n\ntest-1\ntest-2\n\n\n\n\n175\n296\n\n\n329\n376\n\n\n238\n309\n\n\n60\n222\n\n\n271\n150\n\n\n291\n316\n\n\n364\n321\n\n\n402\n447\n\n\n70\n220\n\n\n335\n375\n\n\n300\n310\n\n\n245\n310\n\n\n186\n282\n\n\n31\n317\n\n\n104\n362\n\n\n132\n338\n\n\n94\n263\n\n\n38\n138\n\n\n62\n329\n\n\n139\n292\n\n\n94\n275\n\n\n48\n150\n\n\n68\n319\n\n\n138\n300\n\n\n\nSo data were collected from the same students. In this case, we will apply the paired t-test to test whether there is any improvement in students’ test scores after attending the lecture or not.\nWe first need to test the assumptions (Skipped for it. You can check the assumptions here)\nWe will set-up our hypothesis\n\nH_0 : There is no difference in students’ test performance before and after the lecture (null hypothesis)\nH_1 : There’s is a difference in students’ test performance before and after the lecture.\n\nThen, we would set the level of significance as 5% (.05).\nWe will perform the test now using the same function but this time we specify the paired parameter as TRUE.\n\n\n\nScreenshot 2021-03-09 at 14.25.27\n\n\nIn the results, if we look at the p-value and that is .00000894. This value is smaller than \\(\\alpha\\) which means we can reject the null hypothesis. We accept the alternative hypothesis.\n\n\nANOVA\nANOVA or ANalysis Of VAriance test has its usage in a different scenario and here we are going to talk about the most basic one. We use an independent sample t-test when we have only two groups but when we have more than two groups we employ ANOVA test.\nLet’s take an example. We have a diet dataset (you can download it from here). This dataset has attributes person id, gender, height, diet, weight before taking diet, weight after taking the diet. There were three different diets were given. We want to test whether the diet has any impact on weight loss or not.\nSo first we compute the weight loss for each participant.\n&gt; data &lt;- import('anova_test_dataset_diet.csv')\n&gt; str(data)\n'data.frame':   78 obs. of  7 variables:\n $ Person      : int  25 26 1 2 3 4 5 6 7 8 ...\n $ gender      : int  NA NA 0 0 0 0 0 0 0 0 ...\n $ Age         : int  41 32 22 46 55 33 50 50 37 28 ...\n $ Height      : int  171 174 159 192 170 171 170 201 174 176 ...\n $ pre.weight  : int  60 103 58 60 64 64 65 66 67 69 ...\n $ Diet        : int  2 2 1 1 1 1 1 1 1 1 ...\n $ weight6weeks: num  60 103 54.2 54 63.3 61.1 62.2 64 65 60.5 ...\ndata$weightLoss &lt;- data$pre.weight - data$weight6weeks\nNow we will consider two attributes for our ANOVA test: weightLoss and type of diet.\nFollowing are assumptions for the ANOVA test\n\nThe dependent variable (weightLoss in our example) is normally distributed in each group (diet types in our example).\nThe variance of the dependent variable is the same for all groups of the independent variable.\nIndependence data samples.\n\nWe use aov() function in R to perform the ANOVA test.\n\n\n\nScreenshot 2021-03-09 at 15.00.15\n\n\nHere, you need to look for Pr(&gt;F) column that is p-value for the ANOVA test. We can see this value is smaller than .05 (5% level of significance) and even also than .01 (1% level of significance). We can say the null hypothesis is rejected. You can refer to this link for a detailed analysis of the same.\n\n\n\nWhen to do which test?\nYou might be already wondering why we have so many tests and how would I know when to apply which test. To simplify it, you can refer to the following\n\nYou have a single variable (continuous type or numeric in R) that you want to investigate and you are interested in finding some knowledge about the population on that variable.\nOne sample t-test\n\n\nYou have two variables- one continuous and another with categories. The number of categories is two.\nIndependent Sample t-test\n\n\nYou have the same scenario as above but now you have a second variable with more than two categories.\nANOVA\n\n\nYou have two variables (both continuous) containing data collected from the same participants.\nPaired t-test\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/facial-features.html",
    "href": "posts/post-with-code/facial-features.html",
    "title": "Facial feature extraction using OpenFace",
    "section": "",
    "text": "In this post, I will discuss the work I have been doing recently. I needed to extract facial features from the recorded video and for this task, I decided to use OpenFace, an open-source face recognition library. In this post, I am sharing the installation process and tutorial on detecting facial landmarks.\n\n\nInstallation\nI tried to install OpenFace on Mac OS but couldn’t succeed. There were a lot of errors and compatibility issues that I couldn’t get through. Therefore, I decided to install it on Ubuntu. For that, I installed a Virtual box on Mac and installed Ubuntu 18.04.\nTo install OpenFace, I followed the steps given here\nsudo apt-get update\nsudo apt-get install build-essential\nsudo apt-get install g++-8\n\nsudo apt-get install cmake\n\nsudo apt-get install git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\n\nsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-22-dev\n\nwget https://github.com/opencv/opencv/archive/4.1.0.zip\n\nsudo unzip 4.1.0.zip\ncd opencv-4.1.0\nmkdir build\ncd build\n\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_TIFF=ON -D WITH_TBB=ON ..\nmake -j2\nsudo make install\n\nwget http://dlib.net/files/dlib-19.13.tar.bz2\ntar xf dlib-19.13.tar.bz2\ncd dlib-19.13\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release\nsudo make install\nsudo ldconfig\ncd ../..\n\nsudo apt-get install libboost-all-dev\n\ngit clone https://github.com/TadasBaltrusaitis/OpenFace.git\n\ncd OpenFace\nmkdir build\ncd build\n\ncmake -D CMAKE_CXX_COMPILER=g++-8 -D CMAKE_C_COMPILER=gcc-8 -D CMAKE_BUILD_TYPE=RELEASE ..\nmake\nAt this point, we have installed OpenFace. Now we need to download models. You can either download it manually or use a script provided in the OpenFace library.\nManual download links.\n\nscale 0.25\nscale 0.35\nscale 0.50\nscale 1.00\n\nI ran the following command to ran the script to download the model.\ncd ..\nsh ./download_models.sh\nThe script will download models in the directory OpenFace/lib/local/LandmarkDetector/model/patch_experts.\nAfter completing this process, I ran the demo program by running the following command.\n./bin/FaceLandmarkVid -f \"\"../samples/changeLighting.wmv\"\" -f \"\"../samples/2015-10-15-15-14.avi\n\n\n\n\n\n\nExecution error\n\n\n\nI got an error CEN patch expert not found. The command was searching the models in the OpenFace/build/bin/model/patch_experts.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nI copied the files (cen_patches_0.25_of.dat,cen_patches_0.35_of.dat,cen_patches_0.50_of.dat,cen_patches_1.00_of.dat) in OpenFace/build/bin/model/patch_experts directory.\n\n\n\n\nRunning Demo\nIf you have a video with a single face, you can use FaceLandmarkVid or in case of multiple faces, you can use FaceLandmarkVidMulti\n\n\n\n\n\n\nNote\n\n\n\nThese files will be available in OpenFace/build/bin directory. Either you can specify the full path to facial landmark detector or cd to the bin directory and run the following command.\n\n\nFaceLandmarkVid -f file_name\nFollowing is the demonstration of OpenFace on a video clip with single face.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/dash.html",
    "href": "posts/post-with-code/dash.html",
    "title": "Introduction to Python Dash Framework",
    "section": "",
    "text": "Dashboards play a crucial role in conveying useful and actionable information from collected data regardless of context. As with economically feasible sensors, improved computation, storage facility, and IoT framework, it has become easier to collect an enormous amount of data from a variety of contexts (e.g. military, health-care, education). However, finding insights from collected data remains a challenge. This post offers an introduction to Python Dash a framework that is a great option for dashboard development. I personally really like this framework. It allows me to process my data along with its visualization through a web-based application."
  },
  {
    "objectID": "posts/post-with-code/dash.html#dash-framework",
    "href": "posts/post-with-code/dash.html#dash-framework",
    "title": "Introduction to Python Dash Framework",
    "section": "Dash Framework",
    "text": "Dash Framework\ndash uses plotly graphics library for plotting graphs. In addition to their graphic feature, these graphs also have a lot of good features (e.g. automatic scaling of axis for timestamp data, etc). Please refer dash documentation for details.\n\nInstallation\npip install dash-html-components==0.14.0  # HTML components\npip install dash-core-components==0.44.0  # dash core components\npip install dash-table==3.6.0  # Interactive DataTable component (new!)\n\n\nFirst dash application\nNow, we are going to develop our first dash application. For this application, we are going to plot the following data (some records are taken from here. This data has two attributes (House price and area).\nA dash application can have number of components (e.g. div block, table, headings, graph). In our application, we are going to use two components - heading and a graph. Let’s begin developing it. First of all, we need to import the required packages\nimport dash_core_components as dcc\nimport dash_html_components as html\nThe first package is used to create an object of dash application. Second package dash_core_components provides graph components and the third package allows us to include html components (heading, input box) in our application.\nNext, we need to create a dash application.\napp = dash.Dash(__name__)\nname is a special variable in python which contains name of current module. For instance, when you run your python program using command prompt then it contains main.\nNow, we will create components to embed in our application. Just to have a clear idea, we want to create following structured with our application.\n\n\n First Dash Application \n\n graph here.. \n\n\nFor components, we will use dash_html_components and for graph, we will use dash_core_components.\nlist_of_data = [{\n    'y':[114300,114200,114800,94700,119800,114600],\n    'x':[1790,2030,1740,1980,2130,1780],\n    'type':'bar'\n}]\n\nbody = html.Div([\n    html.H2(\"\"First Dash Application\"\"),\n    dcc.Graph(id='first',\n        figure={'data':list_of_data})\n])\nIn the above code, we created the body of our application which is a div block. In this block, we created one H2 heading component and one Graph component. Graph has a attribute figure where we have specified the data to be plotted. The data (list_of_data) is actually a list of dictionaries (It might seems a bit confusing but you will be good after writing some codes). One important thing-&gt; we used ‘type’:‘bar’ which specify that we want to plot Bar chart.\nNext, we will set the layout of the application.\napp.layout = html.Div([body])\nFinally, we will start the server\nif __name__ == \"\"__main__\"\":\n    app.run_server(debug=True)\nYou can download this script from here.\nNow, we will execute our application. The execution of our application will start a server on the port 8050\n\n\n\nRunning the server\n\n\nIn order to see the output of the program, open your browser and type http://127.0.0.1:8050 in the address bar. It will show you the following screen\n\n\n\np6.3.png\n\n\nCheck for more components: dash_core_components, dash_html_components.\n\n\nAdding CSS style to the app\nThe next step towards generating a beautiful dashboard is to add a styling feature. We can use css stylesheet in our application. It can be specified at the time of creating an application.\napp = dash.app = dash.Dash(__name__,external_stylesheets=style)\nWith the above method, you can only add css which are available online. In case if you want to add your local css file, follow the given steps\n\nCreate a folder with name asset in the same directory where your dash script is stored.\nCopy your stylesheet in the asset folder.\nAdd the path in your program\n\ndash.Dash(__name__,external_stylesheets=[\"\"\\asset\\css-file-name\"\"])\n\n\nInstalling Bootstrap component for dash\ndash also supports Bootstrap which is a widely used css library. It can be added in your dash application using dash-bootstrap-component package (complete documentation). This package allows an easy integration of Bootstrap in the dash application.\nYou can install it using the following command.\npip install dash-bootstrap-components\nNow, let’s use it to add a CSS to our first example. We are going to create the following layout for our application. We will utilize Bootstrap’s grid system for structuring our components.\n\n\n\nboot.png\n\n\nFirst, we need to import dash_bootstrap_components in our previous example.\nNext, we will add bootstrap css to your program and then we will create our layout.\nimport dash-bootstrap-components as dbc\napp = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# column1 content\ncolumn_1 = [\n    html.H2(\"\"House Prices\"\"),\n    html.P(\"\"This is a demo application which shows house prices with house area.\"\"\n    ),\n    dbc.Button(\"\"Details\"\", color=\"\"secondary\"\"),\n]\n\n# column content\ncolumn_2 = [\n    html.H2(\"\"Graph\"\"),\n    dcc.Graph(id='first',\n        figure={'data':list_of_data}),\n]\n\n# Creating layout\nbody = dbc.Container(\n    [   html.H1('With Bootstrap'),\n        html.Hr(),\n        dbc.Row(\n            [\n                dbc.Col(column_1,md=4),\n                dbc.Col(column_2,md=8),\n            ]\n        )\n    ]\n)\n\n# Adding CSS\n# dash-bootstrap-components has CDN for bootstrap css in dbc.themes.BOOTSTRAP\napp = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP])\nYou can chek the above source code here."
  },
  {
    "objectID": "posts/post-with-code/agreement.html",
    "href": "posts/post-with-code/agreement.html",
    "title": "Computing inter-rater aggrement scores using Python",
    "section": "",
    "text": "Recently, I was involved in some annotation processes involving two coders and I needed to compute inter-rater reliability scores. There are multiple measures for calculating the agreement between two or more than two coders/annotators.\n\n\n\n\nIf you have a question regarding ““which measure to use in your case?”“, I would suggest reading (Hayes & Krippendorff, 2007) which compares different measures and provides suggestions on which to use when.\n\n\n\n\nIn this post, I am sharing some of our python code on calculating various measures for inter-rater reliability.\n\n\n\n\nCohen’s Kappa\n\n\n\n\nWe will start with Cohen’s kappa. Let’s say we have two coders who have coded a particular phenomenon and assigned some code for 10 instances. Now let’s write the python code to compute cohen’s kappa.\n\n\n\n\nYou can use either sklearn.metrics or nltk.agreement to compute kappa. We will see examples using both of these packages.\n\nfrom sklearn.metrics import cohen_kappa_score\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\ncoder2 = [1,1,0,0,1,1,2,1,1,0]\nscore = cohen_kappa_score(coder1,coder2)\n\nprint('Cohen\\'s Kappa:',score)\n\n# output\n# Cohen's Kappa: 0.3220338983050848\n\n\nIn order to use nltk.agreement package, we need to structure our coding data into a format of [coder, instance, code]. For instance, the first code in coder1 is 1 which will be formatted as [1,1,1] which means coder1 assigned 1 to the first instance.\n\n\n\n\nLet’s convert our codes given in the above example in the format of [coder,instance,code]. Here we have two options to do that. I have included the first option for better understanding. Second option is a short one line solution to our problem.\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\n\ncoder1_new = []\ncoder2_new = []\nfor i in range(len(coder1)):\n    coder1_new.append([1,i,coder1[i]])\n    coder2_new.append([2,i,coder2[i]])\n\n\nformatted_codes = coder1_new + coder2_new\nprint(formatted_codes)\n[[1, 0, 1], [1, 1, 0], [1, 2, 2], [1, 3, 0], [1, 4, 1], [1, 5, 1], [1, 6, 2], [1, 7, 0], [1, 8, 1], [1, 9, 1], [2, 0, 1], [2, 1, 1], [2, 2, 0], [2, 3, 0], [2, 4, 1], [2, 5, 1], [2, 6, 2], [2, 7, 1], [2, 8, 1], [2, 9, 0]]\nformatted_codes = [[1,i,coder1[i]] for i in range(len(coder1))] + [[2,i,coder2[i]] for i in range(len(coder2))] \nprint(formatted_codes)\n\n\n[[1, 0, 1], [1, 1, 0], [1, 2, 2], [1, 3, 0], [1, 4, 1], [1, 5, 1], [1, 6, 2], [1, 7, 0], [1, 8, 1], [1, 9, 1], [2, 0, 1], [2, 1, 1], [2, 2, 0], [2, 3, 0], [2, 4, 1], [2, 5, 1], [2, 6, 2], [2, 7, 1], [2, 8, 1], [2, 9, 0]]\n\n\n\nNow, we have our codes in the required format, we can compute cohen’s kappa using nltk.agreement.\n\n\nfrom nltk import agreement\n\nratingtask = agreement.AnnotationTask(data=formatted_codes)\n\nprint('Cohen\\'s Kappa:',ratingtask.kappa())\n\n\nCohen's Kappa: 0.32203389830508466\n\n\n\nCohen’s Kappa using CSV files\n\n\n\n\nIn this section, we will see how to compute cohen’s kappa from codes stored in CSV files. So let’s say we have two files (coder1.csv, coder2.csv). Each of these files has some columns representing a dimension. Below is the snapshot of such a file.\n\n\n\n\nThe files contain 10 columns each representing a dimension coded by first coder. We have a similar file for coder2 and now we want to calculate Cohen’s kappa for each of such dimensions.\n\n\n\n\n\nSMU\nCF\nKE\nARG\nSTR\nCO\nu1\nu2\nu3\nu4\n\n\n\n\n1\n1\n1\n0\n0\n1\n2\n1\n1\n0\n\n\n1\n1\n1\n0\n0\n2\n1\n2\n1\n1\n\n\n2\n2\n1\n-1\n0\n1\n2\n1\n2\n2\n\n\n2\n2\n1\n1\n0\n2\n2\n2\n2\n1\n\n\n1\n2\n1\n1\n0\n2\n2\n2\n1\n2\n\n\n1\n1\n1\n1\n0\n1\n2\n2\n1\n2\n\n\n2\n1\n1\n1\n0\n2\n2\n2\n1\n2\n\n\n2\n1\n2\n2\n0\n2\n1\n2\n2\n2\n\n\n2\n2\n2\n2\n0\n2\n2\n2\n2\n2\n\n\n\n\n\nWe will use pandas python package to load our CSV file and access each dimension code (Learn basics of Pandas Library).\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import cohen_kappa_score\n\ncoder1 = pd.read_csv('coder1.csv')\ncoder2 = pd.read_csv('coder2.csv')\n\ndimensions = coder1.columns\n\n#iterate for each dimension\nfor dim in dimensions:\n   \n    dim_codes1 = coder1[dim]\n    \n    dim_codes2 = coder2[dim]\n    print('Dimension:',dim)\n    \n    score = cohen_kappa_score(dim_codes1,dim_codes2)\n    \n    print(' ',score)\n\nDimension: SMU\n  0.3076923076923077\nDimension: CF\n  0.55\nDimension: KE\n  0.12903225806451613\nDimension: ARG\n  0.6896551724137931\nDimension: STR\n  0.0\nDimension: CO\n  -0.19999999999999996\nDimension: u1\n  0.0\nDimension: u2\n  0.0\nDimension: u3\n  0.3414634146341463\nDimension: u4\n  0.4375\n\n\n\nFleiss’s Kappa\n\n\n\n\nAs per my understanding, Cohen’s Kappa can be used if you have codes from only two coders. In case, if you have codes from multiple coders then you need to use Fleiss’s kappa.\n\n\n\n\nWe will use nltk.agreement package for calculating Fleiss’s Kappa. So now we add one more coder’s data to our previous example.\n\n\n\nfrom nltk import agreement\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\ncoder2 = [1,1,0,0,1,1,2,1,1,0]\ncoder3 = [1,2,2,1,2,1,2,1,1,0]\n\nformatted_codes = [[1,i,coder1[i]] for i in range(len(coder1))] + [[2,i,coder2[i]] for i in range(len(coder2))]  + [[3,i,coder3[i]] for i in range(len(coder3))]\n\n\nratingtask = agreement.AnnotationTask(data=formatted_codes)\n\nprint('Fleiss\\'s Kappa:',ratingtask.multi_kappa())\n\n\nFleiss's Kappa: 0.3010752688172044\n\n\n\nFleiss’s Kappa using CSV files\n\n\n\n\nNow, let’s say we have three CSV files, one from each coder. Each coder assigned codes on ten dimensions (as shown in the above example of CSV file). The following code compute Fleiss’s kappa among three coders for each dimension.\n\n\n\nimport pandas as pd\nfrom nltk import agreement\n\n\ncoder1 = pd.read_csv('coder1.csv')\ncoder2 = pd.read_csv('coder2.csv')\ncoder3 = pd.read_csv('coder3.csv')\n\ndimensions = coder1.columns\n\n\nfor dim in dimensions:\n   \n    dim_codes1 = coder1[dim]\n    dim_codes2 = coder2[dim]\n    dim_codes3 = coder3[dim]\n    \n    formatted_codes = [[1,i,dim_codes1[i]] for i in range(len(dim_codes1))] + [[2,i,dim_codes2[i]] for i in range(len(dim_codes2))]  + [[3,i,dim_codes3[i]] for i in range(len(dim_codes3))]\n    \n    ratingtask = agreement.AnnotationTask(data=formatted_codes)\n    print('Dimension:')\n    print(' Fleiss\\'s Kappa:',ratingtask.multi_kappa())\n\n\n\nCronbach’s Alpha\n\n\n\n\nCronbach’s alpha is mostly used to measure the internal consistency of a survey or questionnaire. For this measure, I am using Pingouin package (&lt;a href=““https://pingouin-stats.org/index.html”“&gt;link).\n\n\n\n\nLet’s say we have data from a questionnaire (which has questions with Likert scale) in a CSV file. For example, I am using a dataset from Pingouin with some missing values.\n\n\n\nimport pingouin as pg\n\ndata = pg.read_dataset('cronbach_wide_missing')\n\ndata.head()\n\n\nQ1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11\n1.0,1,1.0,1,1.0,1,1,1,1.0,1,1\n1.0,1,1.0,1,1.0,1,1,1,0.0,1,0\n,0,1.0,1,,1,1,1,1.0,0,0\n1.0,1,1.0,0,1.0,1,0,1,1.0,0,0\n1.0,1,1.0,1,1.0,0,0,0,1.0,0,0\n0.0,1,,0,1.0,1,1,1,0.0,0,0\n1.0,1,1.0,1,0.0,0,1,0,0.0,0,0\n1.0,1,1.0,1,1.0,0,0,0,0.0,0,0\n0.0,1,0.0,1,1.0,0,0,0,0.0,1,0\n1.0,0,0.0,1,0.0,1,0,0,,0,0\n1.0,1,1.0,0,0.0,0,0,0,0.0,0,0\n1.0,0,0.0,1,0.0,0,0,0,0.0,0,0\n\n\npg.cronbach_alpha(data=data)\n\n\n(0.732661, array([0.435, 0.909]))\n\n\n\nKrippendorff’s Alpha & Scott’s Pi\n\n\n\n\nWe can use nltk.agreement python package for both of these measures. I will show you an example of that.\n\n\n\n\nFor nltk.agreement, we need our formatted data (what we did in the previous example?). Once we have our formatted data, we simply need to call alpha function to get the Krippendorff’s Alpha. Let’s see the python code.\n\n\n\nfrom nltk import agreement\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\ncoder2 = [1,1,0,0,1,1,2,1,1,0]\ncoder3 = [1,2,2,1,2,1,2,1,1,0]\n\nformatted_codes = [[1,i,coder1[i]] for i in range(len(coder1))] + [[2,i,coder2[i]] for i in range(len(coder2))]  + [[3,i,coder3[i]] for i in range(len(coder3))]\n\n\nratingtask = agreement.AnnotationTask(data=formatted_codes)\n\nprint('Krippendorff\\'s alpha:',ratingtask.alpha())\nprint('Scott\\'s pi:',ratingtask.pi())\n\n\nKrippendorff's alpha: 0.30952380952380953\nScott's pi: 0.2857142857142859\n\n\n\nInter-class correlation\n\n\n\n\nI am using Pingouin package mentioned before as well. The function used is intraclass_corr. This function returns a Pandas Datafame having the following information (from R package psych). Six cases are returned (ICC1, ICC2, ICC3, ICC1k, ICCk2, ICCk3) by the function and the following are the meaning for each case.\n\n\n\n\nShrout and Fleiss (1979) consider six cases of reliability of ratings done by k raters on n targets.\n\n\n\n\nICC1: Each target is rated by a different judge and the judges are selected at random. (This is a one-way ANOVA fixed effects model and is found by (MSB- MSW)/(MSB+ (nr-1)*MSW))\n\n\n\n\n\nICC2: A random sample of k judges rate each target. The measure is one of absolute agreement in the ratings. Found as (MSB- MSE)/(MSB + (nr-1)MSE + nr(MSJ-MSE)/nc)\n\n\n\n\n\nICC3: A fixed set of k judges rate each target. There is no generalization to a larger population of judges. (MSB - MSE)/(MSB+ (nr-1)*MSE) \n\n\n\nThen, for each of these cases, there are second variant (e.g., ICC1k). The difference between the two variants of classes is that in the first case, the 1 rating is equivalent to the average intercorrelation, while, the k rating case uses Spearman Brown adjusted reliability.)\n\n\n\n\nICC1 is sensitive to differences in means between raters and is a measure of absolute agreement.\n\n\n\n\nICC2 and ICC3 remove mean differences between judges, but are sensitive to interactions of raters by judges. The difference between ICC2 and ICC3 is whether raters are seen as fixed or random effects.\n\n\n\n\nICC1k, ICC2k, ICC3K reflect the means of k raters.\n\n\n\n\nThe dataset from Pingouin has been used in the following example.\n\n\nimport pingouin as pg\n\ndata = pg.read_dataset('icc')\n\nicc = pg.intraclass_corr(data=data, targets='Wine', raters='Judge',ratings='Scores')\nicc\n\n\n     Type    Description     ICC     F   df1     df2     pval    CI95%\n0   ICC1    Single raters absolute  0.728   11.680  7   24  0.000002    [0.43, 0.93]\n1   ICC2    Single random raters    0.728   11.788  7   21  0.000005    [0.43, 0.93]\n2   ICC3    Single fixed raters     0.730   11.788  7   21  0.000005    [0.43, 0.93]\n3   ICC1k   Average raters absolute     0.914   11.680  7   24  0.000002    [0.75, 0.98]\n4   ICC2k   Average random raters   0.914   11.788  7   21  0.000005    [0.75, 0.98]\n5   ICC3k   Average fixed raters    0.915   11.788  7   21  0.000005    [0.75, 0.98]\n\n\n\nReferences\n\n\n\n\n\nHayes, A. F., & Krippendorff, K. (2007). Answering the Call for a Standard Reliability Measure for Coding Data. Communication Methods and Measures, 1(1), 77–89. https://doi.org/10.1080/19312450709336664\n\n\n\nImage by katemangostar on Freepik ”\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/concepts/pca.html",
    "href": "posts/concepts/pca.html",
    "title": "Understanding Principal Component Analysis under the hood",
    "section": "",
    "text": "Let’s assume that you have a dataset with a higher number of attributes and you are thinking Is there any way to compress this information into a smaller number of attributes.\nWell, dimensionality reduction methods offer that functionality. Among many dimensionality reduction methods, PCA (Principal Component Analysis) is widely used and this post introduction to PCA and its working.\nLet’s start with a simple dataset with two attributes, \\(x\\), and \\(y\\) which we need to reduce from two attributes to one. \n\n\n\n\n\n\n$x$\n$y$\n\n\n10\n5\n\n\n12\n5\n\n\n16\n6\n\n\n20\n7\n\n\n19\n5\n\n\n\n\n\n\n\n\n\nThe simplest approach is to select one of the attributes. Let’s say we are selecting \\(x\\). In other words, we are taking the projection of the data points on X-axis (Projection: simply drawing a perpendicular line from data points to the x-axis and taking that corresponding value). For example, in the following diagram, we have taken the projection of data points along the x-axis.\nSo here we have two options, we can take the projection of data points to either x-axis or y-axis. Our aim is to select one which offers more information about the data. To measure it, we can use variance which computes spread of data or in other words how the values are different from their mean value. So if you have an attribute with zero variance that means all values in that attribute are same. Variance can be computed using the following formula \\[ \\sigma^2 = \\sum\\limits_{i=1}^N (X -\\mu)^2 \\] where: \\(X\\) is the set of data points \\(\\mu\\) is the mean \\(N\\) is the number of data points in \\(X\\)\n\n\n\n\nSo, now if we look at our options (projection along x-axis or y-axis) then we find x-axis as a better option due to the larger variance shown in below figure\n\n\n\n\n\n\n\n\n\nDo we really have only these two options?\n\n\n\nNo, there are infinite number of possibilities (How: draw any line and take the projection of data points on that line in a similar manner as we did with x/y axis). In this case, if we can find a line which gives maximum variance of the data compared to other possible options then it can be used at the place of \\(x\\) and \\(y\\) attributes.\n\n\n\nPrincipal Component Analysis (PCA)\n\nAs we discussed in the above section the infinite possibilities for our two-dimensional data, PCA finds the one which offers maximum variance. Let’s go deeper into the mathematics of PCA.\n\n\n\n\nUnderstanding Mathematics behind PCA\n\n\nFirst, we are going to write the problem statement (finding a direction/vector/line which offers a maximum variance of projected data) of PCA into mathematics format. First, we need to see how to represent the projection. A projection of a data point along with a line can be computed using \\(dot\\) product. Let’s say we want to compute the projection of the first data point (10,5) on the x-axis.\nLet’s represent our data point as a vector \\(\\vec{x_1}\\). A vector has two properties- direction and magnitude (more info). The unit vector in the direction of x-axis and y-axis is represented by \\(\\hat{i}\\) and \\(\\hat{j}\\), respectively. Every vector then denoted by number of units in the direction of x-axis and y-axis. Our data point (10,5) can be represented as 10 units in x-axis direction and 5 units in y-axis direction. The dot product between \\(\\vec{x_1}\\) and \\(\\hat{i}\\) will give the projection over the x-axis.\n\n\n\n\n\\[= (10\\hat{i},5\\hat{j}) . (\\hat{i},0)\\]\n\n\n\n\n\\[= 10\\]\n\n\n\n\n\nIn simpler terms, if you have two vectors or list of numbers \\((a_1,a_2,a_3)\\) and\\((b_1,b_2,b_3)\\) then their dot product will be \\(a_1*b_1+a_2*b_2+a_3*b_3\\). It can be written in matrix form as following \\[\n      dot(A,B)=\\begin{bmatrix}\n      a_1 & a_2 & a_3\\\n      \\end{bmatrix}\\begin{bmatrix}\n      b_1 \\\\\n      b_2 \\\\\n      b_3 \\\\\n      \\end{bmatrix}\n     = A^TB\n  \\] here, \\(A^T\\) is transpose of \\(A\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nA dot product of \\(\\vec{A}\\) with its own gives you \\(A^2\\).\n\n\n\\[\n  A^2 = a_1^2+a_2^2+a_3^2 \\\\\n  = a_1*a_1+a_2*a_2+a_3*a_3\n  = A^TA\n  \\]\n\nNow coming back to PCA problem statement, let’s denote a direction (/vector/line) \\(\\vec{L}\\). So projection of our dataset \\(X\\) can be written as \\[\nX_{new}=X^TL\n\\] here, \\(X_{new}\\) represents the new values obtained after projection of \\(X\\) over \\(L\\). To ease the understanding of next step, let’s assume we transformed \\(X_{new}\\) in such a way that it has zero mean (we can do that by simply replacing every value in \\(X_{new}\\) by \\(X_{new}-Mean)\\). Next, we compute the variance of \\(X_{new}\\).\n\\[\nvar(X_{new}) = (X_{new}-0)^2 \\\\\n= X_{new}^2 \\\\\n= X_{new}^TX_{new}\n= (X^TL)^T(X^TL)\n= (L^TXX^TL)\n= (L^T  \\sum L)\n\\]\n\nhere, \\(\\sum\\) is covariance matrix of \\(X_{new}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nRule used: \\((AB)^T = B^TA^T\\) and \\((A^T)^T = A\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe add a constraint that L must be a unit vector which means \\(L^TL=1\\).\nWhy: In a single direction there can be infinite possibilities for example \\(2\\hat{i}+1\\hat{j}\\), \\(4\\hat{i}+2\\hat{j}\\), \\(\\hat{i}+.5\\hat{j}\\) all vectors are in the same direction. Therefore, to avoid it, we put a constraint that we will check only a single vector in a direction and that one with unit vector (to avoid large values).\n\n\nNow, our problem is to find \\(L\\) which maximizes \\(var(X_{new})\\) with constraint \\(L^TL=1\\) (constrain of unit vector). (also known as constrained maximization problem)\nTo formulate this problem, we will use Lagrange Multiplier. Our problem can be written as follows \\[\n\\max_{L} = (L^T\\sum L)-\\lambda(L^TL-1)\n\\]\n\nTo solve it, we will differentiate it with respect to \\(L\\) and then equate it to zero. As we seen above that \\(L^TL=L^2\\), therefore, differentiating it gives us \\(2L\\). \\[\n\\sum L - \\lambda (L) = 0\n\\]\n\n\n\n\n\\[ \\sum L = \\lambda L \\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above equation is actually stating that \\(L\\) must be eigenvector of covariance matrix of \\(X\\).\nA brief about eigen vector\n\n\n\n\n\nLet’s consider a matrix as a system of transformation. When it is multiplied with a vector that vector gets transformed into a new vector. Let’s take an example. We have a matrix \\[ M =\n    \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\] We will multiply (or apply) it on a vector \\(A = [2,3]\\). The multiplication of \\(M\\) and \\(A\\) will give \\[\n  \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\begin{bmatrix}\n      3 \\\\\n      2  \\\\\n      \\end{bmatrix}\n  = \\begin{bmatrix}\n      -2 \\\\\n      -5 \\\\\n      \\end{bmatrix}\n  \\] As it’s shown in the below figure that point (3,2) transformed into new point (-2,-5). Now, if we multiply \\(B = [1,1]\\) with M then we will get result in the same direction.\n \\[\n  \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\begin{bmatrix}\n      1 \\\\\n      1  \\\\\n      \\end{bmatrix}\n  = \\begin{bmatrix}\n      -2 \\\\\n      -1 \\\\\n      \\end{bmatrix}\n  = -2\\begin{bmatrix}\n      1 \\\\\n      1 \\\\\n      \\end{bmatrix}\n  \\]\nTherefore vector \\(B\\) is eigen vector for the matrix in above example.\n\n\n\n\nComing back to our last equation \\(\\sum L = \\lambda L\\), now the question is which eigenvector (as there can be many eigen vectors of a matrix) to use. Let’s have a look on our variance \\((L^T\\sum L)\\) which we want to maximize.\n\n\\[\n= (L^T\\sum L)\n= (L^T\\lambda L)\n= \\lambda(L^T L)\n= \\lambda\n\\] \n\n\n\n\n\n\nTip\n\n\n\nSo, to maximize the variance, we need to take eigenvector with maximum eigen value (\\(\\lambda\\)). Here is our first vector, one with highest Eigen value.\n\n\nUsing this result as a basis, we then have the following steps for PCA\n\n\n\n\n\nCompute Covariance matrix (\\(\\sum\\)) of \\(X\\)\n\n\nSubtract \\(X_\\mu\\) (mean) from \\(X\\)\n\n\nCompute eigenvector \\(\\sum\\)\n\n\nReorder the eigen vectors according to their corresponding eigen value\n\n\nProject data set on those eigenvectors beginning from the start (as the first eigenvector has the highest eigenvalue, second eigenvector with second-highest, and so on)\n\n\n\n\n\nKey points\n\n\n\n\n\nPCA is a unsupvervised machine learning algorithm.\n\n\nPCA dimensions are linear combinations of original attributes. Therefore, it is a linear DR method. However, there are variants of PCA (e.g. kernelPCA) which offers non-linearity feature.\n\n\nUneven data range of attributes can influence the PCA results, therefore, standardize your data before applying PCA.\n\n\n\n\n\n\nExample\nPython’s library scikit-learn has numerous inbuilt functions for dimensionality reduction. In this example, we will see how to use that function. First, we need to import the packages\n\n\n\n# For loading iris dataset\nfrom sklearn import datasets\n\n# For standardizing our data\nfrom sklearn.preprocessing import StandardScaler\n\n# For PCA\nfrom sklearn import decomposition\n\n\n# Load your dataset here\niris = datasets.load_iris()\n\n# In the next step, we will standardize our data.&lt;/p&gt;\n\n# Creating Standard Scaler\nscaler = StandardScaler()\n\n# Fitting iris data to a scaler\nscaler.fit(iris)\n\n# Transform the data into standardised form\nnew_iris = scaler.transform(iris)\n\n# Create PCA object\npca = decomposition.PCA(n_components=3)\n\n# Fitting data to PCA\npca.fit(new_iris)\n\n# Computing new dimensions\ndr_iris = pca.transform(new_iris)\nLet’s check now how much variance offered by new dimensions. There is an attribute of PCA class in sklearn library: explained_variance_ratio_ which offer this information.\nprint(PCA.explained_variance_ratio_)\n# Output\n# array([0.72962445, 0.22850762, 0.03668922])&lt;/code&gt;&lt;/pre&gt;\nAs it can be seen that first two principal components offered in total around 94% variance o f original data.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "The following projects focus on utilizing the power of analytics to support teaching and learning experience.\n\n\nDemo | Publication | Source code\nPython, Dash\n\nA Raspberry Pi-based prototype to capture audio data from face-to-face group activity\nAnalyzed the direction of arrival of captured audio to compute speaking time and turn-taking using python\nDeveloped an interactive dashboard for data visualization\n\n\n\n\n\n🏆 Best Demo Award at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nDemo | Publication | Source code\nPython , jQuery, MySQL, Etherpad, plotly, Google Speech-to-Text\n\nPython Django app to collect multimodal learning data from group activities in classrooms\nVoice activity detection to compute speaking time of individual in group activities in real-time\nGoogle Speech-To-Text integration to process audio in real-time\nA real-time dashboard visualizing group’s writing behavior, speaking participation, and speech content (in the form of word cloud)\n\n\n\n\n\n\nPublication | Source code\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio and log data gathered from Estonian classrooms during group activities\nExplored use of different temporal window size (30s, 60s, 90s, 120s, 180s, 240s) to process features such as speaking time, turn-taking\nDeveloped machine learning models to predict collaboration quality using audio and log data features processed with different window sizes\n\n\n\n\n\n\nPublication\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio, video and log data to develop machine learning models to predict collaboration quality in classroom settings during group activity\nUsed Random Forest algorithm to develop model and evaluated its generalizability with a different dataset collected from a different Estonian school"
  },
  {
    "objectID": "Projects.html#edtech-projects",
    "href": "Projects.html#edtech-projects",
    "title": "Projects",
    "section": "",
    "text": "The following projects focus on utilizing the power of analytics to support teaching and learning experience.\n\n\nDemo | Publication | Source code\nPython, Dash\n\nA Raspberry Pi-based prototype to capture audio data from face-to-face group activity\nAnalyzed the direction of arrival of captured audio to compute speaking time and turn-taking using python\nDeveloped an interactive dashboard for data visualization\n\n\n\n\n\n🏆 Best Demo Award at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nDemo | Publication | Source code\nPython , jQuery, MySQL, Etherpad, plotly, Google Speech-to-Text\n\nPython Django app to collect multimodal learning data from group activities in classrooms\nVoice activity detection to compute speaking time of individual in group activities in real-time\nGoogle Speech-To-Text integration to process audio in real-time\nA real-time dashboard visualizing group’s writing behavior, speaking participation, and speech content (in the form of word cloud)\n\n\n\n\n\n\nPublication | Source code\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio and log data gathered from Estonian classrooms during group activities\nExplored use of different temporal window size (30s, 60s, 90s, 120s, 180s, 240s) to process features such as speaking time, turn-taking\nDeveloped machine learning models to predict collaboration quality using audio and log data features processed with different window sizes\n\n\n\n\n\n\nPublication\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio, video and log data to develop machine learning models to predict collaboration quality in classroom settings during group activity\nUsed Random Forest algorithm to develop model and evaluated its generalizability with a different dataset collected from a different Estonian school"
  },
  {
    "objectID": "Projects.html#web-development-projects",
    "href": "Projects.html#web-development-projects",
    "title": "Projects",
    "section": "Web-Development Projects",
    "text": "Web-Development Projects\nI worked on the following projects as part of my web-developer role at Tallinn University.\n\nTruestedUx\nWebsite\nPython , Django, jQuery, Bootstrap\n\nAn application which allows assessment of trust aspect of technology usage.\nDeveloped the complete front-end part and the most of back-end of the website.\n\n\n\nTinda\nWebsite\nDrupal , jQuery, Highcharts\n\nAn application which allows assessment of digital competencies of professionals.\nAdded an module to download the responses of users to questionnaires on digital competencies following the DigiComp Framework.\nAdded a dashboard visualizing users’ responses and their overall score for digital competencies.\nAdded a report download functionality.\n\n\n\nSeeds\nWebsite\nPython , Django, jQuery, Bootstrap, Plotly\n\nA map-based application to allows users to filter energy transition scenario based on their preferences.\nHandled the entire development of the project including front-end and back-end."
  },
  {
    "objectID": "posts/post-with-code/basics_nlp.html#installation",
    "href": "posts/post-with-code/basics_nlp.html#installation",
    "title": "Basics of Natural Language Processing in Python",
    "section": "Installation",
    "text": "Installation\nFirst we need to install nltk library. The following command can be used to do that.\n\npip install nltk"
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Blog",
    "section": "",
    "text": "Hands-on experience with llama2 with Python: Building a simple Q&A app\n\n\n2 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Natural Language Processing in Python\n\n\n10 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation data visualization on Map using Python\n\n\n2 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nSep 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python Dash Framework\n\n\n5 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nJun 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing inter-rater aggrement scores using Python\n\n\n10 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nMay 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Gramian Angular Field\n\n\n4 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR tutorial on accessing, filtering, aggregating and plotting data\n\n\n7 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nApr 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up MQTT server and Client\n\n\n9 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nJan 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical tests in R\n\n\n14 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python’s Pandas API\n\n\n9 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nMay 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Principal Component Analysis under the hood\n\n\n9 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nApr 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFacial feature extraction using OpenFace\n\n\n2 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nMar 28, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV\n  \n\n\n\n  \n\n\n\n\n Back to top"
  }
]