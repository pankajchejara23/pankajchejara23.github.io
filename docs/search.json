[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Download current CV\n  \n\n\n\n  \n\n\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Home\n    Publications"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2025",
    "text": "2025\n\n\n\n  \n    Supporting Student-Centered Learning with Flexible Learning Trajectories and Open Learner Models\n    Chejara, P., Tammets, K., Laanpere, M., Volt, A., Tammets, P., & Savitski, P.\n    15th International Conference on Learning Analytics & Knowledge\n    (2025) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Teacher-AI complementarity: From design to implementation and reflection\n    Chejara, P., Tammets, K., Laanpere, M., Volt, A., Kasepalu, R., Sarmiento-M√°rquez, E. M., & Sillat, L. H.\n    15th International Conference on Learning Analytics and Knowledge\n    (2025) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2024",
    "text": "2024\n\n\n\n  \n    Bringing Collaborative Analytics using Multimodal Data to the Masses: Evaluation and Design Guidelines for Developing a MMLA System for Research and Teaching Practices in CSCL\n    Chejara, P., Kasepalu, R., Prieto, L., P., Rodr√≠guez-Triana, M. J., & Ruiz-Calleja, A.\n    14th International Learning Analytics and Knowledge Conference\n    (2024) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    How well do collaboration quality estimation models generalize across authentic school contexts\n    Chejara, P., Kasepalu, R., Prieto, L., P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., & Schneider, B.\n    British Journal of Educational Technology\n    (2024) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Impact of data noise on the performance of supervised machine learning models using multimodal data to estimate collaboration quality\n    Chejara, P., Prieto, L., P., Dimitriadis, Y., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., Kasepalu, R., & Shankar, S. K\n    Journal of Learning Analytics\n    (2024) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "2023",
    "text": "2023\n\n\n\n  \n    CIMLA: A Modular and Modifiable Data Preparation, Organization, and Fusion Infrastructure to Partially Support the Development of Context-aware MMLA Solutions.\n    Shankar, S. K., Ruiz-Calleja, A., Prieto, L. P., Rodr√≠guez-Triana, M. J., Chejara, P., & Tripathi, S.\n    Journal of Universal Computer Science\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics\n    Chejara, P., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., Kasepalu, R. Chounta, I.-A., & Schneider, B.\n    In the 18th European Conference on Technology Enhanced Learning (EC-TEL)\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    How to build more generalizable models for collaboration quality? Lessons learned from exploring multi-contexts audio-log datasets using Multimodal Learning Analytics\n    Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J.,   Kasepalu, R. &  Shankar, S. K.,\n    13th International Learning Analytics and Knowledge Conference\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics.\n    Chejara, P., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., & Khalil, M.\n    13th International Learning Analytics and Knowledge Conference\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Multimodal Learning Analytics Research in the Wild: Challenges and Their Potential Solutions.\n    Chejara, P., Kasepalu, R. Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Shankar, S. K.\n    13th International Learning Analytics and Knowledge Conference\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Studying teacher withitness in the wild: comparing a mirroring and an alerting & guiding dashboard for collaborative learning\n    Kasepalu, R., Chejara, P., Prieto, L.P., & Ley, T.\n    International Journal of Computer-Supported Collaborative Learning\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‚Äêbased learning analytics\n    Kasepalu, R., Chejara, P., Prieto, L.P., & Ley, T.\n    International Journal of Computer-Assisted Learning\n    (2023) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\n\n\n  \n    Co-designing a multimodal dashboard for collaborative analytics\n    Kasepalu, R., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Prieto, L. P.\n    15th International Conference on Computer-Supported Collaborative Learning (CSCL)\n    (2022) \n    \n      Details\n    \n    \n    \n    \n  \n  \n\n  \n    Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard\n    Kasepalu, R., Chejara, P., Prieto, L. P., & Ley, T.\n    Technology, Knowledge and Learning\n    (2022) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Teacher AI-Supported Pedagogical Actions in Collaborative Learning Coregulation: A Wizard-of-Oz Study\n    Kasepalu, R., Ley, T.,  Prieto, L. P., & Chejara, P.\n    Frontiers in Education\n    (2022) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications",
    "section": "2021",
    "text": "2021\n\n\n\n  \n    EFAR-MMLA: An evaluation framework to assess and report generalizability of machine learning models in MMLA\n    Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J.,  Shankar, S. K., & Kasepalu, R. \n    Sensors\n    (2022) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n  \n    Exploring the triangulation of dimensionality reduction when interpreting multimodal learning data from authentic settings\n    Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Shankar, S. K.\n    14th European Conference on Technology Enhanced Learning (EC-TEL) \n    (2021) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n  \n  \n\n  \n    Quantifying collaboration quality in face-to-face classroom settings using MMLA\n    Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J.,  Shankar, S. K., & Kasepalu, R.\n    International Conference on Collaboration Technologies and Social Computing (CollabTech)\n    (2021) \n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       PDF\n    \n    \n  \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pankaj Chejara",
    "section": "",
    "text": "Hi there üëã, I am Pankaj. Professionally, I love tasks around data analysis to gain insights to solve tasks at hand. I am a strong supporter of lifelong learning, and this is also what I have been engaged with since my schooling days. Apart from work, I love to go for long walks in nature, spend time with close family & friends, and watch movies with my son (Yes, you guessed it right, animation movies üòä).\n\n\n Back to top"
  },
  {
    "objectID": "talks/finland.html",
    "href": "talks/finland.html",
    "title": "Supporting Teachers during Collaborative Learning Using MMLA and AI",
    "section": "",
    "text": "This talk was given at Tampere Research Centre for Information & Media (TAU), Tampere University in Finland during a short research visit. In this talk, I presented my research together with my colleague Dr.¬†Reet Kasepalu on the potential of Multimodal Learning Analytics to support teachers in collaborative learning activities with monitoring.\nPresentation slides:  Slides\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/phd.html",
    "href": "talks/phd.html",
    "title": "Designing and building automated systems for collaboration monitoring in classroom settings",
    "section": "",
    "text": "This talk was given to defend my PhD thesis on the topic of Classroom collaboration analytics: designing and building automated systems for collaboration monitoring in classroom settings at Tallinn University.\nYou can access the thesis here:üìï Phd Thesis\nPresentations slides are available here:   Slides"
  },
  {
    "objectID": "talks/phd.html#abstract",
    "href": "talks/phd.html#abstract",
    "title": "Designing and building automated systems for collaboration monitoring in classroom settings",
    "section": "Abstract",
    "text": "Abstract\nCollaboration is a key skill in current education, also identified as one of the main ‚Äú21st Century skills‚Äù. Moreover, the research evidence on the positive impact of collaboration on learning further underscores the need to develop this skill among students. Teaching students to collaborate effectively involves teachers monitoring each group‚Äôs activities and offering support to groups when needed. However, it is extremely difficult for teachers to be aware of how all their students collaborate, especially when multiple groups are working at the same time and their interaction is not only face-to-face but also computer-mediated. Therefore, it is essential to assist teachers in classrooms with monitoring and understanding of their students‚Äô collaboration to effectively develop such skills among students.\nResearchers have explored the use of a wide range of data sources such as audio, log data, video, etc. to understand and support learning and teaching. This field of research is known as Multimodal Learning Analytics (MMLA). MMLA researchers have combined learning traces from digital as well as physical spaces of interaction to holistically understand collaboration. The field has gained substantial traction among researchers from diverse disciplines, such as learning sciences, psychology, computer science, etc. This popularity can also be witnessed in an increasing number of research studies using MMLA with a focus on collaboration, learning and teaching. This area of research has identified data indicators for collaboration, developed tools to support teacher monitoring and understanding of collaboration processes. However, the majority of this research has been conducted in laboratory settings and there is still a lack of understanding of the feasibility of the automated estimation of collaboration measures in authentic settings.\nIn particular, this dissertation has identified and addressed three research gaps: first, a lack of systematisation in the evaluation of machine learning solutions developed in MMLA for collaboration; second, a lack of knowledge on building generalisable machine learning models for collaboration quality and its dimensions (e.g., argumentation) in classroom settings; and third, a lack of understanding of the generalisability of machine learning models for collaboration quality when used in authentic classroom settings."
  },
  {
    "objectID": "talks/lak24.html",
    "href": "talks/lak24.html",
    "title": "Bringing Collaborative Analytics using Multimodal Data to the Masses",
    "section": "",
    "text": "This talk was given at the 14th International Conference of Learning Analytics & Knowledge to present results from an evaluation study of CoTrack (a multimodal learning analytics tool), also published in the form of a conference paper. The study analyzed and reported results from teachers‚Äô responses on using CoTrack in their classroom for collaborative learning activities.\nYou can read more about the presented work here:  Paper.\nPresentation slides:  Slides"
  },
  {
    "objectID": "talks/lak24.html#abstract",
    "href": "talks/lak24.html#abstract",
    "title": "Bringing Collaborative Analytics using Multimodal Data to the Masses",
    "section": "Abstract",
    "text": "Abstract\nThe Multimodal Learning Analytics (MMLA) research community has significantly grown in the past few years. Researchers in this field have harnessed diverse data collection devices such as eye- trackers, motion sensors, and microphones to capture rich mul- timodal data about learning. This data, when analyzed, has been proven highly valuable for understanding learning processes across a variety of educational settings. Notwithstanding this progress, an ubiquitous use of MMLA in education is still limited by challenges such as technological complexity, high costs, etc. In this paper, we introduce CoTrack, a MMLA system for capturing the multimodal- ity of a group‚Äôs interaction in terms of audio, video, and writing logs in online and co-located collaborative learning settings. The system offers a user-friendly interface, designed to cater to the needs of teachers and students without specialized technical expertise. Our usability evaluation with 2 researchers, 2 teachers and 24 students has yielded promising results regarding the system‚Äôs ease of use. Furthermore, this paper offers design guidelines for the develop- ment of more user-friendly MMLA systems. These guidelines have significant implications for the broader aim of making MMLA tools accessible to a wider audience, particularly for non-expert MMLA users."
  },
  {
    "objectID": "talks/lak23.html",
    "href": "talks/lak23.html",
    "title": "How to build more generalizable models for collaboration quality?",
    "section": "",
    "text": "This talk presented a research paper at the 14th International Conference of Learning Analytics & Knowledge organized at Arizona State University, Arizona. The paper included results from a study exploring methodologies to build generalizable collaboration prediction models for authentic classroom settings.\nYou can read more about the presented work here:  Paper.\nPresentation slides:  Slides"
  },
  {
    "objectID": "talks/lak23.html#abstract",
    "href": "talks/lak23.html#abstract",
    "title": "How to build more generalizable models for collaboration quality?",
    "section": "Abstract",
    "text": "Abstract\nMultimodal learning analytics (MMLA) research for building collab- oration quality estimation models has shown significant progress. However, the generalizability of such models is seldom addressed. In this paper, we address this gap by systematically evaluating the across-context generalizability of collaboration quality mod- els developed using a typical MMLA pipeline. This paper further presents a methodology to explore modelling pipelines with differ- ent configurations to improve the generalizability of the model. We collected 11 multimodal datasets (audio and log data) from face-to- face collaborative learning activities in six different classrooms with five different subject teachers. Our results showed that the models developed using the often-employed MMLA pipeline degraded in terms of Kappa from Fair (.20 &lt; Kappa &lt; .40) to Poor (Kappa &lt; .20) when evaluated across contexts. This degradation in performance was significantly ameliorated with pipelines that emerged as high- performing from our exploration of 32 pipelines. Furthermore, our exploration of pipelines provided statistical evidence that often- overlooked contextual data features improve the generalizability of a collaboration quality model. With these findings, we make recommendations for the modelling pipeline which can potentially help other researchers in achieving better generalizability in their collaboration quality estimation models"
  },
  {
    "objectID": "talks/lak23-2.html",
    "href": "talks/lak23-2.html",
    "title": "Impact of temporal window on the performance of collaboration prediction models",
    "section": "",
    "text": "This talk presented a research paper at the 14th International Conference of Learning Analytics & Knowledge organized at Arizona State University, Arizona. The paper included results from a study investigating the impact of using different temporal window sizes for feature aggregation on the performance of collaboration prediction models for authentic classroom settings.\nYou can read more about the presented work here:  Paper.\nPresentation slides:  Slides"
  },
  {
    "objectID": "talks/lak23-2.html#abstract",
    "href": "talks/lak23-2.html#abstract",
    "title": "Impact of temporal window on the performance of collaboration prediction models",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics (MMLA) has been applied to col- laborative learning, often to estimate collaboration quality with the use of multimodal data, which often have uneven time scales. The difference in time scales is usually handled by dividing and aggregating data using a fixed-size time window. So far, the cur- rent MMLA research lacks a systematic exploration of whether and how much window size affects the generalizability of collaboration quality estimation models. In this paper, we investigate the impact of different window sizes (e.g., 30 seconds, 60s, 90s, 120s, 180s, 240s) on the generalizability of classification models for collaboration quality and its underlying dimensions (e.g., argumentation). Our results from an MMLA study involving the use of audio and log data showed that a 60 seconds window size enabled the development of more generalizable models for collaboration quality (AUC 61%) and argumentation (AUC 64%). In contrast, for modeling dimensions focusing on coordination, interpersonal relationship, and joint in- formation processing, a window size of 180 seconds led to better performance in terms of across-context generalizability (on average from 56% AUC to 63% AUC). These findings have implications for the eventual application of MMLA in authentic practice."
  },
  {
    "objectID": "articles/2021/collabtech.html",
    "href": "articles/2021/collabtech.html",
    "title": "Quantifying collaboration quality in face-to-face classroom settings using MMLA",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., Shankar, S. K., & Kasepalu, R. (2020). Quantifying collaboration quality in face-to-face classroom settings using MMLA. In International Conference on Collaboration Technologies and Social Computing (CollabTech) (pp.¬†159-166). Springer, Cham. https://doi.org/10.1007/978-3-030-58157-2_11"
  },
  {
    "objectID": "articles/2021/collabtech.html#citation-apa-7",
    "href": "articles/2021/collabtech.html#citation-apa-7",
    "title": "Quantifying collaboration quality in face-to-face classroom settings using MMLA",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., Shankar, S. K., & Kasepalu, R. (2020). Quantifying collaboration quality in face-to-face classroom settings using MMLA. In International Conference on Collaboration Technologies and Social Computing (CollabTech) (pp.¬†159-166). Springer, Cham. https://doi.org/10.1007/978-3-030-58157-2_11"
  },
  {
    "objectID": "articles/2021/collabtech.html#abstract",
    "href": "articles/2021/collabtech.html#abstract",
    "title": "Quantifying collaboration quality in face-to-face classroom settings using MMLA",
    "section": "Abstract",
    "text": "Abstract\nThe estimation of collaboration quality using manual observation and coding is a tedious and difficult task. Researchers have proposed the automation of this process by estimation into few categories (e.g., high vs.¬†low collaboration). However, such categorical estimation lacks in depth and actionability, which can be critical for practitioners. We present a case study that evaluates the feasibility of quantifying collaboration quality and its multiple sub-dimensions (e.g., collaboration flow) in an authentic classroom setting. We collected multimodal data (audio and logs) from two groups collaborating face-to-face and in a collaborative writing task. The paper describes our exploration of different machine learning models and compares their performance with that of human coders, in the task of estimating collaboration quality along a continuum. Our results show that it is feasible to quantitatively estimate collaboration quality and its sub-dimensions, even from simple features of audio and log data, using machine learning. These findings open possibilities for in-depth automated quantification of collaboration quality, and the use of more advanced features and algorithms to get their performance closer to that of human coders."
  },
  {
    "objectID": "articles/2021/sensors.html",
    "href": "articles/2021/sensors.html",
    "title": "EFAR-MMLA: An evaluation framework to assess and report generalizability of machine learning models in MMLA",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., Shankar, S. K., & Kasepalu, R. (2021). Efar-mmla: An evaluation framework to assess and report generalizability of machine learning models in mmla. Sensors, 21(8), 2863."
  },
  {
    "objectID": "articles/2021/sensors.html#citation-apa-7",
    "href": "articles/2021/sensors.html#citation-apa-7",
    "title": "EFAR-MMLA: An evaluation framework to assess and report generalizability of machine learning models in MMLA",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., Shankar, S. K., & Kasepalu, R. (2021). Efar-mmla: An evaluation framework to assess and report generalizability of machine learning models in mmla. Sensors, 21(8), 2863."
  },
  {
    "objectID": "articles/2021/sensors.html#abstract",
    "href": "articles/2021/sensors.html#abstract",
    "title": "EFAR-MMLA: An evaluation framework to assess and report generalizability of machine learning models in MMLA",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics (MMLA) researchers are progressively employing machine learning (ML) techniques to develop predictive models to improve learning and teaching practices. These predictive models are often evaluated for their generalizability using methods from the ML domain, which do not take into account MMLA‚Äôs educational nature. Furthermore, there is a lack of systematization in model evaluation in MMLA, which is also reflected in the heterogeneous reporting of the evaluation results. To overcome these issues, this paper proposes an evaluation framework to assess and report the generalizability of ML models in MMLA (EFAR-MMLA). To illustrate the usefulness of EFAR-MMLA, we present a case study with two datasets, each with audio and log data collected from a classroom during a collaborative learning session. In this case study, regression models are developed for collaboration quality and its sub-dimensions, and their generalizability is evaluated and reported. The framework helped us to systematically detect and report that the models achieved better performance when evaluated using hold-out or cross-validation but quickly degraded when evaluated across different student groups and learning contexts. The framework helps to open up a ‚Äúwicked problem‚Äù in MMLA research that remains fuzzy (i.e., the generalizability of ML models), which is critical to both accumulating knowledge in the research community and demonstrating the practical relevance of these techniques."
  },
  {
    "objectID": "articles/2023/lak2.html",
    "href": "articles/2023/lak2.html",
    "title": "Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics.",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., & Khalil, M. (2023). Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics. In the 13th International Learning Analytics and Knowledge Conference (LAK23) (pp.¬†559-565). ACM. https://doi.org/10.1145/3576050.3576143"
  },
  {
    "objectID": "articles/2023/lak2.html#citation-apa-7",
    "href": "articles/2023/lak2.html#citation-apa-7",
    "title": "Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics.",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., & Khalil, M. (2023). Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics. In the 13th International Learning Analytics and Knowledge Conference (LAK23) (pp.¬†559-565). ACM. https://doi.org/10.1145/3576050.3576143"
  },
  {
    "objectID": "articles/2023/lak2.html#abstract",
    "href": "articles/2023/lak2.html#abstract",
    "title": "Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics.",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics (MMLA) has been applied to collaborative learning, often to estimate collaboration quality with the use of multimodal data, which often have uneven time scales. The difference in time scales is usually handled by dividing and aggregating data using a fixed-size time window. So far, the current MMLA research lacks a systematic exploration of whether and how much window size affects the generalizability of collaboration quality estimation models. In this paper, we investigate the impact of different window sizes (e.g., 30 seconds, 60s, 90s, 120s, 180s, 240s) on the generalizability of classification models for collaboration quality and its underlying dimensions (e.g., argumentation). Our results from an MMLA study involving the use of audio and log data showed that a 60 seconds window size enabled the development of more generalizable models for collaboration quality (AUC 61%) and argumentation (AUC 64%). In contrast, for modeling dimensions focusing on coordination, interpersonal relationship, and joint information processing, a window size of 180 seconds led to better performance in terms of across-context generalizability (on average from 56% AUC to 63% AUC). These findings have implications for the eventual application of MMLA in authentic practice."
  },
  {
    "objectID": "articles/2023/cscl.html",
    "href": "articles/2023/cscl.html",
    "title": "Studying teacher withitness in the wild: comparing a mirroring and an alerting & guiding dashboard for collaborative learning",
    "section": "",
    "text": "Kasepalu, R., Chejara, P., Prieto, L.P. et al.¬†Studying teacher withitness in the wild: comparing a mirroring and an alerting & guiding dashboard for collaborative learning. Intern. J. Comput.-Support. Collab. Learn 18, 575‚Äì606 (2023). https://doi.org/10.1007/s11412-023-09414-z"
  },
  {
    "objectID": "articles/2023/cscl.html#citation-apa-7",
    "href": "articles/2023/cscl.html#citation-apa-7",
    "title": "Studying teacher withitness in the wild: comparing a mirroring and an alerting & guiding dashboard for collaborative learning",
    "section": "",
    "text": "Kasepalu, R., Chejara, P., Prieto, L.P. et al.¬†Studying teacher withitness in the wild: comparing a mirroring and an alerting & guiding dashboard for collaborative learning. Intern. J. Comput.-Support. Collab. Learn 18, 575‚Äì606 (2023). https://doi.org/10.1007/s11412-023-09414-z"
  },
  {
    "objectID": "articles/2023/cscl.html#abstract",
    "href": "articles/2023/cscl.html#abstract",
    "title": "Studying teacher withitness in the wild: comparing a mirroring and an alerting & guiding dashboard for collaborative learning",
    "section": "Abstract",
    "text": "Abstract\nTeachers in a collaborative learning (CL) environment have the demanding task of monitoring several groups of students at the same time and intervening when needed. This withitness (both the situational awareness and interventions taken in class) of the teacher might be increased with the help of a guiding dashboard alerting the teacher of problems and providing suggestions for interventions. This paper introduces a quasi-experiment carried out in authentic classrooms. We examined how a mirroring and an alerting & guiding dashboard affected the withitness of teachers in a face-to-face learning environment while students discussed and used a collaborative writing tool. Twenty-four teachers were observed, interviewed, and answered surveys in three different conditions altogether: with no extra information about the situation, using a dashboard mirroring low-level data about the collaboration, and additionally an AI assistant indicating problems in pedagogical terms and potential solutions (i.e., a guiding dashboard). The results show that the situational awareness of the teachers increased with the introduction of a mirroring dashboard. The workload of the participating teachers dropped more with the introduction of an alerting & guiding dashboard, helping teachers feel less frustrated and more accomplished."
  },
  {
    "objectID": "articles/2023/cmmla.html",
    "href": "articles/2023/cmmla.html",
    "title": "Multimodal Learning Analytics Research in the Wild: Challenges and Their Potential Solutions.",
    "section": "",
    "text": "Chejara, P., Kasepalu, R. Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Shankar, S. K., (2023). Multimodal learning analytics research in the wild: challenges and their potential solution. In CrossMMLA workshop at 13th International Learning Analytics and Knowledge Conference (LAK23) (pp.¬†36-42). CEUR workshop proceedings."
  },
  {
    "objectID": "articles/2023/cmmla.html#citation-apa-7",
    "href": "articles/2023/cmmla.html#citation-apa-7",
    "title": "Multimodal Learning Analytics Research in the Wild: Challenges and Their Potential Solutions.",
    "section": "",
    "text": "Chejara, P., Kasepalu, R. Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Shankar, S. K., (2023). Multimodal learning analytics research in the wild: challenges and their potential solution. In CrossMMLA workshop at 13th International Learning Analytics and Knowledge Conference (LAK23) (pp.¬†36-42). CEUR workshop proceedings."
  },
  {
    "objectID": "articles/2023/cmmla.html#abstract",
    "href": "articles/2023/cmmla.html#abstract",
    "title": "Multimodal Learning Analytics Research in the Wild: Challenges and Their Potential Solutions.",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics (MMLA) has enabled researchers to address learning in physical settings which have long been either overlooked or studied using observational methods. With the use of sensors, researchers have been able to understand learning through an entirely new perspective (eg, analyzing heart-rate variability to find collaboration indicators). Consequently, MMLA has grown significantly in the past few years, moving from a nascent stage towards a more mature field. It raises a question on how the MMLA researcher can move further, ie, the transition towards practice which started getting researchers‚Äô attention. This paper discusses the challenges we faced while conducting MMLA studies in classroom settings over four years and potential solutions to realize the goal of transitioning MMLA research to educational practice. This paper aims to start a discussion in the field of MMLA over the transition of research to practice."
  },
  {
    "objectID": "articles/2024/bjet.html",
    "href": "articles/2024/bjet.html",
    "title": "How well do collaboration quality estimation models generalize across authentic school contexts",
    "section": "",
    "text": "Chejara, P., Kasepalu, R., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz Calleja, A., & Schneider, B. (2024). How well do collaboration quality estimation models generalize across authentic school contexts? British Journal of Educational Technology, 55, 1602‚Äì1624. https://doi.org/10.1111/bjet.13402"
  },
  {
    "objectID": "articles/2024/bjet.html#citation-apa-7",
    "href": "articles/2024/bjet.html#citation-apa-7",
    "title": "How well do collaboration quality estimation models generalize across authentic school contexts",
    "section": "",
    "text": "Chejara, P., Kasepalu, R., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz Calleja, A., & Schneider, B. (2024). How well do collaboration quality estimation models generalize across authentic school contexts? British Journal of Educational Technology, 55, 1602‚Äì1624. https://doi.org/10.1111/bjet.13402"
  },
  {
    "objectID": "articles/2024/bjet.html#abstract",
    "href": "articles/2024/bjet.html#abstract",
    "title": "How well do collaboration quality estimation models generalize across authentic school contexts",
    "section": "Abstract",
    "text": "Abstract\nMultimodal learning analytics (MMLA) research has made significant progress in modelling collaboration quality for the purpose of understanding collaboration behaviour and building automated collaboration estimation models. Deploying these automated models in authentic classroom scenarios, however, remains a challenge. This paper presents findings from an evaluation of collaboration quality estimation models. We collected audio, video and log data from two different Estonian schools. These data were used in different combinations to build collaboration estimation models and then assessed across different subjects, different types of activities (collaborative-writing, group-discussion) and different schools. Our results suggest that the automated collaboration model can generalize to the context of different schools but with a 25% degradation in balanced accuracy (from 82% to 57%). Moreover, the results also indicate that multimodality brings more performance improvement in the case of group-discussion-based activities than collaborative-writing-based activities. Further, our results suggest that the video data could be an alternative for understanding collaboration in authentic settings where higher-quality audio data cannot be collected due to contextual factors. The findings have implications for building automated collaboration estimation systems to assist teachers with monitoring their collaborative classrooms."
  },
  {
    "objectID": "articles/2024/cotrack.html",
    "href": "articles/2024/cotrack.html",
    "title": "Bringing Collaborative Analytics using Multimodal Data to the Masses: Evaluation and Design Guidelines for Developing a MMLA System for Research and Teaching Practices in CSCL",
    "section": "",
    "text": "Chejara, P., Kasepalu, R., Prieto, L., P., Rodr√≠guez-Triana, M. J., & Ruiz-Calleja, A. (2024). Bringing collaboration analytics using multimodal data to the masses: Evaluation and design guidelines for developing a mmla system for research and teaching practices in CSCL. In the 14th International Learning Analytics and Knowledge Conference (LAK24). ACM. https://doi.org/10.1145/3636555.3636877"
  },
  {
    "objectID": "articles/2024/cotrack.html#citation-apa-7",
    "href": "articles/2024/cotrack.html#citation-apa-7",
    "title": "Bringing Collaborative Analytics using Multimodal Data to the Masses: Evaluation and Design Guidelines for Developing a MMLA System for Research and Teaching Practices in CSCL",
    "section": "",
    "text": "Chejara, P., Kasepalu, R., Prieto, L., P., Rodr√≠guez-Triana, M. J., & Ruiz-Calleja, A. (2024). Bringing collaboration analytics using multimodal data to the masses: Evaluation and design guidelines for developing a mmla system for research and teaching practices in CSCL. In the 14th International Learning Analytics and Knowledge Conference (LAK24). ACM. https://doi.org/10.1145/3636555.3636877"
  },
  {
    "objectID": "articles/2024/cotrack.html#abstract",
    "href": "articles/2024/cotrack.html#abstract",
    "title": "Bringing Collaborative Analytics using Multimodal Data to the Masses: Evaluation and Design Guidelines for Developing a MMLA System for Research and Teaching Practices in CSCL",
    "section": "Abstract",
    "text": "Abstract\nThe Multimodal Learning Analytics (MMLA) research community has significantly grown in the past few years. Researchers in this field have harnessed diverse data collection devices such as eye-trackers, motion sensors, and microphones to capture rich multimodal data about learning. This data, when analyzed, has been proven highly valuable for understanding learning processes across a variety of educational settings. Notwithstanding this progress, an ubiquitous use of MMLA in education is still limited by challenges such as technological complexity, high costs, etc. In this paper, we introduce CoTrack, a MMLA system for capturing the multimodality of a group‚Äôs interaction in terms of audio, video, and writing logs in online and co-located collaborative learning settings. The system offers a user-friendly interface, designed to cater to the needs of teachers and students without specialized technical expertise. Our usability evaluation with 2 researchers, 2 teachers and 24 students has yielded promising results regarding the system‚Äôs ease of use. Furthermore, this paper offers design guidelines for the development of more user-friendly MMLA systems. These guidelines have significant implications for the broader aim of making MMLA tools accessible to a wider audience, particularly for non-expert MMLA users."
  },
  {
    "objectID": "articles/2025/lak_workshop.html",
    "href": "articles/2025/lak_workshop.html",
    "title": "Teacher-AI complementarity: From design to implementation and reflection",
    "section": "",
    "text": "Chejara, P., Tammets, K., Laanpere, M., Volt, A., Kasepalu, R., Sarmiento-M√°rquez, E. M., & Sillat, L. H. (2024). Teacher-AI complementarity: From design to implementation and reflection. Joint Proceedings of LAK 2025 Workshops, co-located with the 15th International Conference on Learning Analytics and Knowledge (LAK 2025), Dublin, Ireland, March 03‚Äì07, 2025.. https://doi.org/10.1111/bjet.13402"
  },
  {
    "objectID": "articles/2025/lak_workshop.html#citation-apa-7",
    "href": "articles/2025/lak_workshop.html#citation-apa-7",
    "title": "Teacher-AI complementarity: From design to implementation and reflection",
    "section": "",
    "text": "Chejara, P., Tammets, K., Laanpere, M., Volt, A., Kasepalu, R., Sarmiento-M√°rquez, E. M., & Sillat, L. H. (2024). Teacher-AI complementarity: From design to implementation and reflection. Joint Proceedings of LAK 2025 Workshops, co-located with the 15th International Conference on Learning Analytics and Knowledge (LAK 2025), Dublin, Ireland, March 03‚Äì07, 2025.. https://doi.org/10.1111/bjet.13402"
  },
  {
    "objectID": "articles/2025/lak_workshop.html#abstract",
    "href": "articles/2025/lak_workshop.html#abstract",
    "title": "Teacher-AI complementarity: From design to implementation and reflection",
    "section": "Abstract",
    "text": "Abstract\nTraditionally Artificial Intelligence (AI) was primarily focused on building adaptive tutoring systems that mimicked the role of teachers to deliver personalized instruction. Over time, AI applications have expanded to other domains, such as drop-out prediction and performance analytics, with a central goal of understanding and enhancing learning. This expansion has driven the growth of research fields like Educational data mining, Learning Analytics, and AI in Education. These fields have illustrated the potential of AI harnessing data from learning platforms and even from physical classroom spaces. Thus, AI can help teachers to efficiently observe and understand what is happening in their classrooms, augmenting the teacher‚Äôs ability to maximize positive impact on learning. One emerging approach to achieving this synergy between humans and AI is hybrid intelligence, which emphasizes the collaboration and co-evolution of humans and AI. In this paper, we present our ongoing research efforts to design and develop educational technologies with an ability to evolve and adapt from their interactions with teachers and students, and align with human values and norms."
  },
  {
    "objectID": "articles/2022/tkl.html",
    "href": "articles/2022/tkl.html",
    "title": "Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard",
    "section": "",
    "text": "Kasepalu, R., Chejara, P., Prieto, L. P., & Ley, T. (2022). Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard. Technology, Knowledge and Learning, 27(3), 971-989."
  },
  {
    "objectID": "articles/2022/tkl.html#citation-apa-7",
    "href": "articles/2022/tkl.html#citation-apa-7",
    "title": "Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard",
    "section": "",
    "text": "Kasepalu, R., Chejara, P., Prieto, L. P., & Ley, T. (2022). Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard. Technology, Knowledge and Learning, 27(3), 971-989."
  },
  {
    "objectID": "articles/2022/tkl.html#abstract",
    "href": "articles/2022/tkl.html#abstract",
    "title": "Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard",
    "section": "Abstract",
    "text": "Abstract\nMonitoring and guiding multiple groups of students in face-to-face collaborative work is a demanding task which could possibly be alleviated with the use of a technological assistant in the form of learning analytics. However, it is still unclear whether teachers would indeed trust, understand, and use such analytics in their classroom practice and how they would interact with such an assistant. The present research aimed to find out what the perception of in-service secondary school teachers is when provided with a dashboard based on audio and digital trace data when monitoring a collaborative learning activity. In a vignette study, we presented twenty-one in-service teachers with videos from an authentic collaborative activity, together with visualizations of simple collaboration analytics of those activities. The teachers perceived the dashboards as providers of useful information for their everyday work. In addition to assisting in monitoring collaboration, the involved teachers imagined using it for picking out students in need, getting information about the individual contribution of each collaborator, or even as a basis for assessment. Our results highlight the need for guiding dashboards as only providing new information to teachers did not compel them to intervene and additionally, a guiding dashboard could possibly help less experienced teachers with data-informed assessment."
  },
  {
    "objectID": "my_projects.html",
    "href": "my_projects.html",
    "title": "Projects",
    "section": "",
    "text": "Collaboration prediction in real-time\n        Developed an app to allow teachers to create and monitor group activities in their classroom. üèÜ Best Demo Award at LAK'23 Conference, TX, USA\n        \n          \n            \n              scikit-learn\n            \n          \n            \n              pandas\n            \n          \n            \n              matplotlib\n            \n          \n            \n              django\n            \n          \n            \n              plotly\n            \n          \n        \n        \n      \n    \n  \n    \n      \n      \n       \n      \n      \n      \n      \n        Literature Analytics and Visualization\n        Developed a library to process literature review dataset, collected at Harvard University, to visualize key insights\n        \n          \n            \n              pandas\n            \n          \n            \n              numpy\n            \n          \n            \n              matplotlib\n            \n          \n            \n              dash\n            \n          \n            \n              plotly\n            \n          \n        \n        \n      \n    \n  \n    \n      \n      \n       \n      \n      \n      \n      \n        Group Conversation Analytics using Raspberry Pi\n        Developed a IoT prototype to process audio data and visualize group speaking analytics through an interactive dashboard\n        \n          \n            \n              pandas\n            \n          \n            \n              numpy\n            \n          \n            \n              matplotlib\n            \n          \n            \n              dash\n            \n          \n            \n              networkx\n            \n          \n        \n        \n      \n    \n  \n    \n      \n      \n       \n      \n      \n      \n      \n        Moodle plugin to track video watching behavior\n        Developed a plugin for Moodle LMS to track users' video watching behavior using YouTube API\n        \n          \n            \n              YouTube API\n            \n          \n            \n              moodle\n            \n          \n            \n              php\n            \n          \n        \n        \n      \n    \n  \n    \n      \n      \n       \n      \n      \n      \n      \n        Genome sequencing data processing pipelines\n        Developed upstream and downstream pipelines to process amplicon sequencing data\n        \n          \n            \n              pandas\n            \n          \n            \n              matplotlib\n            \n          \n            \n              scikit-learn\n            \n          \n            \n              ggplot2\n            \n          \n            \n              qiime2\n            \n          \n            \n              snakemake\n            \n          \n        \n        \n      \n    \n  \n    \n      \n      \n       \n      \n      \n      \n      \n        Impact of different temporal lenghts on collaboration quality prediction\n        Investigated impact of different temporal lengths on the performance of collaboration prediction model\n        \n          \n            \n              scikit-learn\n            \n          \n            \n              pandas\n            \n          \n            \n              matplotlib\n            \n          \n            \n              dash\n            \n          \n            \n              plotly\n            \n          \n        \n        \n      \n    \n  \n    \n      \n      \n       \n      \n      \n      \n      \n        Bayesian modeling of students' mathematics skills\n        Processed and analyzed students' interaction data from a learning platform to model their mathematics skill mastery levels for 9th standard Algebra curriculam\n        \n          \n            \n              pgmpy\n            \n          \n            \n              pandas\n            \n          \n            \n              numpy\n            \n          \n            \n              matplotlib\n            \n          \n            \n              dash\n            \n          \n            \n              plotly\n            \n          \n        \n        \n      \n    \n  \n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "projects/cq_window.html#project-overview",
    "href": "projects/cq_window.html#project-overview",
    "title": "Impact of different temporal lenghts on collaboration quality prediction",
    "section": "Project overview",
    "text": "Project overview\nThis project aimed at identifying optimal temporal window size to aggregate multimodal data into features that can improve generalizability of collaboration prediction models. The project analyzed audio and logs data collected from groups working to solve a given problem in collaboration.\n\nResearch Paper\nThe findings of this project were published in the Learning Analytics & Knowledge conference organized in Arizona, USA in 2023. You can access the paper here\n\n\n\n\n\n\nüéñÔ∏è Honorable Mention:\n\n\n\nThis paper received an Honorable Mention award (top 5% of submissions) at the prestigious Learning Analytics and Knowledge (LAK‚Äô23) conference.\n\n\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nProcessed audio and logs data using different temporal window sizes\nDeveloped machine learning models for predicting collaboration using processed data\nEvaluated models at different levels of generalizability"
  },
  {
    "objectID": "projects/cq_window.html#skills-applied",
    "href": "projects/cq_window.html#skills-applied",
    "title": "Impact of different temporal lenghts on collaboration quality prediction",
    "section": "Skills Applied",
    "text": "Skills Applied\nPython Programming, Web Development, DevOps, Bayesian Modeling"
  },
  {
    "objectID": "projects/cq_window.html#libraries-used",
    "href": "projects/cq_window.html#libraries-used",
    "title": "Impact of different temporal lenghts on collaboration quality prediction",
    "section": "Libraries Used",
    "text": "Libraries Used\npgmpy,pandas,matplotlib,numpy,dash"
  },
  {
    "objectID": "projects/vidtrack.html#project-overview",
    "href": "projects/vidtrack.html#project-overview",
    "title": "Moodle plugin to track video watching behavior",
    "section": "Project overview",
    "text": "Project overview\nMoodle Activity Plugin is a Moodle activity plugin that enables seamless integration of YouTube videos into courses while capturing detailed student interactions. Using the YouTube iFrame API, it embeds videos and records user engagement events for later analysis.\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nIntegrated YouTube API for tracking users‚Äô interactions with videos\nDeveloped a tracking component to store all interactions (e.g., play, pause, end)"
  },
  {
    "objectID": "projects/vidtrack.html#skills-applied",
    "href": "projects/vidtrack.html#skills-applied",
    "title": "Moodle plugin to track video watching behavior",
    "section": "Skills Applied",
    "text": "Skills Applied\nPHP, Web Development, Moodle"
  },
  {
    "objectID": "projects/vidtrack.html#libraries-used",
    "href": "projects/vidtrack.html#libraries-used",
    "title": "Moodle plugin to track video watching behavior",
    "section": "Libraries Used",
    "text": "Libraries Used\nmoodle, YouTube API"
  },
  {
    "objectID": "projects/harvard.html#project-overview",
    "href": "projects/harvard.html#project-overview",
    "title": "Literature Analytics and Visualization",
    "section": "Project overview",
    "text": "Project overview\nThis project was completed during my research stay at Harvard University, USA in 2023. The project goal was to systematize the processing of literature review dataset consisting of around 150 research studies in the area of Multimodal Collaboration Analytics, and provide an interactive dashboard for users to explore the dataset.\nThe project was developed in collaboration with Prof.¬†Bertrand Schneider from Learning, Innovation and Technology Lab at Harvard Graduate School of Education.\n\nTechnical Implementation\nThe solution combines a Python library and a dashboard application built upon the library.\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nDeveloped a Python library for systematizing the processing of literature review dataset.\nBuilt an interactive dashboard (web-based) to visualize key insights."
  },
  {
    "objectID": "projects/harvard.html#skills-applied",
    "href": "projects/harvard.html#skills-applied",
    "title": "Literature Analytics and Visualization",
    "section": "Skills Applied",
    "text": "Skills Applied\nPython Programming, Web Development, DevOps"
  },
  {
    "objectID": "projects/harvard.html#libraries-used",
    "href": "projects/harvard.html#libraries-used",
    "title": "Literature Analytics and Visualization",
    "section": "Libraries Used",
    "text": "Libraries Used\npandas,matplotlib,cytoscape,numpy, dash"
  },
  {
    "objectID": "projects/harvard.html#try-it-now",
    "href": "projects/harvard.html#try-it-now",
    "title": "Literature Analytics and Visualization",
    "section": "Try it now",
    "text": "Try it now\nCheck the live application here (The application is using free tier on Render which may cause a delay in loading of the application.)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Teacher-AI complementarity: From design to implementation and reflection\n      \n               Learning Analytics & Knowledge Conference, Dublin  \n             | Ireland\n            03 Mar 2025\n         \n      \n      \n          This talk was at the Hybrid Intelligence: Human-AI Collaboration and Learning workshop at 15th International Conference of Learning Analytics & Knowledge. In this talk, I presented three technological AI tools from Tallinn University as cases of human-AI hybrid intelligence from a complementarity point of view.\n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Classroom collaboration analytics\n      \n               Learning Analytics Course  \n             | Remote\n            26 September 2024\n         \n      \n      \n          I gave this talk (remote) in a joint Learning Analytics course run by KTH Royal Institute of Technology and University of Bergen coordinated by Prof. Deniel Spikol. The talk shared current research in the area of multimodal collaboration analytics.\n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Designing and building automated systems for collaboration monitoring in classroom settings\n      \n               PhD Defense, Tallinn  \n             | Estonia\n            15 August 2024\n         \n      \n      \n          This talk was given to defend my PhD thesis on the topic of Classroom collaboration analytics: designing and building automated systems for collaboration monitoring in classroom settings at Tallinn University \n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Bringing Collaborative Analytics using Multimodal Data to the Masses\n      \n               Learning Analytics & Knowledge Conference, Kyoto  \n             | Japan\n            20 Mar 2024\n         \n      \n      \n          This talk was given at the 14th International Conference of Learning Analytics & Knowledge to present results from the evaluation of CoTrack (a multimodal learning analytics tool), also published in the form of a conference paper. \n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics\n      \n               EC-TEL Conference, Aveiro  \n             | Portugal\n            13 September 2023\n         \n      \n      \n          This talk was given at the European conference of technology-enhanced learning to present results from a research study exploring relationships between multimodal data and collaboration quality. \n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      How to build more generalizable models for collaboration quality?\n      \n               Learning Analytics & Knowledge Conference, Dallas  \n             | United States\n            16 March 2023\n         \n      \n      \n          This talk presented a research paper at the 14th International Conference of Learning Analytics & Knowledge organized at Arizona State University, Arizona. The paper included results from a study exploring methodologies to build generalizable collaboration prediction models for authentic classroom settings.\n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Impact of temporal window on the performance of collaboration prediction models\n      \n               Learning Analytics & Knowledge Conference, Dallas  \n             | United States\n            15 March 2023\n         \n      \n      \n          This talk presented a research paper at the 14th International Conference of Learning Analytics & Knowledge organized at Arizona State University, Arizona. The paper included results from a study investigating impact of using different temporal window sizes for feature aggregation on the performance of collaboration prediction models for authentic classroom settings.\n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Automated estimation of collaboration quality using multimodal data in classroom\n      \n               Research Seminar, Valladolid  \n             | Spain\n            21 Feb 2022\n         \n      \n      \n          This talk was given during my research stay at the GSIC-EMIC research lab at the University of Valladolid, Spain to introduce my research on Multimodal Learning Analytics to explore collaboration analytics. The talk was attended by most of the lab members including Prof. Yannis Dimitriadis, Prof. Alejandra Mart√≠nez Mon√©s, and Prof. Juan Ignacio Asensio P√©rez.\n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Building an automated system to detect collaboration quality using multimodal learning analytics\n      \n               Research Seminar, Bergen  \n             | Norway\n            12 January 2022\n         \n      \n      \n          This talk was given during my research stay at the SLATE research group at the University of Bergen, Norway. The talk presented research work on building automated systems to support teachers during collaborative learning activities in classrooms.\n      \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      Supporting Teachers during Collaborative Learning Using MMLA and AI\n      \n               Research Seminar, Tampere  \n             | Finland\n            14 September 2021\n         \n      \n      \n          This talk was given at Tampere University in Finland during a short research visit. In this talk, I presented my research on the potential of Multimodal Learning Analytics to support teachers in collaborative learning activities with monitoring.\n      \n    \n  \n\nNo matching items\n\n  \n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/data_r.html",
    "href": "posts/post-with-code/data_r.html",
    "title": "R tutorial on accessing, filtering, aggregating and plotting data",
    "section": "",
    "text": "This post offers a tutorial on how to access, filter, aggregate and plot the data in R. It will mainly covers following topics\n\nHow to access a particular column or row from a dataframe?\nHow to access rows with conditions?\nHow to generate group wise statistics (e.g., salary statistics for male and female groups)?\nHow to create new column in a dataframe?\nHow to plot distribution of data?\nHow to plot group-wise stats?\n\n\n\nThis tutorial requires you to have R-Studio installed on your system. In case if you don‚Äôt have R-Studio on your system, you can access it online by going to rstudio-cloud.\n\n\nGo to your R-Studio and execute following command\ninstall.packages('rio')\ninstall.packages('dplyr')\ninstall.packages('lattice')\nThe above commands will install three pacakages (rio)[https://cran.r-project.org/web/packages/rio/vignettes/rio.html], (dplyr)[https://www.rdocumentation.org/packages/dplyr/versions/0.7.8], and (ggplot2)[https://ggplot2.tidyverse.org/]. We will use these packages in our tutorials.\n\n\n\n\nIn our first tutorial, we will learn how to load dataset into R-studio. For this tutorial, we will use a very simple dataset available here. This dataset contains four attributes (name, age, salary, expenses) (given below).\n\n\n\nname\nage\nsalary\nexpenses\n\n\n\n\nmac\n21\n15000\n600\n\n\nravi\n25\n18000\n800\n\n\ndavid\n27\n17000\n600\n\n\nmoorey\n43\n33000\n1200\n\n\nnolan\n33\n24000\n900\n\n\n\nWe can either use R-studio GUI or we can write code to load the dataset. So let‚Äôs say you have downloaded the dataset and saved it. You can write following codes to load the dataset into R.\n# load the package\nlibrary(rio)\n\n# open dataset file\n# Specify your dataset filename with complete path\ndata &lt;‚Äì import('filename')\nFollowing is the demonstration of both ways for loading the dataset.\n\n\n\n\n\nSo we have loaded the dataset into R and now we want to access two attributes name and salary using dplyr package.\nWe will use following syntax\nselect(data_object,attr_name,attr_name,..)\nOR\ndata_object %&gt;% select(attr_name,attr_name,..)\nHere, data_object is the object name you have used to load your dataset. You then specify the attributes name which you want to access.\nIn our case, we want to access name and salary. Therefore, we will specify these attributes in select().\nlibrary(dplyr)\nlibrary(rio)\n# load dataset\ndataset &lt;- import('sample_data.csv')\n\n# access name and salary attributes\ndataset %&gt;% select(name,salary)\nFollowing is our code from R-Studio.\n\n\n\nScreenshot 2021-02-26 at 11.23.52\n\n\nSimilarly you can specify other attributes name which you want to access from your data. You can refer this link to get more information on usage of select().\nNow, we will look at rows selection. Let‚Äôs say we want to fetch data of peple whose salary is higher than 20000.\nWe will use following syntax\nfilter(data_object,condition)\nOR\ndata_object %&gt;% filter(condition)\nTo fetch our data, we will use the condition salary &gt; 20000. The snapshow below shows the result.\n\n\n\nScreenshot 2021-02-26 at 11.36.23\n\n\nYou can refer here for more information on filter() function. Following are some other examples\n\nAccessing the last row\nslice_tail(data_object)\nAccessing the first row\nslice_head(data_object)\nAccessing row with minimum value of an attribute (e.g., who has the minimum salary)\nslice_min(data_object, attr_name)\nAccessing row with maximum value of an attribute (e.g., who has the maximum salary)\nslice_max(data_object, attr_name)\n\n\n\n\nWe will take an example to understand this scenario. Let‚Äôs say we want to calculate the saving for each of the employee (from our salary dataset). So we want to have another attribute (saving) which will simply contains the amount of salary left after subtracting the expenses.\nWe will use mutate() function for this task. Following is the syntax\ndata_object %&gt;% mutate(expr)\nOR\nmutate(data_object,expr)\nHere expr means the expression that will be used to generate new attribute. In our case, the saving can be compute by subtracting expenses from salary. Therefore, expr is saving = salary - expenses.\n\n\n\nScreenshot 2021-02-26 at 11.51.01\n\n\nRefer here for more detailed information on mutate() function.\n\n\n\nWhiile analyzing the dataset, we are often interested in group-wise statistics. For example, what is average salary for younger and older people or what is difference in sleeping hours in male and female.\nTo answer these question, we need to way to first group the dataset and then compute required statistics.\nFor our dataset, we can have this question: what is average salary of people who are younger and older than 30 years. So this we will use group_by() and summarise() functions from dplyr package.\nFollowing is the representation of how the processing needs to be done.\nIn R, following is the syntax\ndata_object %&gt;% group_by(attribute_name_used_for_grouping or condition) %&gt;% summarize(statistics_you_need)\nhere first we need to specify attribute which will be used for grouping and then we will specify what summary stats we need. Following are the options for summary stats\n\nmean(), median()\nsd(), IQR()\nmin(), max()\nn()\n\nFor more details refer this\nNow, let‚Äôs findout average salary of people who are younger and older than 30 years. So, here we need a condition for grouping. For example, if age &lt; 30 then employee belongs to group-1 otherwise he/she belongs to group-2. Then we need to specify the attribute name for computing average.\nFollowing is the illustration of finding group-wise average.\n\n\n\nScreenshot 2021-02-26 at 12.25.21\n\n\nNow, we will do the same using R. For grouping we will use group_by() function. Here we need to specify the condition. Then we will summarise using summarise() function. Here we need to specify the statistics (e.g., mean, standard deviation, count) and the attribute/s name which will be used to compute statistics\n\n\n\nScreenshot 2021-02-26 at 12.29.02\n\n\n\n\n\nIn this section, we will use lattice package from R to produce graphs. We will cover create following graphs\n\nBar chart\nBox plot\nDensity plot\nHistogram\nScatter plot\n\nYou can use following syntax to create graphs using lattice package\ngraph_type(formula,data=)\nHere, graph_type is the name of graph which you want to plot. In formula, you need to specify what you want to plot. For example, if you want to plot a salary attribute from our dataset, you can write it as ~salary. If you want to plot two attributes (e.g., age and salary) then you can either write age~salary or salary~age.\nFollowing table shows the graph_type for each of aforementioned graph\n\n\n\ngraph_type\nWhat it plots\n\n\n\n\nbarchart\nbar chart\n\n\nbwplot\nboxplot\n\n\nDensityplot\nkernel density plot\n\n\nhistogram\nhistogram\n\n\nxyplot\nscatterplot matrix\n\n\n\nWe will learn more about how to write these formuls in our examples.\n\nLet‚Äôs plot a histogram for salary attribute from our dataset. For this, we will use histogram graph type. We will specify the name of the attribute ~salary.\ndataset &lt;- import(your_file_name_path)\nhistogram(~salary,data=dataset)   \n\n\n\n\nScreenshot 2021-02-26 at 17.57.38\n\n\n\n\n\nimage-20210226175840188\n\n\n\nNow, we will plot the scatterplot for age and salary. We will use xyplot.\n# You already have your dataset loaded in object dataset\nxyplot(age~salary,data=dataset)\n\n\n\nimage-20210226180034354\n\n\nLet‚Äôs plot a boxplot for salary attribute.\nbwplot(~salary,data=dataset   )\n\n\n\n\nimage-20210226180350357"
  },
  {
    "objectID": "posts/post-with-code/data_r.html#plotting-the-data",
    "href": "posts/post-with-code/data_r.html#plotting-the-data",
    "title": "R tutorial on accessing, filtering, aggregating and plotting data",
    "section": "",
    "text": "In this section, we will use lattice package from R to produce graphs. We will cover create following graphs\n\nBar chart\nBox plot\nDensity plot\nHistogram\nScatter plot\n\nYou can use following syntax to create graphs using lattice package\ngraph_type(formula,data=)\nHere, graph_type is the name of graph which you want to plot. In formula, you need to specify what you want to plot. For example, if you want to plot a salary attribute from our dataset, you can write it as ~salary. If you want to plot two attributes (e.g., age and salary) then you can either write age~salary or salary~age.\nFollowing table shows the graph_type for each of aforementioned graph\n\n\n\ngraph_type\nWhat it plots\n\n\n\n\nbarchart\nbar chart\n\n\nbwplot\nboxplot\n\n\nDensityplot\nkernel density plot\n\n\nhistogram\nhistogram\n\n\nxyplot\nscatterplot matrix\n\n\n\nWe will learn more about how to write these formuls in our examples.\n\nLet‚Äôs plot a histogram for salary attribute from our dataset. For this, we will use histogram graph type. We will specify the name of the attribute ~salary.\ndataset &lt;- import(your_file_name_path)\nhistogram(~salary,data=dataset)   \n\n\n\n\nScreenshot 2021-02-26 at 17.57.38\n\n\n\n\n\nimage-20210226175840188\n\n\n\nNow, we will plot the scatterplot for age and salary. We will use xyplot.\n# You already have your dataset loaded in object dataset\nxyplot(age~salary,data=dataset)\n\n\n\nimage-20210226180034354\n\n\nLet‚Äôs plot a boxplot for salary attribute.\nbwplot(~salary,data=dataset   )\n\n\n\n\nimage-20210226180350357"
  },
  {
    "objectID": "posts/post-with-code/agreement.html",
    "href": "posts/post-with-code/agreement.html",
    "title": "Computing inter-rater aggrement scores using Python",
    "section": "",
    "text": "Recently, I was involved in some annotation processes involving two coders and I needed to compute inter-rater reliability scores. There are multiple measures for calculating the agreement between two or more than two coders/annotators.\n\n\n\n\nIf you have a question regarding ‚Äú‚Äúwhich measure to use in your case?‚Äù‚Äú, I would suggest reading (Hayes & Krippendorff, 2007) which compares different measures and provides suggestions on which to use when.\n\n\n\n\nIn this post, I am sharing some of our python code on calculating various measures for inter-rater reliability.\n\n\n\n\nCohen‚Äôs Kappa\n\n\n\n\nWe will start with Cohen‚Äôs kappa. Let‚Äôs say we have two coders who have coded a particular phenomenon and assigned some code for 10 instances. Now let‚Äôs write the python code to compute cohen‚Äôs kappa.\n\n\n\n\nYou can use either sklearn.metrics or nltk.agreement to compute kappa. We will see examples using both of these packages.\n\nfrom sklearn.metrics import cohen_kappa_score\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\ncoder2 = [1,1,0,0,1,1,2,1,1,0]\nscore = cohen_kappa_score(coder1,coder2)\n\nprint('Cohen\\'s Kappa:',score)\n\n# output\n# Cohen's Kappa: 0.3220338983050848\n\n\nIn order to use nltk.agreement package, we need to structure our coding data into a format of [coder, instance, code]. For instance, the first code in coder1 is 1 which will be formatted as [1,1,1] which means coder1 assigned 1 to the first instance.\n\n\n\n\nLet‚Äôs convert our codes given in the above example in the format of [coder,instance,code]. Here we have two options to do that. I have included the first option for better understanding. Second option is a short one line solution to our problem.\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\n\ncoder1_new = []\ncoder2_new = []\nfor i in range(len(coder1)):\n    coder1_new.append([1,i,coder1[i]])\n    coder2_new.append([2,i,coder2[i]])\n\n\nformatted_codes = coder1_new + coder2_new\nprint(formatted_codes)\n[[1, 0, 1], [1, 1, 0], [1, 2, 2], [1, 3, 0], [1, 4, 1], [1, 5, 1], [1, 6, 2], [1, 7, 0], [1, 8, 1], [1, 9, 1], [2, 0, 1], [2, 1, 1], [2, 2, 0], [2, 3, 0], [2, 4, 1], [2, 5, 1], [2, 6, 2], [2, 7, 1], [2, 8, 1], [2, 9, 0]]\nformatted_codes = [[1,i,coder1[i]] for i in range(len(coder1))] + [[2,i,coder2[i]] for i in range(len(coder2))] \nprint(formatted_codes)\n\n\n[[1, 0, 1], [1, 1, 0], [1, 2, 2], [1, 3, 0], [1, 4, 1], [1, 5, 1], [1, 6, 2], [1, 7, 0], [1, 8, 1], [1, 9, 1], [2, 0, 1], [2, 1, 1], [2, 2, 0], [2, 3, 0], [2, 4, 1], [2, 5, 1], [2, 6, 2], [2, 7, 1], [2, 8, 1], [2, 9, 0]]\n\n\n\nNow, we have our codes in the required format, we can compute cohen‚Äôs kappa using nltk.agreement.\n\n\nfrom nltk import agreement\n\nratingtask = agreement.AnnotationTask(data=formatted_codes)\n\nprint('Cohen\\'s Kappa:',ratingtask.kappa())\n\n\nCohen's Kappa: 0.32203389830508466\n\n\n\nCohen‚Äôs Kappa using CSV files\n\n\n\n\nIn this section, we will see how to compute cohen‚Äôs kappa from codes stored in CSV files. So let‚Äôs say we have two files (coder1.csv, coder2.csv). Each of these files has some columns representing a dimension. Below is the snapshot of such a file.\n\n\n\n\nThe files contain 10 columns each representing a dimension coded by first coder. We have a similar file for coder2 and now we want to calculate Cohen‚Äôs kappa for each of such dimensions.\n\n\n\n\n\nSMU\nCF\nKE\nARG\nSTR\nCO\nu1\nu2\nu3\nu4\n\n\n\n\n1\n1\n1\n0\n0\n1\n2\n1\n1\n0\n\n\n1\n1\n1\n0\n0\n2\n1\n2\n1\n1\n\n\n2\n2\n1\n-1\n0\n1\n2\n1\n2\n2\n\n\n2\n2\n1\n1\n0\n2\n2\n2\n2\n1\n\n\n1\n2\n1\n1\n0\n2\n2\n2\n1\n2\n\n\n1\n1\n1\n1\n0\n1\n2\n2\n1\n2\n\n\n2\n1\n1\n1\n0\n2\n2\n2\n1\n2\n\n\n2\n1\n2\n2\n0\n2\n1\n2\n2\n2\n\n\n2\n2\n2\n2\n0\n2\n2\n2\n2\n2\n\n\n\n\n\nWe will use pandas python package to load our CSV file and access each dimension code (Learn basics of Pandas Library).\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import cohen_kappa_score\n\ncoder1 = pd.read_csv('coder1.csv')\ncoder2 = pd.read_csv('coder2.csv')\n\ndimensions = coder1.columns\n\n#iterate for each dimension\nfor dim in dimensions:\n   \n    dim_codes1 = coder1[dim]\n    \n    dim_codes2 = coder2[dim]\n    print('Dimension:',dim)\n    \n    score = cohen_kappa_score(dim_codes1,dim_codes2)\n    \n    print(' ',score)\n\nDimension: SMU\n  0.3076923076923077\nDimension: CF\n  0.55\nDimension: KE\n  0.12903225806451613\nDimension: ARG\n  0.6896551724137931\nDimension: STR\n  0.0\nDimension: CO\n  -0.19999999999999996\nDimension: u1\n  0.0\nDimension: u2\n  0.0\nDimension: u3\n  0.3414634146341463\nDimension: u4\n  0.4375\n\n\n\nFleiss‚Äôs Kappa\n\n\n\n\nAs per my understanding, Cohen‚Äôs Kappa can be used if you have codes from only two coders. In case, if you have codes from multiple coders then you need to use Fleiss‚Äôs kappa.\n\n\n\n\nWe will use nltk.agreement package for calculating Fleiss‚Äôs Kappa. So now we add one more coder‚Äôs data to our previous example.\n\n\n\nfrom nltk import agreement\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\ncoder2 = [1,1,0,0,1,1,2,1,1,0]\ncoder3 = [1,2,2,1,2,1,2,1,1,0]\n\nformatted_codes = [[1,i,coder1[i]] for i in range(len(coder1))] + [[2,i,coder2[i]] for i in range(len(coder2))]  + [[3,i,coder3[i]] for i in range(len(coder3))]\n\n\nratingtask = agreement.AnnotationTask(data=formatted_codes)\n\nprint('Fleiss\\'s Kappa:',ratingtask.multi_kappa())\n\n\nFleiss's Kappa: 0.3010752688172044\n\n\n\nFleiss‚Äôs Kappa using CSV files\n\n\n\n\nNow, let‚Äôs say we have three CSV files, one from each coder. Each coder assigned codes on ten dimensions (as shown in the above example of CSV file). The following code compute Fleiss‚Äôs kappa among three coders for each dimension.\n\n\n\nimport pandas as pd\nfrom nltk import agreement\n\n\ncoder1 = pd.read_csv('coder1.csv')\ncoder2 = pd.read_csv('coder2.csv')\ncoder3 = pd.read_csv('coder3.csv')\n\ndimensions = coder1.columns\n\n\nfor dim in dimensions:\n   \n    dim_codes1 = coder1[dim]\n    dim_codes2 = coder2[dim]\n    dim_codes3 = coder3[dim]\n    \n    formatted_codes = [[1,i,dim_codes1[i]] for i in range(len(dim_codes1))] + [[2,i,dim_codes2[i]] for i in range(len(dim_codes2))]  + [[3,i,dim_codes3[i]] for i in range(len(dim_codes3))]\n    \n    ratingtask = agreement.AnnotationTask(data=formatted_codes)\n    print('Dimension:')\n    print(' Fleiss\\'s Kappa:',ratingtask.multi_kappa())\n\n\n\nCronbach‚Äôs Alpha\n\n\n\n\nCronbach‚Äôs alpha is mostly used to measure the internal consistency of a survey or questionnaire. For this measure, I am using Pingouin package (&lt;a href=‚Äú‚Äúhttps://pingouin-stats.org/index.html‚Äù‚Äú&gt;link).\n\n\n\n\nLet‚Äôs say we have data from a questionnaire (which has questions with Likert scale) in a CSV file. For example, I am using a dataset from Pingouin with some missing values.\n\n\n\nimport pingouin as pg\n\ndata = pg.read_dataset('cronbach_wide_missing')\n\ndata.head()\n\n\nQ1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11\n1.0,1,1.0,1,1.0,1,1,1,1.0,1,1\n1.0,1,1.0,1,1.0,1,1,1,0.0,1,0\n,0,1.0,1,,1,1,1,1.0,0,0\n1.0,1,1.0,0,1.0,1,0,1,1.0,0,0\n1.0,1,1.0,1,1.0,0,0,0,1.0,0,0\n0.0,1,,0,1.0,1,1,1,0.0,0,0\n1.0,1,1.0,1,0.0,0,1,0,0.0,0,0\n1.0,1,1.0,1,1.0,0,0,0,0.0,0,0\n0.0,1,0.0,1,1.0,0,0,0,0.0,1,0\n1.0,0,0.0,1,0.0,1,0,0,,0,0\n1.0,1,1.0,0,0.0,0,0,0,0.0,0,0\n1.0,0,0.0,1,0.0,0,0,0,0.0,0,0\n\n\npg.cronbach_alpha(data=data)\n\n\n(0.732661, array([0.435, 0.909]))\n\n\n\nKrippendorff‚Äôs Alpha & Scott‚Äôs Pi\n\n\n\n\nWe can use nltk.agreement python package for both of these measures. I will show you an example of that.\n\n\n\n\nFor nltk.agreement, we need our formatted data (what we did in the previous example?). Once we have our formatted data, we simply need to call alpha function to get the Krippendorff‚Äôs Alpha. Let‚Äôs see the python code.\n\n\n\nfrom nltk import agreement\n\ncoder1 = [1,0,2,0,1,1,2,0,1,1]\ncoder2 = [1,1,0,0,1,1,2,1,1,0]\ncoder3 = [1,2,2,1,2,1,2,1,1,0]\n\nformatted_codes = [[1,i,coder1[i]] for i in range(len(coder1))] + [[2,i,coder2[i]] for i in range(len(coder2))]  + [[3,i,coder3[i]] for i in range(len(coder3))]\n\n\nratingtask = agreement.AnnotationTask(data=formatted_codes)\n\nprint('Krippendorff\\'s alpha:',ratingtask.alpha())\nprint('Scott\\'s pi:',ratingtask.pi())\n\n\nKrippendorff's alpha: 0.30952380952380953\nScott's pi: 0.2857142857142859\n\n\n\nInter-class correlation\n\n\n\n\nI am using Pingouin package mentioned before as well. The function used is intraclass_corr. This function returns a Pandas Datafame having the following information (from R package psych). Six cases are returned (ICC1, ICC2, ICC3, ICC1k, ICCk2, ICCk3) by the function and the following are the meaning for each case.\n\n\n\n\nShrout and Fleiss (1979) consider six cases of reliability of ratings done by k raters on n targets.\n\n\n\n\nICC1: Each target is rated by a different judge and the judges are selected at random. (This is a one-way ANOVA fixed effects model and is found by (MSB- MSW)/(MSB+ (nr-1)*MSW))\n\n\n\n\n\nICC2: A random sample of k judges rate each target. The measure is one of absolute agreement in the ratings. Found as (MSB- MSE)/(MSB + (nr-1)MSE + nr(MSJ-MSE)/nc)\n\n\n\n\n\nICC3: A fixed set of k judges rate each target. There is no generalization to a larger population of judges. (MSB - MSE)/(MSB+ (nr-1)*MSE) \n\n\n\nThen, for each of these cases, there are second variant (e.g., ICC1k). The difference between the two variants of classes is that in the first case, the 1 rating is equivalent to the average intercorrelation, while, the k rating case uses Spearman Brown adjusted reliability.)\n\n\n\n\nICC1 is sensitive to differences in means between raters and is a measure of absolute agreement.\n\n\n\n\nICC2 and ICC3 remove mean differences between judges, but are sensitive to interactions of raters by judges. The difference between ICC2 and ICC3 is whether raters are seen as fixed or random effects.\n\n\n\n\nICC1k, ICC2k, ICC3K reflect the means of k raters.\n\n\n\n\nThe dataset from Pingouin has been used in the following example.\n\n\nimport pingouin as pg\n\ndata = pg.read_dataset('icc')\n\nicc = pg.intraclass_corr(data=data, targets='Wine', raters='Judge',ratings='Scores')\nicc\n\n\n     Type    Description     ICC     F   df1     df2     pval    CI95%\n0   ICC1    Single raters absolute  0.728   11.680  7   24  0.000002    [0.43, 0.93]\n1   ICC2    Single random raters    0.728   11.788  7   21  0.000005    [0.43, 0.93]\n2   ICC3    Single fixed raters     0.730   11.788  7   21  0.000005    [0.43, 0.93]\n3   ICC1k   Average raters absolute     0.914   11.680  7   24  0.000002    [0.75, 0.98]\n4   ICC2k   Average random raters   0.914   11.788  7   21  0.000005    [0.75, 0.98]\n5   ICC3k   Average fixed raters    0.915   11.788  7   21  0.000005    [0.75, 0.98]\n\n\n\nReferences\n\n\n\n\n\nHayes, A. F., & Krippendorff, K. (2007). Answering the Call for a Standard Reliability Measure for Coding Data. Communication Methods and Measures, 1(1), 77‚Äì89. https://doi.org/10.1080/19312450709336664\n\n\n\nImage by katemangostar on Freepik ‚Äù\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/dash.html",
    "href": "posts/post-with-code/dash.html",
    "title": "Introduction to Python Dash Framework",
    "section": "",
    "text": "Dashboards play a crucial role in conveying useful and actionable information from collected data regardless of context. As with economically feasible sensors, improved computation, storage facility, and IoT framework, it has become easier to collect an enormous amount of data from a variety of contexts (e.g.¬†military, health-care, education). However, finding insights from collected data remains a challenge. This post offers an introduction to Python Dash a framework that is a great option for dashboard development. I personally really like this framework. It allows me to process my data along with its visualization through a web-based application."
  },
  {
    "objectID": "posts/post-with-code/dash.html#dash-framework",
    "href": "posts/post-with-code/dash.html#dash-framework",
    "title": "Introduction to Python Dash Framework",
    "section": "Dash Framework",
    "text": "Dash Framework\ndash uses plotly graphics library for plotting graphs. In addition to their graphic feature, these graphs also have a lot of good features (e.g.¬†automatic scaling of axis for timestamp data, etc). Please refer dash documentation for details.\n\nInstallation\npip install dash-html-components==0.14.0  # HTML components\npip install dash-core-components==0.44.0  # dash core components\npip install dash-table==3.6.0  # Interactive DataTable component (new!)\n\n\nFirst dash application\nNow, we are going to develop our first dash application. For this application, we are going to plot the following data (some records are taken from here. This data has two attributes (House price and area).\nA dash application can have number of components (e.g.¬†div block, table, headings, graph). In our application, we are going to use two components - heading and a graph. Let‚Äôs begin developing it. First of all, we need to import the required packages\nimport dash_core_components as dcc\nimport dash_html_components as html\nThe first package is used to create an object of dash application. Second package dash_core_components provides graph components and the third package allows us to include html components (heading, input box) in our application.\nNext, we need to create a dash application.\napp = dash.Dash(__name__)\nname is a special variable in python which contains name of current module. For instance, when you run your python program using command prompt then it contains main.\nNow, we will create components to embed in our application. Just to have a clear idea, we want to create following structured with our application.\n\n\n First Dash Application \n\n graph here.. \n\n\nFor components, we will use dash_html_components and for graph, we will use dash_core_components.\nlist_of_data = [{\n    'y':[114300,114200,114800,94700,119800,114600],\n    'x':[1790,2030,1740,1980,2130,1780],\n    'type':'bar'\n}]\n\nbody = html.Div([\n    html.H2(\"\"First Dash Application\"\"),\n    dcc.Graph(id='first',\n        figure={'data':list_of_data})\n])\nIn the above code, we created the body of our application which is a div block. In this block, we created one H2 heading component and one Graph component. Graph has a attribute figure where we have specified the data to be plotted. The data (list_of_data) is actually a list of dictionaries (It might seems a bit confusing but you will be good after writing some codes). One important thing-&gt; we used ‚Äòtype‚Äô:‚Äòbar‚Äô which specify that we want to plot Bar chart.\nNext, we will set the layout of the application.\napp.layout = html.Div([body])\nFinally, we will start the server\nif __name__ == \"\"__main__\"\":\n    app.run_server(debug=True)\nYou can download this script from here.\nNow, we will execute our application. The execution of our application will start a server on the port 8050\n\n\n\nRunning the server\n\n\nIn order to see the output of the program, open your browser and type http://127.0.0.1:8050 in the address bar. It will show you the following screen\n\n\n\np6.3.png\n\n\nCheck for more components: dash_core_components, dash_html_components.\n\n\nAdding CSS style to the app\nThe next step towards generating a beautiful dashboard is to add a styling feature. We can use css stylesheet in our application. It can be specified at the time of creating an application.\napp = dash.app = dash.Dash(__name__,external_stylesheets=style)\nWith the above method, you can only add css which are available online. In case if you want to add your local css file, follow the given steps\n\nCreate a folder with name asset in the same directory where your dash script is stored.\nCopy your stylesheet in the asset folder.\nAdd the path in your program\n\ndash.Dash(__name__,external_stylesheets=[\"\"\\asset\\css-file-name\"\"])\n\n\nInstalling Bootstrap component for dash\ndash also supports Bootstrap which is a widely used css library. It can be added in your dash application using dash-bootstrap-component package (complete documentation). This package allows an easy integration of Bootstrap in the dash application.\nYou can install it using the following command.\npip install dash-bootstrap-components\nNow, let‚Äôs use it to add a CSS to our first example. We are going to create the following layout for our application. We will utilize Bootstrap‚Äôs grid system for structuring our components.\n\n\n\nboot.png\n\n\nFirst, we need to import dash_bootstrap_components in our previous example.\nNext, we will add bootstrap css to your program and then we will create our layout.\nimport dash-bootstrap-components as dbc\napp = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# column1 content\ncolumn_1 = [\n    html.H2(\"\"House Prices\"\"),\n    html.P(\"\"This is a demo application which shows house prices with house area.\"\"\n    ),\n    dbc.Button(\"\"Details\"\", color=\"\"secondary\"\"),\n]\n\n# column content\ncolumn_2 = [\n    html.H2(\"\"Graph\"\"),\n    dcc.Graph(id='first',\n        figure={'data':list_of_data}),\n]\n\n# Creating layout\nbody = dbc.Container(\n    [   html.H1('With Bootstrap'),\n        html.Hr(),\n        dbc.Row(\n            [\n                dbc.Col(column_1,md=4),\n                dbc.Col(column_2,md=8),\n            ]\n        )\n    ]\n)\n\n# Adding CSS\n# dash-bootstrap-components has CDN for bootstrap css in dbc.themes.BOOTSTRAP\napp = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP])\nYou can chek the above source code here."
  },
  {
    "objectID": "posts/post-with-code/Train your word embeddings from scratch.html",
    "href": "posts/post-with-code/Train your word embeddings from scratch.html",
    "title": "Guide to train word embeddings from scratch",
    "section": "",
    "text": "This blog post explains the procedure to train word embeddings from scratch. Word embeddings are dense vector representation of text data (Check this blog pos for the basics).\nWord embeddings can be learned from a text corpora in both, supervised and unsupervised ways. In supervised way, we prepare our text data in such a way that each data record has a ground truth. While, the unsupervised way utilizes the text data without any ground truth.\nIn this post, we will explore a supervised way of learning word embeddings i.e., Continuous Bag of Words (CBOW).\n\nContinuous Bag of Words (CBOW)\nIn CBOW, we aim to predict a word given its surrounding words in a pre-defined size of window.\nFor our task, we will use free text available through Project Gutenberg. There are thousands of books are available. For our task, we will use ‚ÄòHitchhikers guide to Internet‚Äô which is available at this link.\nThe book content is in raw form which we need to preprocess and transform into a format which can be utilize for learning word embeddings. For that, we will first read the book content and breaks it into a list of sentences. To do that we will use punkt sentence tokenizer from NLTK library.\n\n\n\n\n\n\nTip\n\n\n\nThe dataset chosen is this example is very small from the NLP perspectives where text corpora usually contains millions words. This is just for the sake of learning and keep things simple.\n\n\n\n# reading book text\nbook = open('pg39.txt')\nbook_content = book.read()\n\n\nimport nltk.data\nimport string\nimport re\n# breaking the text into sentences\nsent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nsentences = sent_tokenizer.tokenize(book_content.strip())\n\nprint('Number of sentences:',len(sentences))\n\nNumber of sentences: 519\n\n\n\n# converting sentences into lower case\nl_sentences =[sent.lower() for sent in sentences]\n\nprocessed = []\n# we will add spaces if there are punctuation in the text (for splitting purposes)\nfor sent in l_sentences:\n    text = re.sub(r\"([?!,])\",r\" \\1 \",sent)\n    text = re.sub(r\"[^a-zA-Z,?!]\",r\" \",sent)\n    processed.append(text)\n\nfinal = []\n\nfor sentence in processed:\n    # tokenize the sentences\n    tokens = nltk.tokenize.word_tokenize(sentence)\n    \n    # removing punctuation\n    processed_tokens = [token for token in tokens if token not in set(string.punctuation)]\n\n    final.append(processed_tokens)\n    \nprint('Samples:',final[10])\n\nSamples: ['a', 'few', 'bsd', 'systems', 'on', 'an', 'ethernet', 'not', 'connected', 'to', 'anywhere', 'else']\n\n\nSo now we have our text pre-processed. We will now move to preparing a dataset file using our processed dataset.\n\ndef get_cbow_instances(text,window_size=2):\n    \"\"\"\n    Argument:\n    ---------\n        text: str\n            tokenized form of a sentence\n    \n    Return:\n        list\n            list of tuples with x,y\n    \"\"\"\n    max_elements_in_window = 2 * window_size + 1\n    \n    tuples = []\n    current_index = 0\n\n    instances = []\n    \n    while True:\n        if current_index == len(text):\n            break\n        \n        current_window_min = current_index\n        current_window_max = current_index\n        \n        for min_option in range(window_size + 1):\n            if (current_index - min_option) &gt;= 0:\n                current_window_min = current_index - min_option\n                \n        for max_option in range(window_size + 1):\n            if (current_index + max_option) &lt; len(text):\n                current_window_max = current_index + max_option\n        \n        current_text = text[current_window_min:current_window_max+1]\n        \n        current_relative_index = current_index - current_window_min\n        \n        current_context = []\n        for ind,t in enumerate(current_text):\n            if ind == current_relative_index:\n                continue\n            current_context.append(t)\n        \n        instances.append((text[current_index],\" \".join(current_context)))\n        current_index += 1\n    return instances\n                \n\n\nd = ['the', 'project', 'gutenberg', \n     'ebook', 'of', 'hitchhiker', \n     \"'s\", 'guide', 'to', 'the', 'internet', \n     'this', 'ebook', 'is', 'for', 'the', 'use', \n     'of', 'anyone', 'anywhere', 'in', 'the', 'united', \n     'states', 'and', 'most', 'other', 'parts', 'of', 'the', \n     'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', \n     'restrictions', 'whatsoever']\n\n\ncbow_instances = get_cbow_instances(d,window_size=2)\nprint(cbow_instances[:5])\n\n[('the', 'project gutenberg'), ('project', 'the gutenberg ebook'), ('gutenberg', 'the project ebook of'), ('ebook', 'project gutenberg of hitchhiker'), ('of', \"gutenberg ebook hitchhiker 's\")]\n\n\nNow we will process the all the sentences and prepare a DataFrame to build our model.\n\nimport pandas as pd\nx = []\ny = []\nfor sent in final:\n    instances = get_cbow_instances(sent)\n    \n    for pair in instances:\n        x.append(pair[1])\n        y.append(pair[0])\n\ndf = pd.DataFrame({'x':x,'y':y})\n\ndf.to_csv('book.csv',index=False)\n\n\n\nPreparing dataset for PyTorch\nWe will now vectorize our input data to use for learning embeddings using PyTorch. In a previous post, we delved deeper into the vectorization process using vocabulary and vectorizer. You can check that post.\nWe will briefly explain that process here.\n\nFirst, we will build a vocabulary which mainly offers mapping between all the unique words in the corpus and their indices.\nSecond, we transform our text input in a vector of the size of vocabulary.\nThird, we will use PyTorch Dataset and DataLoader classes to finalized our dataset for training\n\nFor the first step, we will reuse our Vocabulary class from a previous post. The source code is given below. For more details on the code please refer to the post.\n\nclass CBOWVocabulary(object):\n    def __init__(self, token_to_idx = None, add_unk=True, unk_token='&lt;UNK&gt;'):\n        \"\"\"\n        params:\n            token_to_idx (dict): mapping from token to index\n            add_unk (bool): flag to add a special token to the vocabulary for unknowns tokens\n            unk_token (str): Token used as special token\n        \n        \"\"\"\n        if token_to_idx is None:\n            token_to_idx ={}\n        self._token_to_idx = token_to_idx\n        self._idx_to_token = {idx:token for token,idx in token_to_idx}\n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token)\n        \n    def add_token(self,token):\n        \"\"\"\n        Add token to the vocabulary\n        \n        params:\n            token (str): token to add to the vocabulary\n            \n        returns:\n            idx (int): index of token\n        \n        \"\"\"\n        if token in self._token_to_idx:\n            return self._token_to_idx[token]\n        else:\n            idx = len(self)\n            self._token_to_idx[token] = idx\n            self._idx_to_token[idx] = token\n        return idx\n    \n    def lookup_idx(self,idx):\n        \"\"\"\n        Lookup vocabulary to fetch  token at idx\n        \n        params:\n            idx(int) : index of token to be fetched\n            \n        returns:\n            token (str): token stored at idx\n        \"\"\"\n        if idx not in self._idx_to_token:\n            raise KeyError(\"Vocabulary does not have token with specified index:\"%idx)\n        return self._idx_to_token[idx]\n    \n    def lookup_token(self,token):\n        \"\"\"\n        Lookup vocabulary to fetch index of a token\n        \n        params:\n            token(str): token to lookup\n            \n        returns:\n            idx (int): index of token\n        \"\"\"\n        \n        if token not in self._token_to_idx:\n            return self.unk_index\n        else:\n            return self._token_to_idx[token]\n    \n    def __len__(self):\n        return len(self._idx_to_token)\n    \n    def __str__(self):\n        return \"Vocabulary (size = %d)\" % len(self)\n\nWe will now populate our vocabulary with the text data from the book.\nThe following code add all tokens to the vocabulary. One thing to note here is that we are adding every token but in practice you may find discarding some of them with very low frequency count.\n\n# Build CBOWVocabulary\nvocab = CBOWVocabulary()\n\n# Populate vocabulary\nfor sent in final:\n    for tok in sent:\n        vocab.add_token(tok)\n        \n# printing size\nprint(len(vocab))\n\n1898\n\n\nNow, we will prepare our Vectorizer. The vectorizer will transform our input text data which into a vector containing indices of tokens in the Vocabulary.\nFor example, in our vocabulary, the indices of words project and gutenberg are 0, 1 respectively. That means when we apply the vectorizer, it will return a vector containing 0 and 1.\nWe know that the window size chosen was 2. That means the maximum number of items in the vector representing context can be 4 (2 in the left side and 2 in the right side). We also know that context can also be made of two words, e.g., context project gutenberg for the word the. To keep the size of the vector consitent, we add padding to our vector for cases when the context data has less than four words.\nThe following code achieves that functionality. It returns a vector of size 4 for specified context data.\n\nimport numpy as np\ndef vectorizer(context,window_size=2):\n    \"\"\"\n    This function transforms the context text into a vector of integer representing indices of words in the text.\n    \n    Argument:\n    ---------\n    context: str\n        a string containing context words\n    \n    window_size: int\n        window size to determine the size of the returned vector and add padding\n        \n    Returns:\n        np.array\n            an array of indices of words in context\n    \n    \"\"\"\n    # in the context there can be max of double of window size, e.g., 2 words in left size, 2 in right side\n    max_context_size = 2 * window_size\n    \n    vector = np.zeros(max_context_size)\n    \n    for ind, word in enumerate(context.split(\" \")):\n        vector[ind] = vocab.lookup_token(word)\n        \n    return vector\n\nvectorizer('project gutenberg')\n\narray([2., 3., 0., 0.])\n\n\nLet‚Äôs now move to the final step of preparing the dataset for training our classifier. We will now use PyTorch‚Äôs Dataset and DataLoader classes to simplify the process of generating batches for training in a format needed.\nThe Dataset and DataLoader classes take care of transforming arrays into tensors, and preparation of batches.\nWe will create a new class CBOWDataset which basically implements two functions __getitem__ and __len__. The first function returns the vectorized context data and the index of the word to predict. The second function returns the number of total instances in the dataset.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CBOWDataset(Dataset):\n    def __init__(self,df):\n        \"\"\"\n        Argument:\n        --------\n            df: Pandas DataFrame\n                dataframe containing context and word to predict\n        \"\"\"\n        \n        self.df = df\n    \n    def __getitem__(self,index):\n        \"\"\"\n        Returns vectorized context data and the index of the word to predict\n        \"\"\"\n        record = self.df.iloc[index,:].to_dict()\n        \n        return {'input_x':vectorizer(record['x']),'y':vocab.lookup_token(record['y'])}\n    \n    def __len__(self):\n        \"\"\"\n        Returns the size of dataset\n        \"\"\"\n        \n        return self.df.shape[0]\n\n\n\nBuilding model using PyTorch\nWe will now build our model architecture using PyTorch. PyTorch offers an Embedding layer which makes it easy to handle word embeddings. The layer used to store and retrieve word embeddings using indices. That‚Äôs the reason why in our vectorized form we only have indices of tokens.\nWe will use an Embedding layer in the start of our model which will then fetch the corresponding word embeddings and pass them to the next layer.\nTo create an Embedding layer, we need to specify num_embeddings that is the size of vocabulary (e.g., total number of tokens in the vocabulary), embeddings_dim that is the number of dimensions (i.e., how many dimensions to use to represent a word embedding, e.g., 100, 200).\nDuring our vectorization process, we added padding (i.e., 0) to the vector when the size was less than four. To tell the model that this 0 should not have any effect during training (or in other words model must not update these values during training), we can specify padding_idx as 0.\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nnum_tokens = len(vocab)\nembedding_dim = 50\n\nclass CBOWClassifier(nn.Module):\n    \"\"\"\n    Classifier for learning word embeddings\n    \"\"\"\n    def __init__(self,vocab_size,embedding_dim):\n        \"\"\"\n        Arguments:\n            vocab_size: int\n                Size of vocabulary\n            \n            embedding_size: int\n                Embedding dimensions\n        \"\"\"\n        super(CBOWClassifier,self).__init__()\n        \n        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n                                      embedding_dim=embedding_dim,\n                                      padding_idx=0)\n        \n        self.fc1 = nn.Linear(in_features=embedding_dim,out_features=vocab_size)\n        \n    def forward(self,input_x,apply_softmax=False):\n        \"\"\"\n        Performs the forward pass\n            \n        Arguments:\n            input_x: torch.tensor\n                input tensor of shape (batch_size, input_dim)\n                    \n            apply_softmax: bool\n                flag to perform softmax\n        \"\"\"\n        intermediate = self.embedding(input_x).sum(dim=1)\n            \n        output = self.fc1(intermediate)\n            \n        if apply_softmax:\n            output = f.soft_max(output, dim=1)\n        return output\n\n\n\nTraining CBOW Classifier\nThe following training procedure utilizes the entire dataset. This is just for the sake of learning and keep the post simple to understand.\nIn practice, the dataset is often divided into three parts: train, validation, test. It is recommeded to the follow the same when working on a real-world ML project.\n\nimport torch.optim as optim\n\nnum_epochs = 30\nclassifier = CBOWClassifier(num_tokens,100)\n\ndataset = CBOWDataset(df)\nloader = DataLoader(dataset,batch_size=50)\nadam = optim.Adam(classifier.parameters(),lr=.001)\nloss_fnc = nn.CrossEntropyLoss()\n\nclassifier.train()\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    \n    for batch_index, batch in enumerate(loader):\n    \n        # setting gradients from previous epochs to zero\n        adam.zero_grad()\n    \n        # forward pass\n        output = classifier(batch['input_x'].int())\n    \n        # computing loss\n        loss = loss_fnc(output,batch['y'])\n    \n        # backward pass\n        loss.backward()\n        \n        #update parameters\n        adam.step()\n        \n        running_loss += (loss.item() - running_loss)/(batch_index+1)\n    \n    print('Epoch:',epoch,' Loss:',running_loss)\n\nEpoch: 0  Loss: 7.323485131575679\nEpoch: 1  Loss: 5.867017787193583\nEpoch: 2  Loss: 5.039742054226242\nEpoch: 3  Loss: 4.407589763124413\nEpoch: 4  Loss: 3.8932629177503517\nEpoch: 5  Loss: 3.471296124369183\nEpoch: 6  Loss: 3.1283544541519386\nEpoch: 7  Loss: 2.8483516849647055\nEpoch: 8  Loss: 2.614800505548995\nEpoch: 9  Loss: 2.415219902992248\nEpoch: 10  Loss: 2.241242004332143\nEpoch: 11  Loss: 2.087147696970779\nEpoch: 12  Loss: 1.9489659980357257\nEpoch: 13  Loss: 1.8238577800933438\nEpoch: 14  Loss: 1.7096240920162649\nEpoch: 15  Loss: 1.6044389607611103\nEpoch: 16  Loss: 1.5074336672915487\nEpoch: 17  Loss: 1.417175150363245\nEpoch: 18  Loss: 1.3331712223589423\nEpoch: 19  Loss: 1.2546023246701625\nEpoch: 20  Loss: 1.1812430473967133\nEpoch: 21  Loss: 1.1125499971002064\nEpoch: 22  Loss: 1.0486516395461891\nEpoch: 23  Loss: 0.9879383787721678\nEpoch: 24  Loss: 0.9312528835731407\nEpoch: 25  Loss: 0.8782325799481193\nEpoch: 26  Loss: 0.8286848590909462\nEpoch: 27  Loss: 0.7823101626709101\nEpoch: 28  Loss: 0.7389837774474207\nEpoch: 29  Loss: 0.6985426304435415\n\n\n\n\nUsing trained embeddings\nNow, we will use our trained embeddings to find words which are close to a specified word. To do this task we will follow the following steps\n\nGet the weights of embedding layer of the classifier (which are the word embeddings)\nObtain the index of the word for which we want to search close words\nObtain the word embeddings of the word using its index\nIterate over all the words in the vocabulary, obtaining their indices, fetching their word embeddings and then computing the distance\nGet n words with highest distance measures\n\n\ndef get_close_words(word_to_search, word_to_index,embeddings,n=10):\n    \"\"\"\n    Get n closes words to the specified word\n    Arguments:\n    ----------\n    word_to_search: str\n        word which we want to search\n        \n    word_to_index: dictionary\n        mapping from word to index in vocabulary\n        \n    n: int\n        number of words to return\n        \n    Returns:\n    -------\n        list: a list of n words which are closest to the specified word.\n    \"\"\"\n\n\n\n    word_embedding = embeddings[word_to_index[word_to_search]]\n\n    distances = []\n\n    for word, index in word_to_index.items():\n        if word == '&lt;UNK&gt;' or word == word_to_search:\n            continue\n    \n        distances.append((word, torch.dist(word_embedding, embeddings[index]).item()))\n    \n    sort_distances = sorted(distances, key=lambda x: x[1])\n\n    return sort_distances[1:n+2]\n\n\nembeddings = classifier.embedding.weight.data\n\nclose_words = get_close_words('network',vocab._token_to_idx,embeddings,n=5)\n\nfor c in close_words:\n    print('[{:.2f}] -- {}'.format(c[1],c[0]))\n\n[12.84] -- directional\n[13.04] -- tables\n[13.06] -- sections\n[13.15] -- city\n[13.15] -- users\n[13.20] -- backs\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/snakemake_testing.html",
    "href": "posts/post-with-code/snakemake_testing.html",
    "title": "Generating and Performing Automated Test for Snakemake Workflow",
    "section": "",
    "text": "Snakemake is a workflow manager which simplifies the execution of complex pipelines, but as a workflow grows, testing becomes critical. Automated tests help catch errors early and ensure reproducibility. In this post, I will show how to generate and run unit tests for Snakemake workflows."
  },
  {
    "objectID": "posts/post-with-code/snakemake_testing.html#generated-tests",
    "href": "posts/post-with-code/snakemake_testing.html#generated-tests",
    "title": "Generating and Performing Automated Test for Snakemake Workflow",
    "section": "Generated tests",
    "text": "Generated tests\nThe above command generates a new directory .tests in your workflow directory. This directory contains the following structure.\n.tests/\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ common.py\n‚îú‚îÄ‚îÄ rule1/\n‚îÇ   ‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ expected/\n‚îú‚îÄ‚îÄ rule2/\n‚îÇ   ‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ expected/\n‚îú‚îÄ‚îÄ test_rule1.py\n‚îî‚îÄ‚îÄ test_rule2.py\n\nThe below table provides a short explanation for each of those file/sub-directory\n\n\n\n\n\n\n\n\nPath\nType\nPurpose\n\n\n\n\n.tests/\nDirectory\nRoot folder for all unit test artifacts\n\n\n__init__.py\nFile\nMakes Python treat .tests as an importable package\n\n\ncommon.py\nFile\nContains shared pytest fixtures (shared code) and configurations\n\n\nrule*/\nDirectory\nPer-rule test container (e.g., rule1/, rule2/)\n\n\nrule*/data/\nDirectory\nStores input files for testing the rule\n\n\nrule*/expected/\nDirectory\nStores expected output files for validating rule execution\n\n\ntest_rule*.py\nFile\nContains pytest test cases for specific rules"
  },
  {
    "objectID": "posts/post-with-code/snakemake_testing.html#error",
    "href": "posts/post-with-code/snakemake_testing.html#error",
    "title": "Generating and Performing Automated Test for Snakemake Workflow",
    "section": "Error",
    "text": "Error\nHowever, this execution likely to fail (as it happened with me and others also encountered a similar problem) due to snakemake‚Äôs inability to locate the config file. The snapshot below shows the failed execution.\n\nTo understand the issue, we will direct our focus on investigating errors occurring while executing a specific test. I am opting here the first rule in the workflow, i.e., running fastqc to generate quality reports.\npytest .tests/unit/test_fastqc_before.py\n\n\n\n\n\n\nconfig.yaml not found\n\n\n\nOn inspection of the error messages generated by executing the above command, I discovered this error ‚ÄúWorkflow defines configfile config/config.yaml but it is not present or accessible‚Äù\n\n\nThis error is due to the use of a temporary directory which is treated as a working directory for snakemake execution for each rule. On execution of the workflow, Snakemake attempts to find the config file and fails. We will address this error by making our config file available in the temporarily created directory (follow the instructions given in the next section)."
  },
  {
    "objectID": "posts/post-with-code/snakemake_testing.html#solution",
    "href": "posts/post-with-code/snakemake_testing.html#solution",
    "title": "Generating and Performing Automated Test for Snakemake Workflow",
    "section": "Solution",
    "text": "Solution\nTo address the error, I made some changes with the help of the Internet. These changes are the following.\n\nI copied the subdirectory containing the config file (or its symlink) in unit directory in the newly created .tests directory and then added the code given below in each test file. This ensures the availability of a config file to the execution environment. This solution is available at this link.\n\nconfig_path = PurePosixPath(\".tests/unit/config\")\n\n# Copy data to the temporary workdir.\nshutil.copytree(data_path, workdir) # this is where raw_data will be available and results will be stored\nshutil.copytree(config_path, workdir / \"config\")\n\nI also updated common.py file to add customized code to test the generated files. By default, each generated file is compared with its expected file in a byte-by-byte manner. This may not suit your need if each execution generates a new file. For example, my fastqc test was failing because the rule was generating a new file (e.g., with an updated timestamp) on each execution, causing the comparison to fail.\n\nI hope you find this post helpful."
  },
  {
    "objectID": "posts/post-with-code/facial-features.html",
    "href": "posts/post-with-code/facial-features.html",
    "title": "Facial feature extraction using OpenFace",
    "section": "",
    "text": "In this post, I will discuss the work I have been doing recently. I needed to extract facial features from the recorded video and for this task, I decided to use OpenFace, an open-source face recognition library. In this post, I am sharing the installation process and tutorial on detecting facial landmarks.\n\n\nInstallation\nI tried to install OpenFace on Mac OS but couldn‚Äôt succeed. There were a lot of errors and compatibility issues that I couldn‚Äôt get through. Therefore, I decided to install it on Ubuntu. For that, I installed a Virtual box on Mac and installed Ubuntu 18.04.\nTo install OpenFace, I followed the steps given here\nsudo apt-get update\nsudo apt-get install build-essential\nsudo apt-get install g++-8\n\nsudo apt-get install cmake\n\nsudo apt-get install git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\n\nsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-22-dev\n\nwget https://github.com/opencv/opencv/archive/4.1.0.zip\n\nsudo unzip 4.1.0.zip\ncd opencv-4.1.0\nmkdir build\ncd build\n\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_TIFF=ON -D WITH_TBB=ON ..\nmake -j2\nsudo make install\n\nwget http://dlib.net/files/dlib-19.13.tar.bz2\ntar xf dlib-19.13.tar.bz2\ncd dlib-19.13\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release\nsudo make install\nsudo ldconfig\ncd ../..\n\nsudo apt-get install libboost-all-dev\n\ngit clone https://github.com/TadasBaltrusaitis/OpenFace.git\n\ncd OpenFace\nmkdir build\ncd build\n\ncmake -D CMAKE_CXX_COMPILER=g++-8 -D CMAKE_C_COMPILER=gcc-8 -D CMAKE_BUILD_TYPE=RELEASE ..\nmake\nAt this point, we have installed OpenFace. Now we need to download models. You can either download it manually or use a script provided in the OpenFace library.\nManual download links.\n\nscale 0.25\nscale 0.35\nscale 0.50\nscale 1.00\n\nI ran the following command to ran the script to download the model.\ncd ..\nsh ./download_models.sh\nThe script will download models in the directory OpenFace/lib/local/LandmarkDetector/model/patch_experts.\nAfter completing this process, I ran the demo program by running the following command.\n./bin/FaceLandmarkVid -f \"\"../samples/changeLighting.wmv\"\" -f \"\"../samples/2015-10-15-15-14.avi\n\n\n\n\n\n\nExecution error\n\n\n\nI got an error CEN patch expert not found. The command was searching the models in the OpenFace/build/bin/model/patch_experts.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nI copied the files (cen_patches_0.25_of.dat,cen_patches_0.35_of.dat,cen_patches_0.50_of.dat,cen_patches_1.00_of.dat) in OpenFace/build/bin/model/patch_experts directory.\n\n\n\n\nRunning Demo\nIf you have a video with a single face, you can use FaceLandmarkVid or in case of multiple faces, you can use FaceLandmarkVidMulti\n\n\n\n\n\n\nNote\n\n\n\nThese files will be available in OpenFace/build/bin directory. Either you can specify the full path to facial landmark detector or cd to the bin directory and run the following command.\n\n\nFaceLandmarkVid -f file_name\nFollowing is the demonstration of OpenFace on a video clip with single face.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/pandas.html",
    "href": "posts/post-with-code/pandas.html",
    "title": "Introduction to Python‚Äôs Pandas API",
    "section": "",
    "text": "Pandas is a Python API for processing data in a easy and efficient way. This post offers an introduction to this amazing API , especially for beginners.\nThe post starts with installation instructions of the API and then introduces its functionality with the help of examples."
  },
  {
    "objectID": "posts/post-with-code/pandas.html#installation",
    "href": "posts/post-with-code/pandas.html#installation",
    "title": "Introduction to Python‚Äôs Pandas API",
    "section": "Installation",
    "text": "Installation\n\n\nThere are two options to install the Pandas API on your system. The first option is to install it through Anaconda Distribution which comes with all essential Python packages. While the second option is to install Python on the system and then install Pandas package using the following command. \n\npip install pandas"
  },
  {
    "objectID": "posts/post-with-code/pandas.html#introduction",
    "href": "posts/post-with-code/pandas.html#introduction",
    "title": "Introduction to Python‚Äôs Pandas API",
    "section": "Introduction",
    "text": "Introduction\nNow we will explore the API basics. Basically, we will cover the following topics which I think will be good enough to start using Pandas API.\n\n\n\n\n\nPandas DataFrame and Series objects\n\n\nReading data CSV files and accessing basic information\n\n\nQuerying data using loc() and iloc() function\n\n\nHandling missing data\n\n\nAdding, deleting columns or rows\n\n\n\n\n\nPandas DataFrame and Series objects\nPandas‚Äô Series is a one-dimensional array representation of values. we can understand it as an attribute in a dataset. For instance, consider a dataset with three attributes sid (student-id), name, and marks. Now, each of these attributes in pandas is represented as a Series object.\n\n\nLet‚Äôs write a program to create these three Series objects sid, name, and marks.\n\nimport pandas as pd\n\n# Creating Series using list\nid = pd.Series([101,102,103,104,105])\n\nname = pd.Series(['pradeep','manoj','kiran','pushpendra','sambit']\n\nmarks = pd.Series([20,30,40,32,28])&lt;/code&gt;&lt;/pre&gt;\nThe first line import pandas as pd imports the pandas package in wer program. Next, three lines create three Series objects with a given list.\n\nPandas‚Äô DataFrame object is a two-dimensional data structure where each attribute is a Series object. we can create a DataFrame using a dictionary of key:value pair where the key represents attribute name and the value represents the values of that attribute.\nLet‚Äôs create a dataframe using the Series objects we created in the above program.\n\ndf = pd.DataFrame({'sid':id,'name':name,'marks':marks})\n\n\nReading data CSV files and accessing basic information\nPandas have a function read__csv() to read CSV format data files. This function comes with multiple useful options which we will learn in this section. The data file used in this tutorial can be downloaded from the link. The name of the downloaded data file is iris_csv.csv. \n\nOpen and read a CSV data file\nimport pandas as pd\ndf=pd.read_csv('iris_csv.csv')\ndf.head()\n\ndf=pd.read_csv(‚Äòiris_csv.csv‚Äô) opens and reads the specified CSV file (here we can specify the name of wer data file). The third line df.head() shows first five records (we can specify the number of records) from wer data file.\n\n\n\nAssign/Rename column names\n\nIn case, if wer data file does not have column names or we want to assign a different column name then we can use the names option of read_csv() function. Example:\n\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv',names=['sep-len','sep-wid','pet-len','pet-wid','class'])\n\n\nReading data file with different seperator\n\nSometimes the data files have columns seperated by other characters (e.g.¬†spaces, colon). In such cases, in order to read the CSV file we need to specify the sep option in the read_csv() function.\n\n# reading file having data seperated by :\ndf = pd.read_csv('data_file',sep=':')\n\n\nSkipping rows while reading data\n\nIn case, if wer data file does not have data records from the first line (let‚Äôs say it contains some summary or description and data records begins from line 4), we can skip those lines by specifying skip rows option.\n\ndf = pd.read_csv('data_file',skiprows=3)\n\n\nAccessing sizes of data\nwe can check the size of wer data set (e.g.¬†number of rows, number of columns) using shape property of the DataFrame.\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv')\nprint(df.shape)\n# output\n# (150, 5)\n\nHere, 150 is the number of rows and 5 is number of columns.\n\n\n\nChecking data types of columns\n\nTo check the data types of columns in the data file, we can use &lt;a href=‚Äú‚Äúhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html‚Äù‚Äú&gt;dtypes property.\n\n\n\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv')\ndf.dtypes&lt;/code&gt;&lt;/pre&gt;\n\nOutput:\n\nsep-len    float64\nsep-wid    float64\npet-len    float64\npet-wid    float64\nclass       object\ndtype: object\n\n\n\n\n\n\n\nTip\n\n\n\nAs the data processing modules requires wer data to be in numeric data types (e.g.¬†int, float) it is best practice to check the data types before processing it.\n\n\n\n\nBasic stats of data\nIf we want to learn about our data in more depth, we can use describe() function. This function provides information about count, minimum, maximum, mean, standard deviation, quartiles for each column. An example is given below.\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv',skiprows=1,names=['sep-len','sep-wid','pet-len','pet-wid','class'])\ndf.describe()&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\nQuerying data using loc() and iloc() function\nPandas offers two different functions (there is one more ix() which is actually deprecated) for accessing data from the dataframe- .loc() and .iloc(). In these functions, we specify the labels or positions of rows and columns to access data. However, if we do not specify columns selector then by default all columns are accessed.\n\n\n\n\n\n\n\nTip\n\n\n\n: operator used for slicing purpose. It works differently in case of label and position. When applied with labels (start:end), it include end element in the result. However, in case of positions (start:end), it does not include end in the result.\n\n\n\nLet‚Äôs understand the difference between labels and positions. In the following code, we are creating a simple dataframe with two columns sid and name.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\ndf.head()\nAs shown in the figure below, the row labels are (0,1,2,3,4) and column labels are (sid, name). The position for rows and columns begins with 0 that means the first row has position 0, second row has position 1 and so on. \nIn the above example, the rows position and labels are same. To make the difference clear, let‚Äôs try to change the index of our dataframe and then see it. we can change the index of dataframe using set_index() function. In the following example, we are setting the first column sid as the index of our dataset. This function create a copy of dataframe, apply changes it it and then return the updated copy. In order to make changes to dataframe inplace=True parameter needs to be passed.\ndf.set_index('sid',inplace=True)\ndf.head()\n\nAs we can see in the figure below, rows‚Äô indices are (101,102,103,104,105) whereas rows‚Äô positions are the same as previous.\n\n\n\nSome Examples\nThe following figure shows some examples for accessing data with label and position-based selections.\n\n\nTo access a particular row, we need to specify its label or position in the row-selector (for example, we have to specify label 0 to access first row). In case, if we want to access multiple rows, we need to specify their corresponding labels or positions in a list or we can use : operator for slicing (for example, row selector for accessing first three rows can be [0,1,2] or 0:3).\n\n# import pandas package\nimport pandas as pd\n\n# create the dataframe\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# set sid as index\ndf.set_index('sid',inplace=True)\n\n# Access first row\ndf.loc[101]  # lable-based\n\ndf.iloc[0]  #  Position-based\n\n\n# Access first three rows\ndf.loc[101:103]  # label-based\n\ndf.iloc[0:3]  # position-based\n\n\nCondition-based data access\n\n\n\nFunction loc() and iloc() both support condition-based data access using boolean array. Let‚Äôs say we want to access the first row. To do that we need to specify a boolean array for rows selection. This array will contain a boolean value for each row and only one True value.\nIf we want to show a particular set-of rows, we can do that by specifying a boolean array with True values on corresponding location of those rows. Same applies for column selection.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\ndf.loc[[True,False,False,False,False],[True,True]]\n\n#Output\n#      sid     name\n#0     101     pradeep\n\n\nIn the above example, we spcified a boolean array for rows selection and another for columns selection. In the first array, True is specified at the first index (which corresponds to the first row). The second array contains all True(which corresponds to all columns). Hence, we get the values from the first row and both columns.\n\n\n\n\nHandling missing data\nPandas offers a great support to handle missing values. If the dataset has some values missing, Pandas automatically marks them as NaN values. To demonstrate it, I have prepared a CSV file with three columns- eid, name, salary.\n\nIn this file, I intentionally kept the salary field for the third record empty for the following exercises on the missing values.\nNow let‚Äôs read this file using pandas.\ndf=pd.read_csv('emp.csv')\ndf.head()\n\nOutput:\n\n\n\nWe can handle missing values in two ways: delete or replace. The following sections discusses these both ways.\n\n\nDeleting missing values\n\n\nWe can use the dropna() function to delete missing records.\nIn this function, we need to specify axis=0 if we want to delete the row/rows having NaN and for deleting column/columns having NaN specify axis=1. \n\ndf=pd.read_csv('emp.csv')\n\n#delete row\ndf1 = df.dropna(axis=0)\ndf1.head()\n\n#delete column\ndf1 = df.dropna(axis=1)\ndf1.head()&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\n\n\n\nTip\n\n\n\nTo change the original dataframe, specify inplace=True in the dropna() function.\n\n\n\n\n\nFilling missing values\n\n\n\nFunction fillna is useful in filling missing values in the dataframe.\n\ndf=pd.read_csv('emp.csv')\n\n#fill with 0\ndf1 = df.fillna(0)\ndf1.head()\n\n#fill using forward fill method\ndf2 = df.fillna(method='ffill')\ndf2.head()\n\n# fill using backward fill method\ndf3 = df.fillna(method='bfill')\ndf3.head()\n\n# fill using mean value of column\ndf4 = df.fillna(df['salary'].mean())\ndf4.head()\n\n\n\n\n\n\n\nTip\n\n\n\nffill replaces NaN with the previous value in the same column. While, bfill replaces NaN with the next value in the same column (order of values top to bottom).\n\n\n\n\n\nAdd or delete row/column\n\nThis section will show we how to add a new row or column to an already existing dataframe.\n\n\nAdding row/column\nWe can simply add a row using append() or loc()/iloc() function. We can use key:value pair in the append function, where the key is the attribute name and the value represents the value we want to add. Pandas automatically puts NaN if some attributes values are not provided.\nNow, let‚Äôs add a record with sid as 106 and name as ‚Äògaurav‚Äô.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# using the append function\ndf = df.append(append({'sid':106,'name':'gaurav'},ignore_index=True)\nprint(df)\n\n# Adding a column\ndf['marks'] = [20,30,40,32,28,50]\nThe same record can also be added using df.loc[5]=[106,‚Äògaurav‚Äô].\n\n\nDeleting row/column\n\n\n\nTo delete some columns or rows, the drop function can be used. In this function, we need to specify the label of row or column we want to delete. In case, it is a column then we also need to pass a parameter axis=1.\nThe following example illustrates the use of drop function.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# delete sid column\ndf.drop('sid',axis=1)"
  },
  {
    "objectID": "posts/post-with-code/statistical_test.html",
    "href": "posts/post-with-code/statistical_test.html",
    "title": "Statistical tests in R",
    "section": "",
    "text": "This post delves into inferential statistics and explains two different statistical test. Additionally, this post also talks about the need of performing these tests along with their suitability based on available data.\nIn particular, this post tackles the following three questions.\n\nWhy we do the statistical test?\nHow to do it?\nWhen to do which test?\n\n\nWhy we do the statistical test?\nAs we have already discussed that we often do not have access to the entire population. Instead, we have access to a small portion of the population known as a sample. We use this sample to infer knowledge about the population.\nFollowing are a few examples to explain it further.\n\nWe collected a sample of height data from students studying in university X. We computed the average of our sample and obtained the average height of students in our sample. Now we want to test whether the average height found in our sample is the same for the population.\nWe collected two samples from two universities‚Äô students‚Äô salaries. We want to see is there any difference between the average salary between two universities or not.\nWe collected two samples of test scores from the same classroom at the start of the semester and end of semester. We want to see students whether the students‚Äô test scores‚Äô were improved significantly or not.\nWe collected a salary sample of people working in Tallinn with their education levels (e.g., primary, secondary, bachelors, master, doctorate). We want to test is there any significant difference in salary among these various levels of education levels.\n\nAll these aforementioned examples illustrate some of the cases where we can apply the statistical test to test our assumption or guess (aka. hypothesis). One thing to note here is we are not just interested in finding knowledge for our sample, we want to rather use it to gain knowledge about the population.\n\nThe above diagram gives a pictorial representation of what we have just talked about. There are two classes- A and B. Let‚Äôs say there are 100 and 80 students in class A and B, respectively. We collected a sample of student‚Äôs test scores data from both classes. In our sample, we have test scores for only a few students (let‚Äôs say 35-40). We computed the average for both the samples and found that the sample-1 mean is higher than sample-2 mean. This is what we learned from our sample. Now, we want to see is it also the same for the population (classroom-A average is higher than classroom-B average test scores or not). Here comes the statistical test. We use them to infer knowledge about the population based on collected sample data.\n\n\nHow to do it?\nWe will cover one sample t-test, independent sample t-test, paired t-test, and ANOVA in R.\n\nIndependent sample t-test\nWe do this test when we have two independent samples and we want to compare a statistic for these two groups. For example, we want to compare the average salary of students working part-time from two different universities. In this case, we have two samples collected from different universities. These samples are independent because they contain data of different people.\nMake sure the following assumption in your dataset when you do a t-test\n\nContinuous dependent variable.\nCategorical independent variable for grouping.\nIndependent data samples\nDependent variable has a normal distribution.\nEach group has the same variance.\n\nWe will look at the above assumptions with the help of an example. Let‚Äôs say we have test scores from two classes: A and B. Now we want to test whether the difference between these two classes‚Äô test scores is significant or not.\n\n\n\nClass\nTestScore\n\n\n\n\nA\n99\n\n\nA\n93\n\n\nA\n81\n\n\nA\n92\n\n\nA\n89\n\n\nA\n80\n\n\nA\n88\n\n\nA\n81\n\n\nA\n100\n\n\nA\n82\n\n\nB\n98\n\n\nB\n66\n\n\nB\n86\n\n\nB\n62\n\n\nB\n78\n\n\nB\n87\n\n\nB\n77\n\n\nB\n60\n\n\nB\n60\n\n\nB\n79\n\n\n\nYou can see the dataset has one continuous variable (TestScore: Numeric type) and one class variable as the grouping variable. This class variable tells the classroom from which the scores are collected.\nAssumptions\n\nContinuous dependent variable: the dataset has continuous dependent variable (testScore).\nCategorical independent variable: the dataset has a class variable which is a categorical variable(a variable that has categories of value).\nThe independence assumption can only ensure while data collection. We assume here that the data were collected independently.\nNormal distribution: We can plot the test scores for each class and check for bell shape.\nlibrary(lattice)\nlibrary(rio)\n\n# load dataset\ndata &lt;- import('data-file-name')\n\n# plot the distribution\ndensityplot(~ data$TestScore|data$Class)\n\nSame variance in both groups (class A and B): We will compute the variance in both groups. We can use the dplyr package for computing variance class-wise.\ndata %&gt;% group_by(Class) %&gt;% summarise(var(TestScore))\nThe variance for Class A and B are following\n\n\n\nClass\nVariance\n\n\n\n\nA\n55.8\n\n\nB\n169\n\n\n\nAs we can see the variance is not the same and the difference is not small.\n\nIf the variance is the same, we will use Student‚Äôs T-test.\nIf the variance is not the same then we will use Welsh T-test.\n\nIn our dataset, the variance is not the same therefore we will apply Welsh t-test.\n\n\nPerforming the test\nWe will first set up our hypothesis. We want to test that is there any difference between the test scores of class A and B. We will follow the below steps\n\nState Null and alternative hypothesis\nDecide significance level \\(\\alpha\\)\nPerform test\nCheck the p-value and decide ‚Äòwhether to reject the null hypothesis or not‚Äô\n\nNow, let‚Äôs talk about each step in detail.\nThe first step is to formulate the null and alternative hypotheses. What are those? We specify what we want to test. For example, in our class‚Äôs test scores dataset, we want to test that the average test scores from two groups are different. It will become our alternative hypothesis. The null hypothesis is a hypothesis of no difference. In other words, the opposite of what we want to test. The below diagram is showing our null and alternative hypotheses.\n\n\n\nhyp\n\n\nThe second step to decide the level of significance (\\(\\alpha\\)). You can choose one of the following: 1%, 1%, 5%. For our example, let‚Äôs select as 5% or .05.\nThe third step is to perform the test and obtain the p-value.\n\np-value =&lt; \\(\\alpha\\) : \\(H_1\\) hypothesis\np-value &gt; \\(\\alpha\\) : \\(H_0\\) hypothesis\n\nWe will perform the t-test on our dataset in R.\nt.test(data$TestScore ~ data$Class, var.equal = FALSE)\nWe use t.test() function from R. We have specified the test score and class variable. Here, the test score is the dependent variable and class is an independent variable. We have also specified that the variance of groups is not equal. It automatically selects Welsh t-test when we tell that the variance among groups is not the same.\n\n\n\nScreenshot 2021-03-09 at 12.27.02\n\n\nIn the results, we can see p-value. We will compare it with \\(\\alpha\\). We can see that the p-value is less than .05 (or our selected level of significance). Hence, we say that null hypothesis is rejected and the alternative hypothesis can be accepted. From the results, we can also see that group-A mean is higher than group-B. We can say that the average test score from class A is significanlty higher than the average test score from class B.\n\n\n\nOne sample t-test\nThis is the most basic scenario for a t-test. In our list of examples, the first example is where we use one-sample t-test. So if you have a single sample and you want to test whether the statistic (e.g., mean, variance, standard deviation) obtained from the sample is population statistic or not.\nFor this test, we will use women dataset from the R datasets package. This dataset has two attributes- height and weight of American women. We want to see whether the average height of American women is 65 (this is what we got from our sample) or not.\nSo here we set up the following hypothesis\n\nH_0 : Average height is same as 65 (hypothesis of no difference)\nH_1: Average height is not the same as 65.\n\nDon‚Äôt forget to test the assumptions for the t-test. In this case, we will go for testing the distribution is normal or not. You can plot the histogram or density plot and check for bell shape.\nWe then set the level of significance as 5% (or .05)\nLet‚Äôs perform the test now.\n# load datasets package\nlibrary(datasets)\n\nt.test(women$height,mu=65)\n\n\n\nScreenshot 2021-03-09 at 14.03.46\n\n\nFrom our results from the t-test, we can see that the p-value is not less than .05 which means we can not reject our null hypothesis.\n\n\nPaired t-test\nLet‚Äôs think about the third example which we have discussed in the start.\n\nWe collected two samples of test scores of same classroom at the start of the semester and end of the semester. We wanted to test whether the students‚Äô test scores‚Äô were improved significantly or not.\n\nFor this exercise, we have the following dataset of test scores of the same students at the start of the lecture and end of the lecture.\n\n\n\ntest-1\ntest-2\n\n\n\n\n175\n296\n\n\n329\n376\n\n\n238\n309\n\n\n60\n222\n\n\n271\n150\n\n\n291\n316\n\n\n364\n321\n\n\n402\n447\n\n\n70\n220\n\n\n335\n375\n\n\n300\n310\n\n\n245\n310\n\n\n186\n282\n\n\n31\n317\n\n\n104\n362\n\n\n132\n338\n\n\n94\n263\n\n\n38\n138\n\n\n62\n329\n\n\n139\n292\n\n\n94\n275\n\n\n48\n150\n\n\n68\n319\n\n\n138\n300\n\n\n\nSo data were collected from the same students. In this case, we will apply the paired t-test to test whether there is any improvement in students‚Äô test scores after attending the lecture or not.\nWe first need to test the assumptions (Skipped for it. You can check the assumptions here)\nWe will set-up our hypothesis\n\nH_0 : There is no difference in students‚Äô test performance before and after the lecture (null hypothesis)\nH_1 : There‚Äôs is a difference in students‚Äô test performance before and after the lecture.\n\nThen, we would set the level of significance as 5% (.05).\nWe will perform the test now using the same function but this time we specify the paired parameter as TRUE.\n\n\n\nScreenshot 2021-03-09 at 14.25.27\n\n\nIn the results, if we look at the p-value and that is .00000894. This value is smaller than \\(\\alpha\\) which means we can reject the null hypothesis. We accept the alternative hypothesis.\n\n\nANOVA\nANOVA or ANalysis Of VAriance test has its usage in a different scenario and here we are going to talk about the most basic one. We use an independent sample t-test when we have only two groups but when we have more than two groups we employ ANOVA test.\nLet‚Äôs take an example. We have a diet dataset (you can download it from here). This dataset has attributes person id, gender, height, diet, weight before taking diet, weight after taking the diet. There were three different diets were given. We want to test whether the diet has any impact on weight loss or not.\nSo first we compute the weight loss for each participant.\n&gt; data &lt;- import('anova_test_dataset_diet.csv')\n&gt; str(data)\n'data.frame':   78 obs. of  7 variables:\n $ Person      : int  25 26 1 2 3 4 5 6 7 8 ...\n $ gender      : int  NA NA 0 0 0 0 0 0 0 0 ...\n $ Age         : int  41 32 22 46 55 33 50 50 37 28 ...\n $ Height      : int  171 174 159 192 170 171 170 201 174 176 ...\n $ pre.weight  : int  60 103 58 60 64 64 65 66 67 69 ...\n $ Diet        : int  2 2 1 1 1 1 1 1 1 1 ...\n $ weight6weeks: num  60 103 54.2 54 63.3 61.1 62.2 64 65 60.5 ...\ndata$weightLoss &lt;- data$pre.weight - data$weight6weeks\nNow we will consider two attributes for our ANOVA test: weightLoss and type of diet.\nFollowing are assumptions for the ANOVA test\n\nThe dependent variable (weightLoss in our example) is normally distributed in each group (diet types in our example).\nThe variance of the dependent variable is the same for all groups of the independent variable.\nIndependence data samples.\n\nWe use aov() function in R to perform the ANOVA test.\n\n\n\nScreenshot 2021-03-09 at 15.00.15\n\n\nHere, you need to look for Pr(&gt;F) column that is p-value for the ANOVA test. We can see this value is smaller than .05 (5% level of significance) and even also than .01 (1% level of significance). We can say the null hypothesis is rejected. You can refer to this link for a detailed analysis of the same.\n\n\n\nWhen to do which test?\nYou might be already wondering why we have so many tests and how would I know when to apply which test. To simplify it, you can refer to the following\n\nYou have a single variable (continuous type or numeric in R) that you want to investigate and you are interested in finding some knowledge about the population on that variable.\nOne sample t-test\n\n\nYou have two variables- one continuous and another with categories. The number of categories is two.\nIndependent Sample t-test\n\n\nYou have the same scenario as above but now you have a second variable with more than two categories.\nANOVA\n\n\nYou have two variables (both continuous) containing data collected from the same participants.\nPaired t-test\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/gramian.html",
    "href": "posts/post-with-code/gramian.html",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "In this post, I will share my learning on this topic. I have been searching for a way to use a convolutional neural network (CNN) for my time-series data and this search led me to Gramian Angular Summation/Difference Fields (GASF/GADF). This post will give you a detailed introduction to Gramian Angular Field and show you ‚Äòhow to convert time-series data into image‚Äô with a help of an example.\n\n\nBefore starting the introduction, I think first we should get our self familiar with the basic concepts of GASF/GADF. If you are already aware then feel free to skip this section. Cartesian coordinates: You may have most likely seen this in your earlier mathematics classes. In this scheme, the position of a point (or other geometrical shapes) is determined by one or more numbers. For example, if we take two-dimensional coordinate systems then a position is determined with a pair of numbers, e.g., (2,3). The position is then presented with a point on distance from two reference lines, known as the x-axis and y-axis.\n\n\n\nImagine a circle with origin as center drawn on the image above shown image in a way that it crosses point (2,3). Now, we will take the radius of this circle and the angle between a line connecting (0,0) to (2,3) and X-axis. These two numbers now represent the same position but using a polar coordinate system. Taking the above example, the point will not be represented as (3.6, 56.3)\n\n\n\n\nLet‚Äôs say we have a set of vectors V. The Gramian (Gram) matrix is a matrix of inner products of every pair of vectors from V. As you can see in the below image, each element in the matrix &lt;vi,vj&gt; is vector production between vectors vi and vj.\n\n\n\n\n\nNow we will move towards the main goal of this post that is understanding the process of representing a time-series is represented in an image. In short, you can understand the process in three following steps. \n\n\n\nAggregate the time series by taking the mean of every M points to reduce the size. This step uses Piecewise Aggregation Approximation (PAA).\nScaling values in the interval [0,1].\nGenerating polar coordinates by taking timestamp as radius and inverse cosine (or arccosine) of scaled value. This will give us the value of the angle. Generate Gramian summation/difference angular fields. In this step, every pair values are summed (subtracted) and then cosine is taken on the summed value.\n\nIn case, if you couldn‚Äôt understand some part of the process no worries, we will see each step in detail below.\n\n\n\nI am providing here an example in python to demonstrate the state by step process of converting a time series into an image using gramian angular field.\n\n\nfrom pyts.approximation import PiecewiseAggregateApproximation\nfrom pyts.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nX = [[1,2,3,4,5,6,7,8],[23,56,52,46,34,67,70,60]]\nplt.plot(X[0],X[1])\nplt.title('Time series')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\n# PAA\ntransformer = PiecewiseAggregateApproximation(window_size=2)\nresult = transformer.transform(X)\n\n# Scaling in interval [0,1]\nscaler = MinMaxScaler()\nscaled_X = scaler.transform(result)\nplt.plot(scaled_X[0,:],scaled_X[1,:])\nplt.title('After scaling')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\narccos_X = np.arccos(scaled_X[1,:])\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(result[0,:], arccos_X)\nax.set_rmax(2)\nax.set_rticks([0.5, 1, 1.5, 2]) # Less radial ticks\nax.set_rlabel_position(-22.5) # Move radial labels away from plotted line\nax.grid(True)\nax.set_title(\"Polar coordinates\", va='bottom')\nplt.show()\n# Gramian angular summation fields\nfield = [a+b for a in arccos_X for b in arccos_X]\ngram = np.cos(field).reshape(-1,4)\nplt.imshow(gram)\n\n\n\n\n\n\n\nSome additional explanation\n\n\n\nThe aforementioned steps are for illustrating the process of converting time series into an image using Gramian Angular Summation/Difference Field. In practice, we don‚Äôt need to compute polar coordinates thanks to trigonometry (following rules). \\[\ncos(A+B) = cos(A)cos(B) - sin(A)sin(B)\n\\] \\[\n1 = sin^2(A) + cos^2(B)\n\\] For computing Cos(A+B) in Gramian Angular Field computation, we expand it as following\n\\[\ncos(A+B) = cos(A)cos(B) - sin(A)sin(B)\n\\] \\[\n= cos(A)cos(B) - \\sqrt(1- cos^2(A)) \\sqrt(1- cos^2(B))\n\\] \\[\n= x_a * x_b - \\sqrt(1- x_a^2) \\sqrt(1- x_b^2)\n\\] Because we computed A and B by taking cosine inverse of time series value (actually on values after PAA and scaling).\nYou can checkout this python library pyts.\n\n\n\n\n\n\n\n\nWang, Z., & Oates, T. (2015). Imaging time-series to improve classification and imputation. IJCAI International Joint Conference on Artificial Intelligence, 2015-January, 3939‚Äì3945.\nEamonn J Keogh and Michael J Paz- zani. Scaling up dynamic time warping for datamining applications. In Proceedings ofthe sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 285‚Äì 289. ACM, 2000.\nhttps://pyts.readthedocs.io/en/stable/_modules/pyts/image/gaf.html#GramianAngularField\nhttps://pyts.readthedocs.io/en/stable/index.html"
  },
  {
    "objectID": "posts/post-with-code/gramian.html#basic-concepts",
    "href": "posts/post-with-code/gramian.html#basic-concepts",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "Before starting the introduction, I think first we should get our self familiar with the basic concepts of GASF/GADF. If you are already aware then feel free to skip this section. Cartesian coordinates: You may have most likely seen this in your earlier mathematics classes. In this scheme, the position of a point (or other geometrical shapes) is determined by one or more numbers. For example, if we take two-dimensional coordinate systems then a position is determined with a pair of numbers, e.g., (2,3). The position is then presented with a point on distance from two reference lines, known as the x-axis and y-axis.\n\n\n\nImagine a circle with origin as center drawn on the image above shown image in a way that it crosses point (2,3). Now, we will take the radius of this circle and the angle between a line connecting (0,0) to (2,3) and X-axis. These two numbers now represent the same position but using a polar coordinate system. Taking the above example, the point will not be represented as (3.6, 56.3)\n\n\n\n\nLet‚Äôs say we have a set of vectors V. The Gramian (Gram) matrix is a matrix of inner products of every pair of vectors from V. As you can see in the below image, each element in the matrix &lt;vi,vj&gt; is vector production between vectors vi and vj."
  },
  {
    "objectID": "posts/post-with-code/gramian.html#gramian-angular-fields",
    "href": "posts/post-with-code/gramian.html#gramian-angular-fields",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "Now we will move towards the main goal of this post that is understanding the process of representing a time-series is represented in an image. In short, you can understand the process in three following steps. \n\n\n\nAggregate the time series by taking the mean of every M points to reduce the size. This step uses Piecewise Aggregation Approximation (PAA).\nScaling values in the interval [0,1].\nGenerating polar coordinates by taking timestamp as radius and inverse cosine (or arccosine) of scaled value. This will give us the value of the angle. Generate Gramian summation/difference angular fields. In this step, every pair values are summed (subtracted) and then cosine is taken on the summed value.\n\nIn case, if you couldn‚Äôt understand some part of the process no worries, we will see each step in detail below.\n\n\n\nI am providing here an example in python to demonstrate the state by step process of converting a time series into an image using gramian angular field.\n\n\nfrom pyts.approximation import PiecewiseAggregateApproximation\nfrom pyts.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nX = [[1,2,3,4,5,6,7,8],[23,56,52,46,34,67,70,60]]\nplt.plot(X[0],X[1])\nplt.title('Time series')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\n# PAA\ntransformer = PiecewiseAggregateApproximation(window_size=2)\nresult = transformer.transform(X)\n\n# Scaling in interval [0,1]\nscaler = MinMaxScaler()\nscaled_X = scaler.transform(result)\nplt.plot(scaled_X[0,:],scaled_X[1,:])\nplt.title('After scaling')\nplt.xlabel('timestamp')\nplt.ylabel('value')\nplt.show()\n\n\n\n\narccos_X = np.arccos(scaled_X[1,:])\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(result[0,:], arccos_X)\nax.set_rmax(2)\nax.set_rticks([0.5, 1, 1.5, 2]) # Less radial ticks\nax.set_rlabel_position(-22.5) # Move radial labels away from plotted line\nax.grid(True)\nax.set_title(\"Polar coordinates\", va='bottom')\nplt.show()\n# Gramian angular summation fields\nfield = [a+b for a in arccos_X for b in arccos_X]\ngram = np.cos(field).reshape(-1,4)\nplt.imshow(gram)\n\n\n\n\n\n\n\nSome additional explanation\n\n\n\nThe aforementioned steps are for illustrating the process of converting time series into an image using Gramian Angular Summation/Difference Field. In practice, we don‚Äôt need to compute polar coordinates thanks to trigonometry (following rules). \\[\ncos(A+B) = cos(A)cos(B) - sin(A)sin(B)\n\\] \\[\n1 = sin^2(A) + cos^2(B)\n\\] For computing Cos(A+B) in Gramian Angular Field computation, we expand it as following\n\\[\ncos(A+B) = cos(A)cos(B) - sin(A)sin(B)\n\\] \\[\n= cos(A)cos(B) - \\sqrt(1- cos^2(A)) \\sqrt(1- cos^2(B))\n\\] \\[\n= x_a * x_b - \\sqrt(1- x_a^2) \\sqrt(1- x_b^2)\n\\] Because we computed A and B by taking cosine inverse of time series value (actually on values after PAA and scaling).\nYou can checkout this python library pyts."
  },
  {
    "objectID": "posts/post-with-code/gramian.html#references",
    "href": "posts/post-with-code/gramian.html#references",
    "title": "Understanding Gramian Angular Field",
    "section": "",
    "text": "Wang, Z., & Oates, T. (2015). Imaging time-series to improve classification and imputation. IJCAI International Joint Conference on Artificial Intelligence, 2015-January, 3939‚Äì3945.\nEamonn J Keogh and Michael J Paz- zani. Scaling up dynamic time warping for datamining applications. In Proceedings ofthe sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 285‚Äì 289. ACM, 2000.\nhttps://pyts.readthedocs.io/en/stable/_modules/pyts/image/gaf.html#GramianAngularField\nhttps://pyts.readthedocs.io/en/stable/index.html"
  },
  {
    "objectID": "posts/post-with-code/Surname_prediction_using_Pytorch.html",
    "href": "posts/post-with-code/Surname_prediction_using_Pytorch.html",
    "title": "Building a surname classifier using PyTorch",
    "section": "",
    "text": "This post presents an NLP (Natural language processing) example based on the book NLP with PyTorch:Building intelligent language applications using deep learning. In the presented example, we will build a neural network to predict nationality based on surnames.\nThe post is targetted for beginners in the field of NLP. It will offer a hands-on experience of building a simple NLP application.\nWe will cover the following in this post.\n\nBasics of transforming a textual data into a numeric form\nPreparing dataset ready for training\nBuilding a neural network using PyTorch\nTraining & testing of the neural network model"
  },
  {
    "objectID": "posts/post-with-code/Surname_prediction_using_Pytorch.html#dictionary",
    "href": "posts/post-with-code/Surname_prediction_using_Pytorch.html#dictionary",
    "title": "Building a surname classifier using PyTorch",
    "section": "Dictionary",
    "text": "Dictionary\nWe will start with the dictionary. In simple terms, the dictionary stores each character (appearing in the surnames) in the storage at a particular index. In addition, it also offers mapping functionalities between position index and the corresponding character.\nThe following class implements the dictionary‚Äôs functionality for our dataset. The class has three crucial functions: add_token, lookup_token, and lookup_index. We also have a class function build_from_dataframe which processes the data frames and returns an object of the implemented class.\nThe following provides a simple and short explanation for each function in the class.\n\nInitialization: During initialization, we create two mapping dictionaries, one for index-token mapping, and another for token-index mapping. We also set the specified unk_token and its flag.  There is a likelihood of a surname during testing having a new character which was not in the training dataset. To handle such cases, we can specify whether to have a unk_token which can be used for any non-available characters in the dictionary.\nAdd a new token: Here, we first simply check whether the token we are going to add already exists in the dictionary or not. If the token is already in the dictionary then we simply return its corresponding index. Otherwise, we will add one entry in both the mappings.\nLooking up for an index: This is where we return the token stored on the specified index.\nLooking up for a token: This is similar to the previous function except this time we return the index for the specified token.\n\n\nclass SurnameVocabulary(object):\n    \n    def __init__(self,index_to_token=None,add_unk = True, unk_token='@'):\n        super(SurnameVocabulary, self)\n        \n        self._index_to_token = {}\n        self._token_to_index = {}\n        \n        if index_to_token is None:\n            index_to_token = {}\n            \n        \n        self._add_unk = add_unk\n            \n        self._unk_token = unk_token\n            \n        if self._add_unk:\n            self.add_token(unk_token)\n        \n        for key,value in index_to_token.items():\n            self._index_to_token[key] = value\n            self._token_to_index[value] = key\n            \n    @classmethod\n    def build_from_dataframe(cls,df,cutoff=5,add_unk=True):\n        temp = {}\n        token_counts = {}\n        for row in df.itertuples():\n            # count the frequency of each character (or token in this case)\n            for token in list(row.surname.lower()):\n                if token in token_counts.keys():\n                    token_counts[token.lower()] += 1\n                else:\n                    token_counts[token.lower()] =  1\n                    \n        for token in token_counts.keys():\n            # consider tokens appearing more than specified cutoff\n            if token_counts[token] &gt;= cutoff:\n                idx = len(temp)\n                \n                temp[idx+1] = token\n        \n        return cls(temp,add_unk)\n        \n    def add_token(self,token):\n        if token in self._token_to_index.keys():\n            idx = self._token_to_index[token]\n        else:\n            idx = len(self._token_to_index)\n            self._token_to_index[token] = idx\n            self._index_to_token[idx] = token\n        return idx\n        \n    def lookup_token(self,token):\n        if token in self._token_to_index.keys():\n            return self._token_to_index[token]\n        else:\n            return self._token_to_index[self._unk_token]\n    \n    def lookup_index(self,index):\n        if index in self._index_to_token.keys():\n            return self._index_to_token[index]\n        else:\n            raise Exception(\"Index not found\")\n            \n    def __len__(self):\n        return len(self._index_to_token)\n    \n    def __str__(self):\n        return 'SurnameVocabulary object(size={})'.format(len(self._index_to_token))\n        \n\n\nDictionary example\nAs we have now our class prepared, we will now initialize the dictionary using our surname dataset.\nWe can see below the mapping of dictionary where each index has a corresponding token. We also have @ as a unknown token (for handling new characters).\n\ns = SurnameVocabulary.build_from_dataframe(df,cutoff=1)\n\nprint(s._index_to_token)\n\n{0: '@', 1: 't', 2: 'o', 3: 'a', 4: 'h', 5: 'b', 6: 'u', 7: 'd', 8: 'f', 9: 'k', 10: 'r', 11: 'y', 12: 's', 13: 'e', 14: 'g', 15: 'c', 16: 'm', 17: 'i', 18: 'n', 19: 'w', 20: 'l', 21: 'z', 22: 'q', 23: 'j', 24: '-', 25: 'p', 26: 'x', 27: ':', 28: 'v', 29: '1', 30: '/', 31: '√©', 32: \"'\", 33: '√ß', 34: '√™', 35: '√ü', 36: '√∂', 37: '√§', 38: '√º', 39: '√∫', 40: '√†', 41: '√≤', 42: '√®', 43: '√≥', 44: '√π', 45: '√¨', 46: '≈õ', 47: 'ƒÖ', 48: '≈Ñ', 49: '√°', 50: '≈º', 51: '≈Ç', 52: '√µ', 53: '√£', 54: '√≠', 55: '√±'}\n\n\nLet‚Äôs now move to the second component of vectorization process, i.e., vectorizer."
  },
  {
    "objectID": "posts/post-with-code/Surname_prediction_using_Pytorch.html#testing-the-classifier-using-validation-set",
    "href": "posts/post-with-code/Surname_prediction_using_Pytorch.html#testing-the-classifier-using-validation-set",
    "title": "Building a surname classifier using PyTorch",
    "section": "Testing the classifier using validation set",
    "text": "Testing the classifier using validation set\nHere, we will use the validation set for prediction. The code is similar to the training code except that we set the mode of classifier to testing and we don‚Äôt make the backward pass and update the parameters.\n\n# setting split for validation data\ndata.set_split('val')\ndataloader_val = DataLoader(data,batch_size=64)\nprint('Val data size:',len(data))\nrunning_loss = 0.0\nrunning_acc = 0.0\n\nclassifier.eval()\n\nfor batch_index,batch in enumerate(dataloader_val):\n    \n    output = classifier(batch['input_x'])\n    \n    loss = loss_fnc(output,batch['label'])\n    \n    loss_t = loss.item()\n    \n    acc_t = compute_accuracy(output,batch['label'])\n    \n    running_loss += (loss_t - running_loss)/(batch_index + 1)\n    running_acc  += (acc_t - running_acc)/(batch_index + 1)\n    \nprint('Validation Loss:%.2f'%running_loss)\nprint('Validation Accuracy:%.2f'%running_acc)\n\nVal data size: 1640\nValidation Loss:1.82\nValidation Accuracy:60.07"
  },
  {
    "objectID": "posts/post-with-code/Surname_prediction_using_Pytorch.html#testing-the-classifier-using-test-set",
    "href": "posts/post-with-code/Surname_prediction_using_Pytorch.html#testing-the-classifier-using-test-set",
    "title": "Building a surname classifier using PyTorch",
    "section": "Testing the classifier using test set",
    "text": "Testing the classifier using test set\nHere, we will apply our classifier to the test set. We will follow the same procedure as we followed with the validation set.\n\n# setting split for test data\ndata.set_split('test')\nprint('Test data size:',len(data))\ndataloader_test = DataLoader(data,batch_size=100)\n\nrunning_loss = 0.0\nrunning_acc = 0.0\nclassifier.eval()\nfor batch_index,batch in enumerate(dataloader_test):\n    \n    output = classifier(batch['input_x'])\n    \n    loss = loss_fnc(output,batch['label'])\n    \n    loss_t = loss.item()\n    \n    acc_t = compute_accuracy(output,batch['label'])\n    \n    running_loss += (loss_t - running_loss)/(batch_index + 1)\n    running_acc  += (acc_t - running_acc)/(batch_index + 1)\n    \nprint('Loss:%.2f'%running_loss)\nprint('Accuracy:%.2f'%running_acc)\n\nTest data size: 1660\nLoss:1.89\nAccuracy:59.88"
  },
  {
    "objectID": "awards.html",
    "href": "awards.html",
    "title": "Awards",
    "section": "",
    "text": "Special Research Award for Combining Teaching Methodology and Technology\nEstonian Youth Board | Estonia  Link Research, Education\n\nThe award was given under the ITL Ustus Agur Scholarship for my research activities.\n\n\n\nBest Educational Technology Demonstration Award\nInternational Society of Learning Analytics Research | USA Link Research, Technology, Education\n\nThe award was given at the International Conference of Learning Analytics and Knowledge, Texas, Usa (2023) (paper acceptance rate: ~30%).\nThe award signifies the importance of the research project (i.e., CoTrack) for the education technology community.\n\n\n\nBest Paper Nominee\nInternational Society of Learning Analytics Research | USA  Paper-link  Research\n\nRecognized in Top 5 research papers in short paper category at the International Conference of Learning Analytics and Knowledge, Texas, Usa (2023).\n\n\n\nEC-TEL Scholarship\nEuropean Association of Technology-Enhanced Learning | EU  Research\n\nAwarded by European Association of Technology-Enhanced Learning for attending EC-TEL summer school in Greece in 2022.\n\n\n\nDora Plus Scholarship for Foreign Doctoral Students\nEuropean Regional Development Fund | Estonia  Research\n\nAwarded to Ph.D.¬†students based on their academic performance.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/Sentiment prediction.html",
    "href": "posts/post-with-code/Sentiment prediction.html",
    "title": "Sentiment prediction using Pytorch",
    "section": "",
    "text": "This year, I started exploring NLP (natural language processing) given its huge potential for building intelligent systems capable of understanding and generating text in human languages. In this exploration, I have developed an understanding of how nlp applications works. I am now engaging myself with building real-life applications.\nThis post reflects upon my understanding of NLP and pitch to build a sentiment classifier with the help of an example. The example is taken from the book that I am reading to gain a deeper understanding of nlp from the practical side Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning.\nI hope this post will be helpful for others who are starting to learn more about NLP.\n\n\nIn this post, I have used a smaller version of Yelp dataset which is a dataset of customer‚Äôs reviews and ratings. The dataset is a processed version of the original dataset.\nFor example, in original dataset the ratings are given from 1 to 5, while in this dataset ratings are divied into two categories, i.e., negative and positive. Addtionally, the dataset is already partitioned into train, val and test sets.\nLet‚Äôs see how the records in the yelp dataset look like\n\nimport pandas as pd\n\n# reading yelp dataset file\ndf = pd.read_csv('./yelp/reviews_with_splits_lite.csv')\n\nprint('Size:',df.shape[0])\n\ndf.head()\n\nSize: 56000\n\n\n\n\n\n\n\n\n\nrating\nreview\nsplit\n\n\n\n\n0\nnegative\nall i can say is that a i had no other option ...\ntrain\n\n\n1\nnegative\ni went here once when my long time stylist mov...\ntrain\n\n\n2\nnegative\ni don t know why i stopped here for lunch this...\ntrain\n\n\n3\nnegative\ndid i order the wrong thing ? or maybe it was ...\ntrain\n\n\n4\nnegative\ni went here for restaurant week . the restaura...\ntrain\n\n\n\n\n\n\n\nIn the dataset, we have three columns: rating, review and split. The first column is the rating assigned by the customers. The second column is the text review and the final column contains labels to decide whether the record belongs to a train, val or test set.\nWe want to build a classifier that can predict the rating given the review. So the text review will be input to the classifier and the rating will be predicted.\nThe challenge we have here is that the input and output data both are in text format while machine learning works only with numbers. To address this challenge, we need to find a way to transform our text review data into numbers which is also known as vectorization in nlp.\nLet‚Äôs understand the basic of vectorization. The below image offers an pictorial presentation of vectorization process.\n\n\n\nvectorization\n\n\nWe have a text review that we like to vectorize or convert into a sequence of numbers. There are multiple ways to do that, and we will use the most basic technique for understanding purpose, i.e., using a vocabulary. Vocabulary is a storage that stores words as represented in the above figure. In other terms, vocabulary offers a mapping service between words and their stored location (or index). For example, the one shown in the above figure stores 500 words and each word has an associated index.\nThe vectorization process will start with creating a vector of size 500. The process then checks which words of vocabulary are present in the text and sets their corresponding values in the vector as 1.\nFor example, the first word of the vocabulary (i.e., one) is not present in the review text, therefore, there is 0 in the vector at the start position. Then the next word in the vocabulary is checked (i.e., i). Now this time ‚Äòi‚Äô is present in the review text, therefore, 1 is stored next in the vector, and so on. This process is repeated for every word stored in the vocabulary.\n\n\n\nAs we have seen in the example above we need to vectorize the review text, and for that we need a vocabulary and a vectorizer.\nWe will start by creating a vocabulary and vectorizer class.\n\n\nThe core process of building a vocabulary includes scanning all the text, breaking it into words, and storing unique words. In addition to storing, vocabulary also provides the functionality of mapping between words (or tokens) and their respective indices (i.e., where those words are stored).\nThe following class offers those functionalities. There is a possibility that during the vectorization process, we are likely to have words that are not available in the vocabulary. To handle those cases, we can store some special words representing all unavailable words, e.g., ‚Äò&lt;UNK&gt;‚Äô.\n\nclass Vocabulary(object):\n    def __init__(self, token_to_idx = None, add_unk=True, unk_token='&lt;UNK&gt;'):\n        \"\"\"\n        params:\n            token_to_idx (dict): mapping from token to index\n            add_unk (bool): flag to add a special token to the vocabulary for unknowns tokens\n            unk_token (str): Token used as special token\n        \n        \"\"\"\n        if token_to_idx is None:\n            token_to_idx ={}\n        \n        self._token_to_idx = token_to_idx\n        self._idx_to_token = {idx:token for token,idx in token_to_idx}\n        \n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        \n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token)\n            \n    def to_serialize(self):\n        \"\"\" function to serialize the content of vocabulary\n        \"\"\"\n        return {'idx_to_token':self._idx_to_token,\n               'add_unk':self._add_unk,\n               'unk_token':self._unk_token}\n    \n    @classmethod\n    def from_serializable(cls,contents):\n        \"\"\"\n        class function to create a vocabulary from serialized content\n        \"\"\"\n        return cls(**contents)\n    \n    def add_token(self,token):\n        \"\"\"\n        Add token to the vocabulary\n        \n        params:\n            token (str): token to add to the vocabulary\n            \n        returns:\n            idx (int): index of token\n        \n        \"\"\"\n        if token in self._token_to_idx:\n            return self._token_to_idx[token]\n        else:\n            idx = len(self)\n            self._token_to_idx[token] = idx\n            self._idx_to_token[idx] = token\n        return idx\n    \n    \n    def lookup_idx(self,idx):\n        \"\"\"\n        Lookup vocabulary to fetch  token at idx\n        \n        params:\n            idx(int) : index of token to be fetched\n            \n        returns:\n            token (str): token stored at idx\n        \"\"\"\n        if idx not in self._idx_to_token:\n            raise KeyError(\"Vocabulary does not have token with specified index:\"%idx)\n        return self._idx_to_token[idx]\n    \n\n    def lookup_token(self,token):\n        \"\"\"\n        Lookup vocabulary to fetch index of a token\n        \n        params:\n            token(str): token to lookup\n            \n        returns:\n            idx (int): index of token\n        \"\"\"\n        \n        if token not in self._token_to_idx:\n            return self.unk_index\n        else:\n            return self._token_to_idx[token]\n    \n    def __len__(self):\n        return len(self._idx_to_token)\n    \n    \n    def __str__(self):\n        return \"Vocabulary (size = %d)\" % len(self)\n    \n    \n\n\n\nLet‚Äôs see one example of using Vocabulary class for the following text.\ntext = \"\"\"This is a good example of illustrating the use of pytorch for natural language processing. The example shows how to build a vocabulary which is a collection of words and their mapping to their corresponding indices. \"\"\"\n\n# raw text\ntext = \"\"\"This is a good example of illustrating the use of pytorch for natural language processing. The example shows how to build a vocabulary which is a collection of words and their mapping to their corresponding indices. \"\"\"\n\n#preparing a vocabulary object\nvoc = Vocabulary(add_unk=True)\n\n#adding token to the vocabulary\nfor word in text.strip().split(' '):\n    voc.add_token(word)\n\n#printing vocabulary mapping\nprint(voc._token_to_idx)\n\n{'&lt;UNK&gt;': 0, 'This': 1, 'is': 2, 'a': 3, 'good': 4, 'example': 5, 'of': 6, 'illustrating': 7, 'the': 8, 'use': 9, 'pytorch': 10, 'for': 11, 'natural': 12, 'language': 13, 'processing.': 14, 'The': 15, 'shows': 16, 'how': 17, 'to': 18, 'build': 19, 'vocabulary': 20, 'which': 21, 'collection': 22, 'words': 23, 'and': 24, 'their': 25, 'mapping': 26, 'corresponding': 27, 'indices.': 28}\n\n\nAs we can see that each word mapped to an index. We also have one special token ‚Äò&lt;UNK&gt;‚Äô. To see its usage, we will perform a lookup for a word which is not in the vocabulary.\n\n# lookup for the word 'estonia'\nprint(voc.lookup_token('estonia'))\n\n0\n\n\nThe vocabulary simply returned the index of ‚Äò&lt;UNK&gt;‚Äô.\n\n\n\n\n\nNow, let‚Äôs move to the vectorization process. This is the process where we will transform review text into vectors. These vectors will contain indices of each word in the review text.\nFor example, let‚Äôs say we have a text ‚Äúhow to build‚Äù which we want to vectorize. For that, we need a mapping (token to index) or vocabulary where we have information about indices of tokens.\nIf we use our demo vocabulary from the above example then ‚Äúhow to build‚Äù will be transformed into a vector.\n\n# creating a list of vocabulary size\nvector = [0]* len(voc)\n\n# lookup for each token in the sentence\nfor token in \"how to build\".split(' '):\n    \n    # fetch index of the current token\n    index = voc.lookup_token(token)\n    \n    # set the value at index as 1\n    vector[index] = 1\nprint('Vectorized version:',vector)\n\nVectorized version: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nNow, that we have an idea of what vectorizer does, we will move to create a vectorizer class offering text-to vector transformation functionality.\n\nclass ReviewVectorizer(object):\n    \"\"\"\n    Vectorizer class to transform review text into vectors\n    \"\"\"\n    def __init__(self,review_vocab, rating_vocab):\n        \"\"\"\n        params:\n            review_vocab (Vocabulary): vocabulary object for review text\n            rating_vocab (Vocabulary): vocabulary obejct for rating \n        \"\"\"\n        self.review_vocab = review_vocab\n        self.rating_vocab = rating_vocab \n        \n    def vectorize(self,text):\n        \"\"\"\n        perform vectorization of given text\n        \n        params:\n            text (str): review text to transform into vector\n            \n        returns:\n            one_hot (array): returns one-hot encoding of text\n        \"\"\"\n        one_hot = np.zeros(len(self.review_vocab),dtype=np.float32)\n        \n        # iterate over each word in the  text\n        for word in text.strip().split():\n            # avoid if the word is a punctuation\n            if word not in string.punctuation:\n                # fetching index of the word \n                idx = self.review_vocab.lookup_token(word)\n                \n                # setting 1 at idx index\n                one_hot[idx] = 1\n                \n        return one_hot\n        \n    @classmethod\n    def from_dataframe(cls,review_df,cutoff=25):\n        \"\"\"\n        This function builds vocabulary for review text and rating.\n        \n        params:\n            review_df (pandas.DataFrame): dataframe containing yelp dataset\n            cutoff (int): a threshold to store words into vocabulary\n            \n        returns:\n            ReviewVectorizer object\n        \"\"\"\n        review_vocab = Vocabulary(add_unk=True)\n        rating_vocab = Vocabulary(add_unk=False)\n        \n        # adding all unique rating to the rating_vocubulary\n        for rating in review_df['rating'].unique():\n            rating_vocab.add_token(rating)\n            \n        word_count = {}\n        \n        # counting frequency of each word which appeared in the review text\n        for review in review_df.review:\n            for word in review.strip().split(' '):\n                if word not in string.punctuation:\n                    if word in word_count.keys():\n                        word_count[word] += 1\n                    else:\n                        word_count[word] = 1\n        \n        # adding tokens from review text to the review vocabulary\n        for word,count in word_count.items():\n            if count &gt; cutoff:\n                review_vocab.add_token(word)\n        \n        return cls(review_vocab,rating_vocab)\n    \n    @classmethod\n    def from_serializable(cls, contents):\n        \"\"\"\n        class function to create ReviewVectorizer from serialzed contents\n        \n        params:\n            contents(dict): a dictionary containing contents for review and rating vacabulary\n        \n        returns:\n            ReviewVectorizer object\n        \"\"\"\n        \n        return cls(review_vocab = Vocabulary.from_serialiazable(contents['reivew_vocab']),\n                  rating_voca = Vocabulary.from_serializable(contents['rating_vocab']))\n    \n    def to_serializable(self):\n        \"\"\"\n        To serialize vocabularies \n        \n        returns:\n            contents (dict): contents of review and rating vocabularies\n        \n        \"\"\"\n        return {'review_vocab':self.review_vocab.to_serializable(),\n               'rating_vocab':self.rating_vocab.to_serializable()}\n\nNow, we have vocabulary and vectorizer classes ready. Next, we need to create a dataset class inherting pytorch‚Äôs Dataset class. This class enables uses of pytorch functionality with out dataset.\n\n\nBefore training a machine learning model, usually there are some preprocessing tasks, e.g., vectorizing the text, converting images to tensors, etc. These tasks are generally performed separately from the model training. PyTorch offers a systematic way to abstract those tasks. Here, we will see the use of two classes ‚ÄòDataset‚Äô and ‚ÄòDataLoader‚Äô.\nThe first class is related to reading data files, performing some preprocessing tasks, and fetching training instances. While the second class offers a wrapper around the dataset class to fetch data in batches for training.\nLet‚Äôs dive into the code.\n\n\nclass YelpDataset(Dataset):\n    \"\"\"\n    Dataset class utilizing pytorch Dataset template\n    \n    \"\"\"\n    def __init__(self, review_df, vectorizer):\n        \"\"\"\n        review_df (pandas.DataFrame): dataframe containing yelp data records\n        vectorizer (ReviewVectorizer): ReviewVectorizer object\n        \"\"\"\n        self.review_df = review_df\n        self._vectorizer =  vectorizer \n        \n        self.train_df = self.review_df[self.review_df.split=='train']\n        self.train_size = self.train_df.shape[0]\n        \n        self.val_df = self.review_df[self.review_df.split=='val']\n        self.val_size = self.val_df.shape[0]\n        \n        self.test_df = self.review_df[self.review_df.split=='test']\n        self.test_size = self.test_df.shape[0]\n        \n        self._lookup_dict = {'train':(self.train_df,self.train_size),\n                             'val':(self.val_df,self.val_size),\n                             'test':(self.test_df,self.test_size)}\n        \n        self.set_split('train')\n        \n    def get_vectorizer(self):\n        return self._vectorizer\n        \n    @classmethod\n    def load_dataset_and_make_vectorizer(cls,review_csv):\n        \"\"\"class function to load dataset and initialize the vectorizer\n        \n        params: \n            review_csv (str): file_name\n        \n        returns:\n            ReviewDataset object\n        \"\"\"\n        \n        review_df = pd.read_csv(review_csv)\n        train_review_df = review_df[review_df.split=='train']\n        \n        return cls(review_df,ReviewVectorizer.from_dataframe(train_review_df))\n        \n    def set_split(self,split='train'):\n        \"\"\"\n        function to set the current active dataset\n        \n        params:\n            split (str): specify part of dataset to be used\n        \n        \"\"\"\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n        \n    def __len__(self):\n        return self._target_size\n        \n    def __getitem__(self,idx):\n        \"\"\"\n        function to fetch record at index idx\n        \n        params:\n            idx (int): index of record to be fetched\n        \"\"\"\n        row = self._target_df.iloc[idx]\n        review_vector = self._vectorizer.vectorize(row.review)\n        review_rating = self._vectorizer.rating_vocab.lookup_token(row.rating)\n        return {'x_data':review_vector,\n               'y_target':review_rating}\n    \n    def get_num_batches(self,batch_size):\n        return self._target_size//batch_size\n\nTo create a custom ‚ÄòDataset‚Äô class, two functions must be defined, i.e., len, getitem. The first function returns the length of the dataset and the second function fetches the training instance at a given index.\nIn our YelpDataset class, we have added some more functions as well to simplify the loading of training, validation, and test sets.\n\n# checking the dataset\nyelpDB = YelpDataset.load_dataset_and_make_vectorizer('./yelp/reviews_with_splits_lite.csv')\n\n\nfor split in ['train','test','val']:\n    yelpDB.set_split(split)\n    print(f'Split:{split}     Size:{len(yelpDB)}')\n\nSplit:train     Size:39200\nSplit:test     Size:8400\nSplit:val     Size:8400\n\n\nWe have our dataset ready. We can now make use of DataLoader from Pytorch to get data instanes in batches.\n\n\n\nThe following function creates a dataloader for our Yelp dataset. This dataloader return data in batches.\n\nimport string\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\ndef generate_batches(dataset, batch_size, shuffle=True,drop_last=True):\n    \"\"\"\n    function to get data batches\n    @ This code is targetted for cpu machines.\n    \n    params:\n        dataset (YelpDataset): YelpDataset object\n        batch_size (int): data batch size\n        shuffle (bool): Whether to shuffle data\n        drop_last (bool): flag to drop the last batch if it is less than the batch size\n        \n    \"\"\"\n    \n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n    for data_dict in dataloader:\n        out_data_dict = {}\n        \n        for name,tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name]\n        yield out_data_dict\n\nLet‚Äôs see now how one batch look like.\n\nbatch = next(iter(generate_batches(yelpDB,batch_size=100)))\nprint('Size:',batch['x_data'].size())\nprint('Batch example:',batch)\n\nSize: torch.Size([100, 7356])\nBatch example: {'x_data': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n        [1., 0., 1.,  ..., 0., 0., 0.],\n        [1., 1., 1.,  ..., 0., 0., 0.],\n        ...,\n        [1., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.],\n        [1., 1., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n        0, 1, 0, 1])}\n\n\n\n\n\nNow, we will move to build our neural network to classify review text into positive or negative categories of rating. To build a custom neural network, the ‚ÄòModule‚Äô class needs to be inherited. The following class does that.\nNext, during the initialization (i.e., inside the init function) neural network architecture is specified. In our case, we have having a single Linear layer.\nFinally, in the forward function the computation is performed. In the following class, we are simply performing linear computation of the form \\(W*X\\) where \\(W\\) represent weights and \\(X\\) represent input data. On the computed result, the sigmoid function is applied.\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ReviewClassifier(nn.Module):\n    \"\"\"\n    ReviewClassifier class\n    \"\"\"\n    def __init__(self,input_size):\n        \"\"\"\n        Initialize the classifier\n        params:\n            input_size (int): number of features\n        \"\"\"\n        super(ReviewClassifier,self).__init__()\n        self.fc1 = nn.Linear(in_features = input_size,\n                             out_features = 1)\n        \n    def forward(self,inputs,apply_sigmoid = False):\n        \"\"\"\n        function performing forward pass\n        params:\n            inputs (tensor): input vectors\n            apply_sigmoid (bool): flag whether to apply sigmoid function or not\n            \n        returns:\n            y_out (tensors): shape (batch_size,)\n        \"\"\"\n        y_out = self.fc1(inputs).squeeze()\n        if apply_sigmoid:\n            y_out = F.sigmoid(y_out)\n        return y_out\n\n\n\n\nWe will load our database which will create a vocabulary and vectorizer. We then get the vectorizer and initialize our classifier.\n\n# loading the dataset\nyelpDB = YelpDataset.load_dataset_and_make_vectorizer('./yelp/reviews_with_splits_lite.csv')\n\n# getting the vectorizer\nvectorizer = yelpDB.get_vectorizer()\n\n# initializing the classifier\nclassifier = ReviewClassifier(len(vectorizer.review_vocab))\n\n\ndef compute_accuracy(y_pred,y_target):\n    \"\"\"\n    function to compute accuracy metrics using logits\n    \"\"\"\n    # applying sigmoid function and generating predicted class indices based on .5 threshold\n    y_pred_indices = (torch.sigmoid(y_pred) &gt; 0.5).long()\n\n    # counting correctly predicted instances\n    n_correct = (y_pred_indices == y_target).sum().item()\n    \n    return n_correct / len(y_pred_indices) * 100\n\nNow, we will move to the most fun part of building our classifier, i.e., training the neural network with the yelp dataset. During the training, we iterate over several epochs. During each epoch, training occurs on the complete training dataset.\nFor each epoch, we start by setting the gradients of the optimizer as zero. Then, we perform a forward pass where computation is performed on the input data. The output of the classifier is used to compute the loss which is then used to make a backward pass. Finally, the weight parameters are updated. We have also included code to compute running loss and accuracy for training and validation both.\n\nDuring validation and testing, we only perform the forward pass.\n\n\nfrom torch import optim\n\n# for reproducibility\nnp.random.seed(3223)\ntorch.manual_seed(3223)\n\n# loss function\nloss_func = nn.BCEWithLogitsLoss()\n\n# optimizer\noptimizer = optim.Adam(classifier.parameters(),lr=.0001)\n\nnum_epochs = 20\nbatch_size = 100\n\n# iterate over number of epochs\nfor epoch in range(num_epochs):\n    \n    # set the split to use training data\n    yelpDB.set_split('train')\n    \n    batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n    \n    train_running_loss = 0.0\n    train_running_acc = 0.0\n    \n    classifier.train()\n\n    for batch_index, batch_dict in enumerate(batch_generator):\n        \n        # zeros the optimizer gradients\n        optimizer.zero_grad()\n        \n        # forward pass\n        y_pred = classifier(batch_dict['x_data'].float())\n        \n        # computing loss\n        loss = loss_func(y_pred,batch_dict['y_target'].float())\n        \n        loss_t = loss.item()\n        \n        train_running_loss += (loss_t - train_running_loss)/ (batch_index + 1)\n        \n        acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n        \n        train_running_acc += (acc_t - train_running_acc) / (batch_index + 1)\n        \n        # backward pass\n        loss.backward()\n        \n        # updating the weight parameters\n        optimizer.step()\n\n    # setting the split to use validation set\n    yelpDB.set_split('val')\n    \n    # batch generator\n    batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n    val_running_loss = 0.0\n    val_running_acc = 0.0\n    classifier.eval()\n    for batch_index, batch_dict in enumerate(batch_generator):\n        \n        # output of neural network\n        y_pred = classifier(batch_dict['x_data'].float())\n        \n        # coputing loss\n        loss = loss_func(y_pred,batch_dict['y_target'].float())\n        \n        loss_t = loss.item()\n        \n        val_running_loss += (loss_t - val_running_loss)/ (batch_index + 1)\n        \n        acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n        val_running_acc += (acc_t - val_running_acc) / (batch_index + 1)\n        \n    print('\\nEpoch :{}'.format(epoch))\n    print('  Training   ==&gt; Loss: {:.2f} Accuracy: {:.2f}'.format(train_running_loss,train_running_acc))\n    print('  Validation ==&gt; Loss: {:.2f} Accuracy: {:.2f}'.format(val_running_loss,val_running_acc))\n\n    \n\n\nEpoch :0\n  Training   ==&gt; Loss: 0.64 Accuracy: 72.57\n  Validation ==&gt; Loss: 0.60 Accuracy: 82.32\n\nEpoch :1\n  Training   ==&gt; Loss: 0.57 Accuracy: 83.34\n  Validation ==&gt; Loss: 0.54 Accuracy: 84.48\n\nEpoch :2\n  Training   ==&gt; Loss: 0.52 Accuracy: 84.79\n  Validation ==&gt; Loss: 0.50 Accuracy: 85.15\n\nEpoch :3\n  Training   ==&gt; Loss: 0.48 Accuracy: 85.93\n  Validation ==&gt; Loss: 0.46 Accuracy: 85.92\n\nEpoch :4\n  Training   ==&gt; Loss: 0.45 Accuracy: 86.85\n  Validation ==&gt; Loss: 0.44 Accuracy: 86.58\n\nEpoch :5\n  Training   ==&gt; Loss: 0.42 Accuracy: 87.57\n  Validation ==&gt; Loss: 0.42 Accuracy: 87.19\n\nEpoch :6\n  Training   ==&gt; Loss: 0.40 Accuracy: 88.26\n  Validation ==&gt; Loss: 0.40 Accuracy: 87.82\n\nEpoch :7\n  Training   ==&gt; Loss: 0.38 Accuracy: 88.91\n  Validation ==&gt; Loss: 0.38 Accuracy: 88.40\n\nEpoch :8\n  Training   ==&gt; Loss: 0.37 Accuracy: 89.40\n  Validation ==&gt; Loss: 0.37 Accuracy: 88.93\n\nEpoch :9\n  Training   ==&gt; Loss: 0.35 Accuracy: 89.82\n  Validation ==&gt; Loss: 0.35 Accuracy: 89.24\n\nEpoch :10\n  Training   ==&gt; Loss: 0.34 Accuracy: 90.19\n  Validation ==&gt; Loss: 0.34 Accuracy: 89.49\n\nEpoch :11\n  Training   ==&gt; Loss: 0.33 Accuracy: 90.47\n  Validation ==&gt; Loss: 0.33 Accuracy: 89.83\n\nEpoch :12\n  Training   ==&gt; Loss: 0.32 Accuracy: 90.78\n  Validation ==&gt; Loss: 0.32 Accuracy: 90.11\n\nEpoch :13\n  Training   ==&gt; Loss: 0.31 Accuracy: 91.01\n  Validation ==&gt; Loss: 0.31 Accuracy: 90.29\n\nEpoch :14\n  Training   ==&gt; Loss: 0.30 Accuracy: 91.22\n  Validation ==&gt; Loss: 0.31 Accuracy: 90.37\n\nEpoch :15\n  Training   ==&gt; Loss: 0.29 Accuracy: 91.44\n  Validation ==&gt; Loss: 0.30 Accuracy: 90.48\n\nEpoch :16\n  Training   ==&gt; Loss: 0.28 Accuracy: 91.62\n  Validation ==&gt; Loss: 0.29 Accuracy: 90.63\n\nEpoch :17\n  Training   ==&gt; Loss: 0.28 Accuracy: 91.72\n  Validation ==&gt; Loss: 0.29 Accuracy: 90.80\n\nEpoch :18\n  Training   ==&gt; Loss: 0.27 Accuracy: 91.87\n  Validation ==&gt; Loss: 0.28 Accuracy: 90.83\n\nEpoch :19\n  Training   ==&gt; Loss: 0.27 Accuracy: 92.01\n  Validation ==&gt; Loss: 0.28 Accuracy: 90.93\n\n\n\n\n\n\nNow, we will see how well our classfier performs on test set.\n\nyelpDB.set_split('test')\n    \nbatch_generator = generate_batches(yelpDB,batch_size=batch_size)\n    \ntest_running_loss = 0.0\ntest_running_acc = 0.0\n    \nclassifier.eval()\n\n    \nfor batch_index, batch_dict in enumerate(batch_generator):\n    y_pred = classifier(batch_dict['x_data'].float())\n    loss = loss_func(y_pred,batch_dict['y_target'].float())\n    loss_t = loss.item()\n    test_running_loss += (loss_t - test_running_loss)/ (batch_index + 1)\n    acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n    test_running_acc += (acc_t - test_running_acc) / (batch_index + 1)\n    \n\nprint('  Test   ==&gt; Loss: {:.2f} Accuracy: {:.2f}'.format(test_running_loss,test_running_acc))\n   \n\n  Test   ==&gt; Loss: 0.29 Accuracy: 90.33"
  },
  {
    "objectID": "posts/post-with-code/Sentiment prediction.html#yelp-dataset",
    "href": "posts/post-with-code/Sentiment prediction.html#yelp-dataset",
    "title": "Sentiment prediction using Pytorch",
    "section": "",
    "text": "In this post, I have used a smaller version of Yelp dataset which is a dataset of customer‚Äôs reviews and ratings. The dataset is a processed version of the original dataset.\nFor example, in original dataset the ratings are given from 1 to 5, while in this dataset ratings are divied into two categories, i.e., negative and positive. Addtionally, the dataset is already partitioned into train, val and test sets.\nLet‚Äôs see how the records in the yelp dataset look like\n\nimport pandas as pd\n\n# reading yelp dataset file\ndf = pd.read_csv('./yelp/reviews_with_splits_lite.csv')\n\nprint('Size:',df.shape[0])\n\ndf.head()\n\nSize: 56000\n\n\n\n\n\n\n\n\n\nrating\nreview\nsplit\n\n\n\n\n0\nnegative\nall i can say is that a i had no other option ...\ntrain\n\n\n1\nnegative\ni went here once when my long time stylist mov...\ntrain\n\n\n2\nnegative\ni don t know why i stopped here for lunch this...\ntrain\n\n\n3\nnegative\ndid i order the wrong thing ? or maybe it was ...\ntrain\n\n\n4\nnegative\ni went here for restaurant week . the restaura...\ntrain\n\n\n\n\n\n\n\nIn the dataset, we have three columns: rating, review and split. The first column is the rating assigned by the customers. The second column is the text review and the final column contains labels to decide whether the record belongs to a train, val or test set.\nWe want to build a classifier that can predict the rating given the review. So the text review will be input to the classifier and the rating will be predicted.\nThe challenge we have here is that the input and output data both are in text format while machine learning works only with numbers. To address this challenge, we need to find a way to transform our text review data into numbers which is also known as vectorization in nlp.\nLet‚Äôs understand the basic of vectorization. The below image offers an pictorial presentation of vectorization process.\n\n\n\nvectorization\n\n\nWe have a text review that we like to vectorize or convert into a sequence of numbers. There are multiple ways to do that, and we will use the most basic technique for understanding purpose, i.e., using a vocabulary. Vocabulary is a storage that stores words as represented in the above figure. In other terms, vocabulary offers a mapping service between words and their stored location (or index). For example, the one shown in the above figure stores 500 words and each word has an associated index.\nThe vectorization process will start with creating a vector of size 500. The process then checks which words of vocabulary are present in the text and sets their corresponding values in the vector as 1.\nFor example, the first word of the vocabulary (i.e., one) is not present in the review text, therefore, there is 0 in the vector at the start position. Then the next word in the vocabulary is checked (i.e., i). Now this time ‚Äòi‚Äô is present in the review text, therefore, 1 is stored next in the vector, and so on. This process is repeated for every word stored in the vocabulary."
  },
  {
    "objectID": "posts/post-with-code/Sentiment prediction.html#building-a-machine-model-to-predict-rating-based-on-review-text",
    "href": "posts/post-with-code/Sentiment prediction.html#building-a-machine-model-to-predict-rating-based-on-review-text",
    "title": "Sentiment prediction using Pytorch",
    "section": "",
    "text": "As we have seen in the example above we need to vectorize the review text, and for that we need a vocabulary and a vectorizer.\nWe will start by creating a vocabulary and vectorizer class.\n\n\nThe core process of building a vocabulary includes scanning all the text, breaking it into words, and storing unique words. In addition to storing, vocabulary also provides the functionality of mapping between words (or tokens) and their respective indices (i.e., where those words are stored).\nThe following class offers those functionalities. There is a possibility that during the vectorization process, we are likely to have words that are not available in the vocabulary. To handle those cases, we can store some special words representing all unavailable words, e.g., ‚Äò&lt;UNK&gt;‚Äô.\n\nclass Vocabulary(object):\n    def __init__(self, token_to_idx = None, add_unk=True, unk_token='&lt;UNK&gt;'):\n        \"\"\"\n        params:\n            token_to_idx (dict): mapping from token to index\n            add_unk (bool): flag to add a special token to the vocabulary for unknowns tokens\n            unk_token (str): Token used as special token\n        \n        \"\"\"\n        if token_to_idx is None:\n            token_to_idx ={}\n        \n        self._token_to_idx = token_to_idx\n        self._idx_to_token = {idx:token for token,idx in token_to_idx}\n        \n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        \n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token)\n            \n    def to_serialize(self):\n        \"\"\" function to serialize the content of vocabulary\n        \"\"\"\n        return {'idx_to_token':self._idx_to_token,\n               'add_unk':self._add_unk,\n               'unk_token':self._unk_token}\n    \n    @classmethod\n    def from_serializable(cls,contents):\n        \"\"\"\n        class function to create a vocabulary from serialized content\n        \"\"\"\n        return cls(**contents)\n    \n    def add_token(self,token):\n        \"\"\"\n        Add token to the vocabulary\n        \n        params:\n            token (str): token to add to the vocabulary\n            \n        returns:\n            idx (int): index of token\n        \n        \"\"\"\n        if token in self._token_to_idx:\n            return self._token_to_idx[token]\n        else:\n            idx = len(self)\n            self._token_to_idx[token] = idx\n            self._idx_to_token[idx] = token\n        return idx\n    \n    \n    def lookup_idx(self,idx):\n        \"\"\"\n        Lookup vocabulary to fetch  token at idx\n        \n        params:\n            idx(int) : index of token to be fetched\n            \n        returns:\n            token (str): token stored at idx\n        \"\"\"\n        if idx not in self._idx_to_token:\n            raise KeyError(\"Vocabulary does not have token with specified index:\"%idx)\n        return self._idx_to_token[idx]\n    \n\n    def lookup_token(self,token):\n        \"\"\"\n        Lookup vocabulary to fetch index of a token\n        \n        params:\n            token(str): token to lookup\n            \n        returns:\n            idx (int): index of token\n        \"\"\"\n        \n        if token not in self._token_to_idx:\n            return self.unk_index\n        else:\n            return self._token_to_idx[token]\n    \n    def __len__(self):\n        return len(self._idx_to_token)\n    \n    \n    def __str__(self):\n        return \"Vocabulary (size = %d)\" % len(self)\n    \n    \n\n\n\nLet‚Äôs see one example of using Vocabulary class for the following text.\ntext = \"\"\"This is a good example of illustrating the use of pytorch for natural language processing. The example shows how to build a vocabulary which is a collection of words and their mapping to their corresponding indices. \"\"\"\n\n# raw text\ntext = \"\"\"This is a good example of illustrating the use of pytorch for natural language processing. The example shows how to build a vocabulary which is a collection of words and their mapping to their corresponding indices. \"\"\"\n\n#preparing a vocabulary object\nvoc = Vocabulary(add_unk=True)\n\n#adding token to the vocabulary\nfor word in text.strip().split(' '):\n    voc.add_token(word)\n\n#printing vocabulary mapping\nprint(voc._token_to_idx)\n\n{'&lt;UNK&gt;': 0, 'This': 1, 'is': 2, 'a': 3, 'good': 4, 'example': 5, 'of': 6, 'illustrating': 7, 'the': 8, 'use': 9, 'pytorch': 10, 'for': 11, 'natural': 12, 'language': 13, 'processing.': 14, 'The': 15, 'shows': 16, 'how': 17, 'to': 18, 'build': 19, 'vocabulary': 20, 'which': 21, 'collection': 22, 'words': 23, 'and': 24, 'their': 25, 'mapping': 26, 'corresponding': 27, 'indices.': 28}\n\n\nAs we can see that each word mapped to an index. We also have one special token ‚Äò&lt;UNK&gt;‚Äô. To see its usage, we will perform a lookup for a word which is not in the vocabulary.\n\n# lookup for the word 'estonia'\nprint(voc.lookup_token('estonia'))\n\n0\n\n\nThe vocabulary simply returned the index of ‚Äò&lt;UNK&gt;‚Äô."
  },
  {
    "objectID": "posts/post-with-code/Sentiment prediction.html#vectorizer",
    "href": "posts/post-with-code/Sentiment prediction.html#vectorizer",
    "title": "Sentiment prediction using Pytorch",
    "section": "",
    "text": "Now, let‚Äôs move to the vectorization process. This is the process where we will transform review text into vectors. These vectors will contain indices of each word in the review text.\nFor example, let‚Äôs say we have a text ‚Äúhow to build‚Äù which we want to vectorize. For that, we need a mapping (token to index) or vocabulary where we have information about indices of tokens.\nIf we use our demo vocabulary from the above example then ‚Äúhow to build‚Äù will be transformed into a vector.\n\n# creating a list of vocabulary size\nvector = [0]* len(voc)\n\n# lookup for each token in the sentence\nfor token in \"how to build\".split(' '):\n    \n    # fetch index of the current token\n    index = voc.lookup_token(token)\n    \n    # set the value at index as 1\n    vector[index] = 1\nprint('Vectorized version:',vector)\n\nVectorized version: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nNow, that we have an idea of what vectorizer does, we will move to create a vectorizer class offering text-to vector transformation functionality.\n\nclass ReviewVectorizer(object):\n    \"\"\"\n    Vectorizer class to transform review text into vectors\n    \"\"\"\n    def __init__(self,review_vocab, rating_vocab):\n        \"\"\"\n        params:\n            review_vocab (Vocabulary): vocabulary object for review text\n            rating_vocab (Vocabulary): vocabulary obejct for rating \n        \"\"\"\n        self.review_vocab = review_vocab\n        self.rating_vocab = rating_vocab \n        \n    def vectorize(self,text):\n        \"\"\"\n        perform vectorization of given text\n        \n        params:\n            text (str): review text to transform into vector\n            \n        returns:\n            one_hot (array): returns one-hot encoding of text\n        \"\"\"\n        one_hot = np.zeros(len(self.review_vocab),dtype=np.float32)\n        \n        # iterate over each word in the  text\n        for word in text.strip().split():\n            # avoid if the word is a punctuation\n            if word not in string.punctuation:\n                # fetching index of the word \n                idx = self.review_vocab.lookup_token(word)\n                \n                # setting 1 at idx index\n                one_hot[idx] = 1\n                \n        return one_hot\n        \n    @classmethod\n    def from_dataframe(cls,review_df,cutoff=25):\n        \"\"\"\n        This function builds vocabulary for review text and rating.\n        \n        params:\n            review_df (pandas.DataFrame): dataframe containing yelp dataset\n            cutoff (int): a threshold to store words into vocabulary\n            \n        returns:\n            ReviewVectorizer object\n        \"\"\"\n        review_vocab = Vocabulary(add_unk=True)\n        rating_vocab = Vocabulary(add_unk=False)\n        \n        # adding all unique rating to the rating_vocubulary\n        for rating in review_df['rating'].unique():\n            rating_vocab.add_token(rating)\n            \n        word_count = {}\n        \n        # counting frequency of each word which appeared in the review text\n        for review in review_df.review:\n            for word in review.strip().split(' '):\n                if word not in string.punctuation:\n                    if word in word_count.keys():\n                        word_count[word] += 1\n                    else:\n                        word_count[word] = 1\n        \n        # adding tokens from review text to the review vocabulary\n        for word,count in word_count.items():\n            if count &gt; cutoff:\n                review_vocab.add_token(word)\n        \n        return cls(review_vocab,rating_vocab)\n    \n    @classmethod\n    def from_serializable(cls, contents):\n        \"\"\"\n        class function to create ReviewVectorizer from serialzed contents\n        \n        params:\n            contents(dict): a dictionary containing contents for review and rating vacabulary\n        \n        returns:\n            ReviewVectorizer object\n        \"\"\"\n        \n        return cls(review_vocab = Vocabulary.from_serialiazable(contents['reivew_vocab']),\n                  rating_voca = Vocabulary.from_serializable(contents['rating_vocab']))\n    \n    def to_serializable(self):\n        \"\"\"\n        To serialize vocabularies \n        \n        returns:\n            contents (dict): contents of review and rating vocabularies\n        \n        \"\"\"\n        return {'review_vocab':self.review_vocab.to_serializable(),\n               'rating_vocab':self.rating_vocab.to_serializable()}\n\nNow, we have vocabulary and vectorizer classes ready. Next, we need to create a dataset class inherting pytorch‚Äôs Dataset class. This class enables uses of pytorch functionality with out dataset.\n\n\nBefore training a machine learning model, usually there are some preprocessing tasks, e.g., vectorizing the text, converting images to tensors, etc. These tasks are generally performed separately from the model training. PyTorch offers a systematic way to abstract those tasks. Here, we will see the use of two classes ‚ÄòDataset‚Äô and ‚ÄòDataLoader‚Äô.\nThe first class is related to reading data files, performing some preprocessing tasks, and fetching training instances. While the second class offers a wrapper around the dataset class to fetch data in batches for training.\nLet‚Äôs dive into the code.\n\n\nclass YelpDataset(Dataset):\n    \"\"\"\n    Dataset class utilizing pytorch Dataset template\n    \n    \"\"\"\n    def __init__(self, review_df, vectorizer):\n        \"\"\"\n        review_df (pandas.DataFrame): dataframe containing yelp data records\n        vectorizer (ReviewVectorizer): ReviewVectorizer object\n        \"\"\"\n        self.review_df = review_df\n        self._vectorizer =  vectorizer \n        \n        self.train_df = self.review_df[self.review_df.split=='train']\n        self.train_size = self.train_df.shape[0]\n        \n        self.val_df = self.review_df[self.review_df.split=='val']\n        self.val_size = self.val_df.shape[0]\n        \n        self.test_df = self.review_df[self.review_df.split=='test']\n        self.test_size = self.test_df.shape[0]\n        \n        self._lookup_dict = {'train':(self.train_df,self.train_size),\n                             'val':(self.val_df,self.val_size),\n                             'test':(self.test_df,self.test_size)}\n        \n        self.set_split('train')\n        \n    def get_vectorizer(self):\n        return self._vectorizer\n        \n    @classmethod\n    def load_dataset_and_make_vectorizer(cls,review_csv):\n        \"\"\"class function to load dataset and initialize the vectorizer\n        \n        params: \n            review_csv (str): file_name\n        \n        returns:\n            ReviewDataset object\n        \"\"\"\n        \n        review_df = pd.read_csv(review_csv)\n        train_review_df = review_df[review_df.split=='train']\n        \n        return cls(review_df,ReviewVectorizer.from_dataframe(train_review_df))\n        \n    def set_split(self,split='train'):\n        \"\"\"\n        function to set the current active dataset\n        \n        params:\n            split (str): specify part of dataset to be used\n        \n        \"\"\"\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n        \n    def __len__(self):\n        return self._target_size\n        \n    def __getitem__(self,idx):\n        \"\"\"\n        function to fetch record at index idx\n        \n        params:\n            idx (int): index of record to be fetched\n        \"\"\"\n        row = self._target_df.iloc[idx]\n        review_vector = self._vectorizer.vectorize(row.review)\n        review_rating = self._vectorizer.rating_vocab.lookup_token(row.rating)\n        return {'x_data':review_vector,\n               'y_target':review_rating}\n    \n    def get_num_batches(self,batch_size):\n        return self._target_size//batch_size\n\nTo create a custom ‚ÄòDataset‚Äô class, two functions must be defined, i.e., len, getitem. The first function returns the length of the dataset and the second function fetches the training instance at a given index.\nIn our YelpDataset class, we have added some more functions as well to simplify the loading of training, validation, and test sets.\n\n# checking the dataset\nyelpDB = YelpDataset.load_dataset_and_make_vectorizer('./yelp/reviews_with_splits_lite.csv')\n\n\nfor split in ['train','test','val']:\n    yelpDB.set_split(split)\n    print(f'Split:{split}     Size:{len(yelpDB)}')\n\nSplit:train     Size:39200\nSplit:test     Size:8400\nSplit:val     Size:8400\n\n\nWe have our dataset ready. We can now make use of DataLoader from Pytorch to get data instanes in batches.\n\n\n\nThe following function creates a dataloader for our Yelp dataset. This dataloader return data in batches.\n\nimport string\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\ndef generate_batches(dataset, batch_size, shuffle=True,drop_last=True):\n    \"\"\"\n    function to get data batches\n    @ This code is targetted for cpu machines.\n    \n    params:\n        dataset (YelpDataset): YelpDataset object\n        batch_size (int): data batch size\n        shuffle (bool): Whether to shuffle data\n        drop_last (bool): flag to drop the last batch if it is less than the batch size\n        \n    \"\"\"\n    \n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n    for data_dict in dataloader:\n        out_data_dict = {}\n        \n        for name,tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name]\n        yield out_data_dict\n\nLet‚Äôs see now how one batch look like.\n\nbatch = next(iter(generate_batches(yelpDB,batch_size=100)))\nprint('Size:',batch['x_data'].size())\nprint('Batch example:',batch)\n\nSize: torch.Size([100, 7356])\nBatch example: {'x_data': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n        [1., 0., 1.,  ..., 0., 0., 0.],\n        [1., 1., 1.,  ..., 0., 0., 0.],\n        ...,\n        [1., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.],\n        [1., 1., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n        0, 1, 0, 1])}\n\n\n\n\n\nNow, we will move to build our neural network to classify review text into positive or negative categories of rating. To build a custom neural network, the ‚ÄòModule‚Äô class needs to be inherited. The following class does that.\nNext, during the initialization (i.e., inside the init function) neural network architecture is specified. In our case, we have having a single Linear layer.\nFinally, in the forward function the computation is performed. In the following class, we are simply performing linear computation of the form \\(W*X\\) where \\(W\\) represent weights and \\(X\\) represent input data. On the computed result, the sigmoid function is applied.\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ReviewClassifier(nn.Module):\n    \"\"\"\n    ReviewClassifier class\n    \"\"\"\n    def __init__(self,input_size):\n        \"\"\"\n        Initialize the classifier\n        params:\n            input_size (int): number of features\n        \"\"\"\n        super(ReviewClassifier,self).__init__()\n        self.fc1 = nn.Linear(in_features = input_size,\n                             out_features = 1)\n        \n    def forward(self,inputs,apply_sigmoid = False):\n        \"\"\"\n        function performing forward pass\n        params:\n            inputs (tensor): input vectors\n            apply_sigmoid (bool): flag whether to apply sigmoid function or not\n            \n        returns:\n            y_out (tensors): shape (batch_size,)\n        \"\"\"\n        y_out = self.fc1(inputs).squeeze()\n        if apply_sigmoid:\n            y_out = F.sigmoid(y_out)\n        return y_out\n\n\n\n\nWe will load our database which will create a vocabulary and vectorizer. We then get the vectorizer and initialize our classifier.\n\n# loading the dataset\nyelpDB = YelpDataset.load_dataset_and_make_vectorizer('./yelp/reviews_with_splits_lite.csv')\n\n# getting the vectorizer\nvectorizer = yelpDB.get_vectorizer()\n\n# initializing the classifier\nclassifier = ReviewClassifier(len(vectorizer.review_vocab))\n\n\ndef compute_accuracy(y_pred,y_target):\n    \"\"\"\n    function to compute accuracy metrics using logits\n    \"\"\"\n    # applying sigmoid function and generating predicted class indices based on .5 threshold\n    y_pred_indices = (torch.sigmoid(y_pred) &gt; 0.5).long()\n\n    # counting correctly predicted instances\n    n_correct = (y_pred_indices == y_target).sum().item()\n    \n    return n_correct / len(y_pred_indices) * 100\n\nNow, we will move to the most fun part of building our classifier, i.e., training the neural network with the yelp dataset. During the training, we iterate over several epochs. During each epoch, training occurs on the complete training dataset.\nFor each epoch, we start by setting the gradients of the optimizer as zero. Then, we perform a forward pass where computation is performed on the input data. The output of the classifier is used to compute the loss which is then used to make a backward pass. Finally, the weight parameters are updated. We have also included code to compute running loss and accuracy for training and validation both.\n\nDuring validation and testing, we only perform the forward pass.\n\n\nfrom torch import optim\n\n# for reproducibility\nnp.random.seed(3223)\ntorch.manual_seed(3223)\n\n# loss function\nloss_func = nn.BCEWithLogitsLoss()\n\n# optimizer\noptimizer = optim.Adam(classifier.parameters(),lr=.0001)\n\nnum_epochs = 20\nbatch_size = 100\n\n# iterate over number of epochs\nfor epoch in range(num_epochs):\n    \n    # set the split to use training data\n    yelpDB.set_split('train')\n    \n    batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n    \n    train_running_loss = 0.0\n    train_running_acc = 0.0\n    \n    classifier.train()\n\n    for batch_index, batch_dict in enumerate(batch_generator):\n        \n        # zeros the optimizer gradients\n        optimizer.zero_grad()\n        \n        # forward pass\n        y_pred = classifier(batch_dict['x_data'].float())\n        \n        # computing loss\n        loss = loss_func(y_pred,batch_dict['y_target'].float())\n        \n        loss_t = loss.item()\n        \n        train_running_loss += (loss_t - train_running_loss)/ (batch_index + 1)\n        \n        acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n        \n        train_running_acc += (acc_t - train_running_acc) / (batch_index + 1)\n        \n        # backward pass\n        loss.backward()\n        \n        # updating the weight parameters\n        optimizer.step()\n\n    # setting the split to use validation set\n    yelpDB.set_split('val')\n    \n    # batch generator\n    batch_generator = generate_batches(yelpDB,batch_size=batch_size)\n    val_running_loss = 0.0\n    val_running_acc = 0.0\n    classifier.eval()\n    for batch_index, batch_dict in enumerate(batch_generator):\n        \n        # output of neural network\n        y_pred = classifier(batch_dict['x_data'].float())\n        \n        # coputing loss\n        loss = loss_func(y_pred,batch_dict['y_target'].float())\n        \n        loss_t = loss.item()\n        \n        val_running_loss += (loss_t - val_running_loss)/ (batch_index + 1)\n        \n        acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n        val_running_acc += (acc_t - val_running_acc) / (batch_index + 1)\n        \n    print('\\nEpoch :{}'.format(epoch))\n    print('  Training   ==&gt; Loss: {:.2f} Accuracy: {:.2f}'.format(train_running_loss,train_running_acc))\n    print('  Validation ==&gt; Loss: {:.2f} Accuracy: {:.2f}'.format(val_running_loss,val_running_acc))\n\n    \n\n\nEpoch :0\n  Training   ==&gt; Loss: 0.64 Accuracy: 72.57\n  Validation ==&gt; Loss: 0.60 Accuracy: 82.32\n\nEpoch :1\n  Training   ==&gt; Loss: 0.57 Accuracy: 83.34\n  Validation ==&gt; Loss: 0.54 Accuracy: 84.48\n\nEpoch :2\n  Training   ==&gt; Loss: 0.52 Accuracy: 84.79\n  Validation ==&gt; Loss: 0.50 Accuracy: 85.15\n\nEpoch :3\n  Training   ==&gt; Loss: 0.48 Accuracy: 85.93\n  Validation ==&gt; Loss: 0.46 Accuracy: 85.92\n\nEpoch :4\n  Training   ==&gt; Loss: 0.45 Accuracy: 86.85\n  Validation ==&gt; Loss: 0.44 Accuracy: 86.58\n\nEpoch :5\n  Training   ==&gt; Loss: 0.42 Accuracy: 87.57\n  Validation ==&gt; Loss: 0.42 Accuracy: 87.19\n\nEpoch :6\n  Training   ==&gt; Loss: 0.40 Accuracy: 88.26\n  Validation ==&gt; Loss: 0.40 Accuracy: 87.82\n\nEpoch :7\n  Training   ==&gt; Loss: 0.38 Accuracy: 88.91\n  Validation ==&gt; Loss: 0.38 Accuracy: 88.40\n\nEpoch :8\n  Training   ==&gt; Loss: 0.37 Accuracy: 89.40\n  Validation ==&gt; Loss: 0.37 Accuracy: 88.93\n\nEpoch :9\n  Training   ==&gt; Loss: 0.35 Accuracy: 89.82\n  Validation ==&gt; Loss: 0.35 Accuracy: 89.24\n\nEpoch :10\n  Training   ==&gt; Loss: 0.34 Accuracy: 90.19\n  Validation ==&gt; Loss: 0.34 Accuracy: 89.49\n\nEpoch :11\n  Training   ==&gt; Loss: 0.33 Accuracy: 90.47\n  Validation ==&gt; Loss: 0.33 Accuracy: 89.83\n\nEpoch :12\n  Training   ==&gt; Loss: 0.32 Accuracy: 90.78\n  Validation ==&gt; Loss: 0.32 Accuracy: 90.11\n\nEpoch :13\n  Training   ==&gt; Loss: 0.31 Accuracy: 91.01\n  Validation ==&gt; Loss: 0.31 Accuracy: 90.29\n\nEpoch :14\n  Training   ==&gt; Loss: 0.30 Accuracy: 91.22\n  Validation ==&gt; Loss: 0.31 Accuracy: 90.37\n\nEpoch :15\n  Training   ==&gt; Loss: 0.29 Accuracy: 91.44\n  Validation ==&gt; Loss: 0.30 Accuracy: 90.48\n\nEpoch :16\n  Training   ==&gt; Loss: 0.28 Accuracy: 91.62\n  Validation ==&gt; Loss: 0.29 Accuracy: 90.63\n\nEpoch :17\n  Training   ==&gt; Loss: 0.28 Accuracy: 91.72\n  Validation ==&gt; Loss: 0.29 Accuracy: 90.80\n\nEpoch :18\n  Training   ==&gt; Loss: 0.27 Accuracy: 91.87\n  Validation ==&gt; Loss: 0.28 Accuracy: 90.83\n\nEpoch :19\n  Training   ==&gt; Loss: 0.27 Accuracy: 92.01\n  Validation ==&gt; Loss: 0.28 Accuracy: 90.93"
  },
  {
    "objectID": "posts/post-with-code/Sentiment prediction.html#evaluation-on-test-data",
    "href": "posts/post-with-code/Sentiment prediction.html#evaluation-on-test-data",
    "title": "Sentiment prediction using Pytorch",
    "section": "",
    "text": "Now, we will see how well our classfier performs on test set.\n\nyelpDB.set_split('test')\n    \nbatch_generator = generate_batches(yelpDB,batch_size=batch_size)\n    \ntest_running_loss = 0.0\ntest_running_acc = 0.0\n    \nclassifier.eval()\n\n    \nfor batch_index, batch_dict in enumerate(batch_generator):\n    y_pred = classifier(batch_dict['x_data'].float())\n    loss = loss_func(y_pred,batch_dict['y_target'].float())\n    loss_t = loss.item()\n    test_running_loss += (loss_t - test_running_loss)/ (batch_index + 1)\n    acc_t = compute_accuracy(y_pred,batch_dict['y_target'])\n    test_running_acc += (acc_t - test_running_acc) / (batch_index + 1)\n    \n\nprint('  Test   ==&gt; Loss: {:.2f} Accuracy: {:.2f}'.format(test_running_loss,test_running_acc))\n   \n\n  Test   ==&gt; Loss: 0.29 Accuracy: 90.33"
  },
  {
    "objectID": "posts/post-with-code/llama.html",
    "href": "posts/post-with-code/llama.html",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "This post shares the step-by-step process of running the LLAMA2 model locally on MAC. The post is a reflection of my learning of Large Language Models and their practical applications.\n Image by Freepik\nThis post assumes your system already has Python installed.\n\n\n\n\ngit clone https://github.com/ggerganov/llama.cpp.git\nYou can create a virtual environment for setting up llama. I have used here conda commant to create a new environment for my setup.\nconda create -n llama\nconda activate llama\nNext, go to the repository and run the make command.\ncd llama.cpp\nmake\nInstall python packages\npip3 install llama-cpp-python\n\n\n\nWe need llama2 model files. To access these files, a request has to be made by filling out the form given¬†here. On submission, you will get an email providing instructions to download llama2 models.\n\n\n\n\n\n\nTip\n\n\n\nThe email will have an URL which is asked while downloading the models.\n\n\n\n\n\nNow, we have llama2 models and installed llama CPP binding for Python. We will now convert the downloaded model files.\nThe first step in converting the model file is to run the following command while being in the llama.cpp directory. To run this command, we need the path of the directory containing the downloaded models.\npython3 convert.py &lt;directory_containing_llama_model&gt;\nThis command will generate a file with the name ggml-model-f16.gguf and save it in the repository of llama_model which has the downloaded models.\nThe second step will run the quantize command.\n./quantize &lt;directory_containing_llama_model&gt;/ggml-model-f16.gguf &lt;directory_containing_llama_model&gt;/ggml-model-q4_0.gguf q4_0\n\n\n\n\n\n\nImportant\n\n\n\nYou can also use the already converted file available here if you get any error while running above two steps.\n\n\n\n\n\nLangChain makes it easier to develop applications using language models. You can learn about it more here\npip3 install langchain \n\n\n\n\nNow, we will see the fun part of asking a question to llama2 and getting its answer.\nThe following python script has been used from the tutorial available here.\nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nllm = LlamaCpp(model_path='../ggml-model-q4_0.gguf',\n    temperature=0.0,\n    top_p=1,\n    n_ctx=6000,\n    callback_manager=callback_manager, \n    verbose=True,\n)\n\nquestion = \"who is mr. narendra modi?\"\nanswer = llm(question)\n\nprint('Q:',question)\nprint('A:',answer)\nOutput:\nMr. Narendra Modi is the current Prime Minister of India, serving since May 2014. He is known ...\n\n\n\n\nhttps://medium.com/@karankakwani/build-and-run-llama2-llm-locally-a3b393c1570e\nhttps://github.com/facebookresearch/llama-recipes/blob/main/demo_apps/HelloLlamaLocal.ipynb"
  },
  {
    "objectID": "posts/post-with-code/llama.html#installing-required-libraries",
    "href": "posts/post-with-code/llama.html#installing-required-libraries",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "git clone https://github.com/ggerganov/llama.cpp.git\nYou can create a virtual environment for setting up llama. I have used here conda commant to create a new environment for my setup.\nconda create -n llama\nconda activate llama\nNext, go to the repository and run the make command.\ncd llama.cpp\nmake\nInstall python packages\npip3 install llama-cpp-python\n\n\n\nWe need llama2 model files. To access these files, a request has to be made by filling out the form given¬†here. On submission, you will get an email providing instructions to download llama2 models.\n\n\n\n\n\n\nTip\n\n\n\nThe email will have an URL which is asked while downloading the models.\n\n\n\n\n\nNow, we have llama2 models and installed llama CPP binding for Python. We will now convert the downloaded model files.\nThe first step in converting the model file is to run the following command while being in the llama.cpp directory. To run this command, we need the path of the directory containing the downloaded models.\npython3 convert.py &lt;directory_containing_llama_model&gt;\nThis command will generate a file with the name ggml-model-f16.gguf and save it in the repository of llama_model which has the downloaded models.\nThe second step will run the quantize command.\n./quantize &lt;directory_containing_llama_model&gt;/ggml-model-f16.gguf &lt;directory_containing_llama_model&gt;/ggml-model-q4_0.gguf q4_0\n\n\n\n\n\n\nImportant\n\n\n\nYou can also use the already converted file available here if you get any error while running above two steps.\n\n\n\n\n\nLangChain makes it easier to develop applications using language models. You can learn about it more here\npip3 install langchain"
  },
  {
    "objectID": "posts/post-with-code/llama.html#building-applications-on-the-top-of-a-language-model",
    "href": "posts/post-with-code/llama.html#building-applications-on-the-top-of-a-language-model",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "Now, we will see the fun part of asking a question to llama2 and getting its answer.\nThe following python script has been used from the tutorial available here.\nfrom langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nllm = LlamaCpp(model_path='../ggml-model-q4_0.gguf',\n    temperature=0.0,\n    top_p=1,\n    n_ctx=6000,\n    callback_manager=callback_manager, \n    verbose=True,\n)\n\nquestion = \"who is mr. narendra modi?\"\nanswer = llm(question)\n\nprint('Q:',question)\nprint('A:',answer)\nOutput:\nMr. Narendra Modi is the current Prime Minister of India, serving since May 2014. He is known ..."
  },
  {
    "objectID": "posts/post-with-code/llama.html#references",
    "href": "posts/post-with-code/llama.html#references",
    "title": "Hands-on experience with llama2 with Python: Building a simple Q&A app",
    "section": "",
    "text": "https://medium.com/@karankakwani/build-and-run-llama2-llm-locally-a3b393c1570e\nhttps://github.com/facebookresearch/llama-recipes/blob/main/demo_apps/HelloLlamaLocal.ipynb"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html",
    "href": "posts/post-with-code/Bike sharing map visualization.html",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "This tutorial will show you how to work with geospatial data using Python with library geoPandas. For this exercise, I have used Boston Bike Sharing dataset which is available here. I have recently started learning and using Map library with Python and this tutorial is the reflection of things I have learned so far. I hope it will be helpful for those learning the same first time.\n\n\nWe start first loading our dataset. There are two CSV files, hubway_trips.csv and hubway_station.csv. The first data file contains information all bike trips (e.g., bike number, start and end time of the trip, start and end station of the trip, user information, etc). The second data file hubway_station.csv contains information about the bike stations (e.g., longitude, lattitude, municipality).\n\nimport pandas as pd\n\n# Bike trip data\ntrips = pd.read_csv('hubway_trips.csv')\ntrips.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n\n\n\n\n\n\n\n\n# Bike station data [seperator in this file is ';']\nstations = pd.read_csv('hubway_stations.csv',sep=';')\nstations.head()\n\n\n\n\n\n\n\n\nid\nterminal\nstation\nmunicipal\nlat\nlng\nstatus\n\n\n\n\n0\n3\nB32006\nColleges of the Fenway\nBoston\n42.340021\n-71.100812\nExisting\n\n\n1\n4\nC32000\nTremont St. at Berkeley St.\nBoston\n42.345392\n-71.069616\nExisting\n\n\n2\n5\nB32012\nNortheastern U / North Parking Lot\nBoston\n42.341814\n-71.090179\nExisting\n\n\n3\n6\nD32000\nCambridge St. at Joy St.\nBoston\n42.361285\n-71.065140\nExisting\n\n\n4\n7\nA32000\nFan Pier\nBoston\n42.353412\n-71.044624\nExisting\n\n\n\n\n\n\n\n\n\n\nWe will first join the data to have a single file with lattitude and longitude information for start station of bike trip. For that we will perform join operation on start_statn in trips and id in stations.\n\n# we exclude status column because it is present in both data file and will cause an error on join operation.\nstations_non_status = stations[['id','lat','lng','station','municipal']]\n\n# combined data with start station geo spatial information\ntrips_stations = trips.join(stations_non_status.set_index('id'),on='strt_statn')\n\ntrips_stations = trips_stations.loc[trips_stations['municipal'] == 'Boston',:]\n\n\ntrips_stations.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\nlat\nlng\nstation\nmunicipal\ngeometry\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n\n\n\n\n\n\n\n\nNow we will move towards setting up our dataset to have geospatial data required by GeoPandas library. GeoPandas is a python library with extends capability of Pandas by including GeoSpatial data processing and plotting functionality.\nFirst install GeoPandas library if you don‚Äôt have it on your computer.\n\n# Installing Geopandas\n#! pip3 install geopandas\n\n\nimport geopandas as gpd\nfrom shapely import Point, Polygon\n\n\ngeo_column = [Point(lng,lat) for lng, lat in zip(trips_stations['lng'],trips_stations['lat'])]\n\nWhat we did in above code was created a list with a Point object for each record in our joined data file. This Point object will provide geospatial data information to GeoPandas to work with.\nNow we will create our GeoPandas dataframe which will have our data with geospatial information.\n\ncrs={'init':'epsg:4326'}\ngdf = gpd.GeoDataFrame(trips_stations,crs=crs,geometry=geo_column)\n\nNow, we created a geopandas dataframe which have all our records from trips_stations dataframe and all those records are associated with a geometric point (e.g., geospatial data).\nEach geopandas dataframe requires one geometry column which has information of geospatial data.\n\n\n\n\ngdf.plot('strt_statn')\n\n\n\n\n\n\n\n\nThe above plot does not show Boston‚Äôs map. In order to do that we would need Map files for Boston.\n\nboston = gpd.read_file('./City_of_Boston_Boundary/')\nboston = boston.to_crs({'init': 'epsg:4326'})\nboston.plot()\n\n/Users/htk/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n\n\n\n\n\n\nNow we will plot the data of start station of bike trip on the map of Boston.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,8))\nboston.plot(ax=ax)\ngdf.plot(ax=ax,color='red',marker='+',alpha=.5)"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#loading-the-dataset",
    "href": "posts/post-with-code/Bike sharing map visualization.html#loading-the-dataset",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "We start first loading our dataset. There are two CSV files, hubway_trips.csv and hubway_station.csv. The first data file contains information all bike trips (e.g., bike number, start and end time of the trip, start and end station of the trip, user information, etc). The second data file hubway_station.csv contains information about the bike stations (e.g., longitude, lattitude, municipality).\n\nimport pandas as pd\n\n# Bike trip data\ntrips = pd.read_csv('hubway_trips.csv')\ntrips.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n\n\n\n\n\n\n\n\n# Bike station data [seperator in this file is ';']\nstations = pd.read_csv('hubway_stations.csv',sep=';')\nstations.head()\n\n\n\n\n\n\n\n\nid\nterminal\nstation\nmunicipal\nlat\nlng\nstatus\n\n\n\n\n0\n3\nB32006\nColleges of the Fenway\nBoston\n42.340021\n-71.100812\nExisting\n\n\n1\n4\nC32000\nTremont St. at Berkeley St.\nBoston\n42.345392\n-71.069616\nExisting\n\n\n2\n5\nB32012\nNortheastern U / North Parking Lot\nBoston\n42.341814\n-71.090179\nExisting\n\n\n3\n6\nD32000\nCambridge St. at Joy St.\nBoston\n42.361285\n-71.065140\nExisting\n\n\n4\n7\nA32000\nFan Pier\nBoston\n42.353412\n-71.044624\nExisting"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#joining-the-data",
    "href": "posts/post-with-code/Bike sharing map visualization.html#joining-the-data",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "We will first join the data to have a single file with lattitude and longitude information for start station of bike trip. For that we will perform join operation on start_statn in trips and id in stations.\n\n# we exclude status column because it is present in both data file and will cause an error on join operation.\nstations_non_status = stations[['id','lat','lng','station','municipal']]\n\n# combined data with start station geo spatial information\ntrips_stations = trips.join(stations_non_status.set_index('id'),on='strt_statn')\n\ntrips_stations = trips_stations.loc[trips_stations['municipal'] == 'Boston',:]\n\n\ntrips_stations.head()\n\n\n\n\n\n\n\n\nseq_id\nhubway_id\nstatus\nduration\nstart_date\nstrt_statn\nend_date\nend_statn\nbike_nr\nsubsc_type\nzip_code\nbirth_date\ngender\nlat\nlng\nstation\nmunicipal\ngeometry\n\n\n\n\n0\n1\n8\nClosed\n9\n7/28/2011 10:12:00\n23.0\n7/28/2011 10:12:00\n23.0\nB00468\nRegistered\n'97217\n1976.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n1\n2\n9\nClosed\n220\n7/28/2011 10:21:00\n23.0\n7/28/2011 10:25:00\n23.0\nB00554\nRegistered\n'02215\n1966.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n2\n3\n10\nClosed\n56\n7/28/2011 10:33:00\n23.0\n7/28/2011 10:34:00\n23.0\nB00456\nRegistered\n'02108\n1943.0\nMale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n3\n4\n11\nClosed\n64\n7/28/2011 10:35:00\n23.0\n7/28/2011 10:36:00\n23.0\nB00554\nRegistered\n'02116\n1981.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)\n\n\n4\n5\n12\nClosed\n12\n7/28/2011 10:37:00\n23.0\n7/28/2011 10:37:00\n23.0\nB00554\nRegistered\n'97214\n1983.0\nFemale\n42.359677\n-71.059364\nMayor Thomas M. Menino - Government Center\nBoston\nPOINT (-71.05936 42.35968)"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#working-with-geopandas-library",
    "href": "posts/post-with-code/Bike sharing map visualization.html#working-with-geopandas-library",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "Now we will move towards setting up our dataset to have geospatial data required by GeoPandas library. GeoPandas is a python library with extends capability of Pandas by including GeoSpatial data processing and plotting functionality.\nFirst install GeoPandas library if you don‚Äôt have it on your computer.\n\n# Installing Geopandas\n#! pip3 install geopandas\n\n\nimport geopandas as gpd\nfrom shapely import Point, Polygon\n\n\ngeo_column = [Point(lng,lat) for lng, lat in zip(trips_stations['lng'],trips_stations['lat'])]\n\nWhat we did in above code was created a list with a Point object for each record in our joined data file. This Point object will provide geospatial data information to GeoPandas to work with.\nNow we will create our GeoPandas dataframe which will have our data with geospatial information.\n\ncrs={'init':'epsg:4326'}\ngdf = gpd.GeoDataFrame(trips_stations,crs=crs,geometry=geo_column)\n\nNow, we created a geopandas dataframe which have all our records from trips_stations dataframe and all those records are associated with a geometric point (e.g., geospatial data).\nEach geopandas dataframe requires one geometry column which has information of geospatial data."
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#plot-geospatial-data",
    "href": "posts/post-with-code/Bike sharing map visualization.html#plot-geospatial-data",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "gdf.plot('strt_statn')\n\n\n\n\n\n\n\n\nThe above plot does not show Boston‚Äôs map. In order to do that we would need Map files for Boston.\n\nboston = gpd.read_file('./City_of_Boston_Boundary/')\nboston = boston.to_crs({'init': 'epsg:4326'})\nboston.plot()\n\n/Users/htk/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)"
  },
  {
    "objectID": "posts/post-with-code/Bike sharing map visualization.html#plotting-geospatial-data-over-bostons-map",
    "href": "posts/post-with-code/Bike sharing map visualization.html#plotting-geospatial-data-over-bostons-map",
    "title": "Location data visualization on Map using Python",
    "section": "",
    "text": "Now we will plot the data of start station of bike trip on the map of Boston.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,8))\nboston.plot(ax=ax)\ngdf.plot(ax=ax,color='red',marker='+',alpha=.5)"
  },
  {
    "objectID": "posts/post-with-code/Snakemake_first.html",
    "href": "posts/post-with-code/Snakemake_first.html",
    "title": "Writing your first Snakemake rule: A quick startup guide for Snakemake",
    "section": "",
    "text": "A workflow is a series of tasks or programs executed in a specific order to achieve a goal. To automate the execution of these tasks, we use a workflow manager. This post introduces and provides a quick startup guide to Snakemake, a widely used workflow management system in bioinformatics.\nüêç Snakemake is a workflow manager that simplifies the creation and execution of workflows. Moreover, it offers robustness and scalability features."
  },
  {
    "objectID": "posts/post-with-code/Snakemake_first.html#a-rule-to-concatenate-two-csv-files",
    "href": "posts/post-with-code/Snakemake_first.html#a-rule-to-concatenate-two-csv-files",
    "title": "Writing your first Snakemake rule: A quick startup guide for Snakemake",
    "section": "A rule to concatenate two CSV files",
    "text": "A rule to concatenate two CSV files\nWe will see how to write a snakemake rule to concatenate two files: one.csv, two.csv.\nfile: one.csv\nName,Age,City\nAlice,25,New York\nBob,30,London\nCharlie,28,Sydney\nDavid,35,Toronto\nEmma,22,Berlin\nfile: two.csv\nName,Age,City\nFrank,27,Paris\nGrace,32,Rome\nHannah,29,Tokyo\nIan,40,Madrid\nJack,23,Dublin\nWe start by writing our first Snakemake rule to concatenate two CSV files. Each Snakemake rule follows a common syntax, which is shown below. We will now break down the rule and explain it in detail\nrule &lt;rule_name&gt;:\n    input:\n        &lt;input_file_1&gt;,\n        &lt;input_file_2&gt;\n    output:\n        &lt;output_file_1&gt;,\n        ...\n        ...\n    run/shell:\n        \"\"\"\n        &lt;commands to execute&gt;\n        \"\"\"    \n\nDeclare your rule name\nThe first line specifies a snakemake rule with specified name. Every rule name must be unique (i.e., it should not conflict with other rule names in your workflow).\nrule concatenate_csv:\n\n\nSpecify your input files\nNext, we will specify the input & output files for the rule. Snakemake uses this information to determine dependencies among rules (e.g., to decide which rule should be executed next).\n    input:\n        'one.csv',\n        'two.csv'\n\n\n\n\n\n\nWarning\n\n\n\nDo not forget comma after each input file when you have multiple input files. Specify your output files\n\n\nWe will now specify our output file, i.e., third.csv.\n\n\nSpecify your output files\nWe will now specify our output file, i.e., third.csv.\n\n\n\n\n\n\nTip\n\n\n\n‚úÖ Snakemake only executes a rule when the output files are not available. In our case, when we run the workflow, Snakemake will automatically decide whether to execute a rule based on the availability of output files. üìÇ\nüîÑ If we execute the workflow a second time, Snakemake will not run the rule again because the output file is already there. üéØ\n\n\n    output:\n        'third.csv'\n\n\nSpecify your log file\nIt is a good practice to have a log file. It comes to very handy to troubleshooting errors when running a workflow with several rules.\n    log:\n        'concatenate.log'\n\n\nSpecify rule logic\nThis is where we will execute our commands to achieve the goal of the rule. We can write here Python code or shell commands. For our case, we need to write a logic to concatenate two csv files. Here, we will illustrates the use of Python and shell both. run/shell\nUse run for python codes and shell for shell commands.\n    run:\n        import pandas as pd\n        first = pd.read_csv('one.csv')\n        second = pd.read_csv('two.csv')\n        \n        third = pd.concat([first,second])\n        third.to_csv('third.csv')\n\n\nComplete Snakefile\nOur final rule will look like the following. First version of Snakefile\nThis is our first version of Snakefile.\n\n\nSnakefile\n\nrule concatenate_csv:\n    input:\n        'one.csv',\n        'two.csv'\n    output:\n        'third.csv',\n    run:\n        import pandas as pd\n        \n        # Load csv files\n        first = pd.read_csv('one.csv')\n        second = pd.read_csv('two.csv')\n            \n        # Concatenate files\n        third = pd.concat([first,second])\n\n        # Save output file\n        third.to_csv('third.csv')    \n\n\n\nDrawbacks of the Rule\n\nError Handling: Any error occurring during the execution of the Python code (e.g., File Not Found) is displayed only in the terminal. It would be better to store these errors in dedicated log files for each rule.\nFlexibility: If we need to run the same workflow for different input files, we must manually modify multiple parts of the workflow, making it less adaptable. üîÑ"
  },
  {
    "objectID": "posts/post-with-code/Snakemake_first.html#second-version-of-snakefile",
    "href": "posts/post-with-code/Snakemake_first.html#second-version-of-snakefile",
    "title": "Writing your first Snakemake rule: A quick startup guide for Snakemake",
    "section": "Second version of Snakefile",
    "text": "Second version of Snakefile\nIn the second version, we will improve the workflow by making the following changes:\n\nModularizing the code ‚Äì We will move the Python code to a separate script file and execute it using shell. Additionally, we will redirect both standard error and standard output to a log file for better error tracking.\nEnhancing flexibility ‚Äì Instead of hardcoding file names, we will store them in a configuration file and import them dynamically. This makes the workflow adaptable to different input files with minimal modifications.\n\n\nSeperate the logic from Snakefile\nWe will now prepare a seperate python script for concatenating two files.\n\n\nconcatenate.py\n\nimport sys\nimport pandas as pd\n\n# Get filenames from command-line arguments\nfile1 = sys.argv[1]\nfile2 = sys.argv[2]\noutput_file = sys.argv[3]\n\n# Load csv files\ndf1 = pd.read_csv(file1)\ndf2 = pd.read_csv(file2)\n\n# Concatenate files\ndf_combined = pd.concat([df1, df2])\ndf_combined.to_csv(output_file, index=False)\n\nprint(f\"Successfully merged {file1} and {file2} into {output_file}\")\n\n\n\nStoring all filenames in a config file\nWe will now write a configuration file which will store all varying information such as input file names, out filename.\n\n\nconfig.yaml\n\nfirst: 'one.csv'\nsecond: 'two.csv'\noutput-file: 'third.csv'\n\n\n\nWorkflow updates\nWe will enhance the workflow with the following changes:\n\nImport config.yaml üìÑ ‚Äì We will extract file names dynamically from a configuration file instead of hardcoding them.\nModify input and output üîÑ ‚Äì The workflow will now use extracted filenames from config.yaml, making it more flexible.\nAdd a log component üìú ‚Äì We will log all execution details for better debugging and tracking.\nExecute concatenate.py via shell üñ•Ô∏è ‚Äì The script will be executed with input and output filenames passed as arguments. ‚û°Ô∏è Additionally, both standard output and standard error will be redirected to the log file using &&gt;{log}.\n\n\n\nSnakefile\n\n# Import config file\nconfigfile: \"config.yaml\"\n    \n# Fetch file names\nfile1 = config['first']\nfile2 = config['second']\nresult = config['output-file']\n\nrule concatenate_csv:\n    input:\n        file1,\n        file2\n    output:\n        result,\n    log:\n        'concatenate.log'\n    shell:\n        \"\"\"\n        python3 concatenate.py {file1} {file2} {result}  &&gt;{log}\n        \"\"\""
  },
  {
    "objectID": "posts/post-with-code/word_embeddings.html",
    "href": "posts/post-with-code/word_embeddings.html",
    "title": "Introduction to word embeddings with hands-on exercises",
    "section": "",
    "text": "A word embedding is a numeric representation of text in natural language processing. The representation is in the form of a vector where each number represents a dimension (or a specific attribute). Let‚Äôs take an example to understand it further.\nConsider the following two sentences\nA likes to drink coffee in the morning\nB likes to drink tea in the morning\nThese two sentences are syntactically and semantically similar. To transform these sentences into vector forms, we can consider attributes like like to drink, morning, etc.\nWe can see that A and B both like to drink some beverages in the morning. Therefore, in our vector representations of these two imaginary persons (i.e., A, B), the values for the attribute like to drink should be close to each other. Similarly, there could be another attribute of time of the day when they like to drink. Such attributes can be decided based on the text data.\nHowever, the manual crafting of such attributes is a very resource-exhaustive process and may take forever to cover a massive amount of text corpora.\nTo solve this problem, machine learning is used to learn these multi-dimensional vector representations also widely known as word embeddings. Word embeddings aim to capture semantic meaning in addition to syntactic information of the text data, and it has been found to a highly successful in solving a variety of NLP tasks, e.g., language modeling, text generation, etc.\nThe word embeddings can be developed from scratch in the context of the task at hand. Additionally, there are some pre-trained word embeddings that are available publicly. These word embeddings include Google Word2Vec, Stanford Glove, Facebook FastText.\nIn this post, we will see how to use these pre-trained word embeddings in Python. We will also build a small application based on the word embeddings to illustrate its use cases.\n\nPre-trained word embeddings\nTo use the pre-trained word embeddings, we will Gensim, a Python library that simplifies working with the aforementioned word embeddings. The library also offers downloading functionality for a number of word embeddings. These word embeddings are the following\n\nimport gensim.downloader\n\nprint(\"\\n\".join(list(gensim.downloader.info()['models'].keys())))\n\nfasttext-wiki-news-subwords-300\nconceptnet-numberbatch-17-06-300\nword2vec-ruscorpora-300\nword2vec-google-news-300\nglove-wiki-gigaword-50\nglove-wiki-gigaword-100\nglove-wiki-gigaword-200\nglove-wiki-gigaword-300\nglove-twitter-25\nglove-twitter-50\nglove-twitter-100\nglove-twitter-200\n__testing_word2vec-matrix-synopsis\n\n\nWe can download any of the aforementioned word embeddings using gensim.downloader. For example, the following code downloads word2vec word embeddings.\n\n# let's download word2vec\nembeds = gensim.downloader.load('word2vec-google-news-300')\n\n#print embeddings for word king\nking_embeddings = embeds['king']\n\nprint('Word embedding size:',len(king_embeddings))\n\nWord embedding size: 300\n\n\nThe above vector contains 300 integers (or 300 dimensions) representations for the word king.\nLet‚Äôs now see how close this word is to another word queen. We can use here similarity function which computes Cosine similarity.\n\nprint('Similarity:',embeds.similarity('king','queen'))\n\nSimilarity: 0.6510957\n\n\n\n\nBuilding an automatic word analogy task solver using word embeddings\nThe analogy task aims to test the relationship between words. The task includes the prediction of a word that has a similar analogy as a given pair of words.\nIt is given in the form of a:b::c:?.\n Here, a and b have some kind of relationship.\n The task is to find a word that has a similar relationship with `c`.\nTo do this task, we will use pre-trained embeddings, i.e., Glove. We can download the Glove embeddings using load function (which we employed in our above example).\n\nembeds = gensim.downloader.load('glove-wiki-gigaword-100')\n\n[==================================================] 100.0% 128.1/128.1MB downloaded\n\n\nWe have our word embeddings downloaded and loaded for use. Next, we will take the user‚Äôs input for a,b,c and predict d. We will follow the following steps\n\nWe will first capture the relationship between a and b. We will do that by performing a subtraction operation on the vectors of words a and b.\nWe will use the difference vector as relationship, and add it to the word embedding of c.\nThe resultant word embedding will then be used to find most similar word embeddings.\nWe will print the first word with the highest similarity measure.\n\n\n# Taking input from the user\na = input('Enter a: ')\nb = input('Enter b: ')\nc = input('Enter c: ')\n\nEnter a: King\nEnter b: Man\nEnter c: Queen\n\n\n\n# Capturing the relationship between `a` and `b`\n\nembed_a = embeds[a.lower()]\nembed_b = embeds[b.lower()]\nembed_c = embeds[c.lower()]\n\nrel_a_b = embed_b - embed_a\n\nembed_d = embed_c + rel_a_b\n\nIn the above code, we first extracted the word embeddings for the words entered by the user. Then, we computed subtraction between word embeddings of b and a. Here, we see the relationship captured through subtraction (i.e., b - a) similar to the relationship between c and d (i.e., d - c).\nWe used the relationship vector and added it to the word embedding of c. This gave us the word embedding for our resultant word.\nFinally, we search through all word embeddings and extract the one with the highest similarity measure. For that, we will use most_similar function from Gensim library.\n\n# finding similar words \npred_d = embeds.similar_by_vector(embed_d, topn=5)\n\nprint(pred_d)\n\n[('woman', 0.8039792776107788), ('man', 0.7791377305984497), ('girl', 0.7349346280097961), ('she', 0.6817952394485474), ('her', 0.6592202186584473)]\n\n\nWe extracted the top 6 words with the highest similarity measure. We will next iterate over this list of words and print the first word which is not in the words entered by the user (i.e., a,b,c).\n\nfor word in pred_d:\n    if word[0] not in [a,b,c]:\n        print(word[0])\n        break\n\nwoman\n\n\nLet‚Äôs put together our code and run it again.\n\n# Taking input from the user\na = input('Enter a: ')\nb = input('Enter b: ')\nc = input('Enter c: ')\n\nprint('\\n\\nGiven task:')\nprint('-'*40)\nprint('{}:{}::{}:?'.format(a,b,c))\nprint('-'*40)\n\n# getting word embeddings for a,b,c\nembed_a = embeds[a.lower()]\nembed_b = embeds[b.lower()]\nembed_c = embeds[c.lower()]\n\n# compute relationship between b and a\nrel_a_b = embed_b - embed_a\n\n# approximate word embedding using the capture relationship\nembed_d = embed_c + rel_a_b\n\n# extract most similar words\npred_d = embeds.similar_by_vector(embed_d, topn=5)\n\nd = ''\n\n# find the most similar words to the computed word embeddings for d\nfor word in pred_d:\n    if word[0] not in [a,b,c]:\n        d= word[0]\n        break\n        \nprint('\\n\\nSolution:')\nprint('='*20)\nprint('{}:{}::{}:{}'.format(a,b,c,d))\n\nEnter a: father\nEnter b: grandfather\nEnter c: mother\n\n\nGiven task:\n----------------------------------------\nfather:grandfather::mother:?\n----------------------------------------\n\n\nSolution:\n====================\nfather:grandfather::mother:grandmother\n\n\n\n\nReferences\n\nhttps://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\nhttps://www.merriam-webster.com/dictionary/orthography\nhttps://machinelearningmastery.com/develop-word-embeddings-python-gensim/ [training your own embedding using gensim]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/Hosting on Render.html",
    "href": "posts/post-with-code/Hosting on Render.html",
    "title": "How to Successfully host your Dash application on Render",
    "section": "",
    "text": "This post will provide detailed instructions on the hosting process of a dash application on Render. Render is a hosting service provider. It offers a free tier that can be used to host Python application projects. It could be a great option for students or developers for their school projects or hobby projects.\nIn this post, we will see the step-by-step process of successfully deploying dash application on Render.\nThe steps are given following.\n\nPrepare your dash application\nPut it on GitHub\nSign up on Render and create a web service\nConfigure your application\nVolla, your application is running online üî•\n\n\n1. Prepare your dash application\nThe first step is to get your dash application ready. For illustration purposes, I am using a minimal dash application available here.\nThe source code is given below.\n\n\napp.py\n\nfrom dash import Dash, html, dcc, callback, Output, Input\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminder_unfiltered.csv')\n\napp = Dash()\n\napp.layout = [\n    html.H1(children='Title of Dash App', style={'textAlign':'center'}),\n    dcc.Dropdown(df.country.unique(), 'Canada', id='dropdown-selection'),\n    dcc.Graph(id='graph-content')\n]\n\n@callback(\n    Output('graph-content', 'figure'),\n    Input('dropdown-selection', 'value')\n)\ndef update_graph(value):\n    dff = df[df.country==value]\n    return px.line(dff, x='year', y='pop')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n\nWe will add the following line after line number 7 (where we create app object).\n\n```python\nserver = app.server\n```\n\nYou can check the updated file here.\nThe next thing you need to do is to make a requirements.txt file. This is the file that contains the dependencies of the application or in other words which Python packages your application is using.\nYou can use the following command to save the list of Python packages used in your system (I would suggest using virtual environments, e.g., venv).\npip3 freeze &gt; requirements.txt\nThe above command will create a new file requirements.txt which lists out all the python packages in your current local dev environment.\nYou need to edit this file and at the end add gunicorn.\n\n\n\n\n\n\nTip\n\n\n\nWe mainly did two things\n\nFirst, we added server = app.server line in the application.\nSecond, we generated requirements.txt file and updated it by adding gunicorn at the end.\n\n\n\n\n\n2. Put the application on GitHub\nOnce your application is ready, push it to GitHub.\nThis is what it looks like after putting your files on GitHub.\n\n\n\n3. Sign-in to Render and create a web-service\nNow, we will move to the Render platform and sign in (You can use Google login). Once you are logged in, you will see a button New+. Click on that button and create a web service.\n\nNext, choose ‚ÄòBuild and deploy from a Git repository‚Äô on the screen (also given below).\n\nOn the next screen, you need to provide url of your Github directory containing code for the dash application. \nOnce you provide your GitHub repository URL, click on the Continue button which will take you to the final step of configuration.\n\n\n4. Configure your application\nIn the configuration phase, we will make a small change in the settings. You will see different settings configurations. What you need to do is changing the start command from its default gunicorn app:app to gunicorn app:server. After editing, don‚Äôt forget to save it.\n\nThat‚Äôs it!!\nIt will take a few seconds. You will see the below screen.\n\nOnce you see the message ==&gt; Your service is live üéâ, that means your application is successfully deployed.\nYou can access the deployed application here: https://render-hosting-fxqr.onrender.com/\nGithub repo:https://github.com/pankajchejara23/render_hosting/tree/main\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/Regular expression.html",
    "href": "posts/post-with-code/Regular expression.html",
    "title": "Introduction to Regular Expressions",
    "section": "",
    "text": "This post will explain the basics of regular expression. Regular expression (regexp in short) is a powerful technique of text searching and text manipulation. If you are planning to make your career in the field of natural language processing then regexp is a must have skill in your skill set.\nExample: Every time you search in your word document using Ctrl+f, regular expression works in the background.\nLet‚Äôs take an example, given below, to understand it further.\n\ntext = \"abc abcd abcde\"\n\nWe want to see if there is a string abc present in our text data. To do that we will use the most basic form of regular expressions which specifies the search query using the actual word or text. For example, we want to search for abc in our text data, thus, we simply specify the term abc as our regular expression.\nThe following code illustrates how to search for abc in our text data. We will use re package.\n\nimport re\n\n# searching for pattern 'abc' in text. use r in the start of the pattern\nresult = re.findall(r'abc',text)\n\nprint(result)\n\n['abc', 'abc', 'abc']\n\n\nWe can see in the result above that there were three occurrences of pattern `abc``. We have now seen the most basic form of using regular expression (i.e., simply using the text).\n\nNow, we will move towards a more advanced form of regular expressions. Let's change our text data the following\n\ntext = \"abc abcd abcde bcd apple ddeffe eef ggh\"\n\nLet‚Äôs now search all the words of three characters [no numbers]. We can only use our first approach of specifying the word itself for searching if we know all the three characters long words in the text data. However, we may not necessarily know about all of them.\nHow to search for three-characters-long words in the text?\nTo answer this we will use a special purpose character . which matches a single occurence of any word character. For our searching, we can simply specify . three times to search every three characters word.\nLet‚Äôs apply it first to see it‚Äôs working.\n\nimport re\n\ntext = \"abc abcd abcde bcd apple ddeffe eef ggh\"\n\nresult = re.findall(r'...',text)\n\nprint(result)\n\n['abc', ' ab', 'cd ', 'abc', 'de ', 'bcd', ' ap', 'ple', ' dd', 'eff', 'e e', 'ef ', 'ggh']\n\n\nWhat went wrong? The results are not what we expected (i.e., all three-characters-long word). The reason is that the text is treated as sequence of characters and here characters are not limited to alphabets and numbers. Blanks are also considered as characters. So when we specify . it matches any single character including blank space.\nTo correct this we will use another special character \\b which matches word boundry (in our case it is blank spaces between words).\nNow, we will change our regular expression to \\b...\\b which will match three-characters-long words which exists independently. Let‚Äôs see now the results.\n\nimport re\n\ntext = \"abc abcd abcde bcd apple ddeffe eef ggh\"\n\nresult = re.findall(r'\\b...\\b',text)\n\nprint(result)\n\n['abc', 'bcd', 'eef', 'ggh']\n\n\nIt worked now as expected.\nSimilar to the special characters we have seen so far, there are other characters as well with special meaning. These characters makes it easier to create patterns for searching. The list of most commonly used special characters is given below.\n\n\n\n\n\n\n\nPattern\nDescription\n\n\n\n\n[abc]\nMatches a single character among a,b,c\n\n\n[^abc]\nMatches a single character except a or b or c\n\n\n[a-z]\nMatches a single character in a-z\n\n\n[^a-z]\nMatches a single character except a-z\n\n\n.\nMatches any single character\n\n\n\\d\nMatches any single digit\n\n\n\\w\nMatches any single word character (i.e., a character between a-z or A-Z or _ or 0-9)\n\n\n\\s\nMatches any white space character (i.e., space, tab, new line)\n\n\n\\b\nMatches word boundary\n\n\n^\nMatches start of string\n\n\n$\nMatches end of string\n\n\na?\nMatches zero or one occurrence of character a\n\n\na+\nMatches one or more occurrences of character a\n\n\na*\nMatches zero or more occurrences of character a\n\n\na{4}\nMatches four occurrences of character a\n\n\na{2,4}\nMatches occurrences of a between 2 to 4\n\n\na{2,}\nMatches either two or more occurrences of a\n\n\n\n\n\nLet‚Äôs now see some examples of using regular expressions.\nExample-1: Search all numbers present in the below text.\n\nimport re\n\ntext = 'Apple Bus 123 Air 34 Data 33 45Egg'\n\n# \\d matches any digit while + matches one or more occurrence of preceeding pattern (i.e., digit)\nresults = re.findall(r'\\d+',text)\n\n# print results\nprint(results)\n\n['123', '34', '33', '45']\n\n\n\n\n\n\n\n\nMatch independent word only\n\n\n\nRemember to use \\b in the expression if you want to extract the standalone numbers not which are parts of a string (e.g., 45 in 45Egg).\n\n\n\nimport re\n\ntext = 'Apple Bus 123 Air 34 Data 33 45Egg'\n\n# \\d matches any digit while + matches one or more occurrence of preceeding pattern (i.e., digit)\nresults = re.findall(r'\\b\\d+\\b',text)\n\n# print results\nprint(results)\n\n['123', '34', '33']\n\n\nReferences 1. https://regex101.com/ 2. https://www.regexone.com/ 3. https://docs.python.org/3/library/re.html"
  },
  {
    "objectID": "posts/post-with-code/Regular expression.html#examples",
    "href": "posts/post-with-code/Regular expression.html#examples",
    "title": "Introduction to Regular Expressions",
    "section": "",
    "text": "Let‚Äôs now see some examples of using regular expressions.\nExample-1: Search all numbers present in the below text.\n\nimport re\n\ntext = 'Apple Bus 123 Air 34 Data 33 45Egg'\n\n# \\d matches any digit while + matches one or more occurrence of preceeding pattern (i.e., digit)\nresults = re.findall(r'\\d+',text)\n\n# print results\nprint(results)\n\n['123', '34', '33', '45']\n\n\n\n\n\n\n\n\nMatch independent word only\n\n\n\nRemember to use \\b in the expression if you want to extract the standalone numbers not which are parts of a string (e.g., 45 in 45Egg).\n\n\n\nimport re\n\ntext = 'Apple Bus 123 Air 34 Data 33 45Egg'\n\n# \\d matches any digit while + matches one or more occurrence of preceeding pattern (i.e., digit)\nresults = re.findall(r'\\b\\d+\\b',text)\n\n# print results\nprint(results)\n\n['123', '34', '33']\n\n\nReferences 1. https://regex101.com/ 2. https://www.regexone.com/ 3. https://docs.python.org/3/library/re.html"
  },
  {
    "objectID": "posts/post-with-code/basics_nlp.html",
    "href": "posts/post-with-code/basics_nlp.html",
    "title": "Basics of Natural Language Processing in Python",
    "section": "",
    "text": "Natural language processing is an interdisciplinary field combining computer science, artificial intelligence, and linguistics. Natural language processing focuses on the processing of natural languages (i.e., human languages such as English, and Hindi). This processing aims at comprehending natural languages and generating text in those languages.\nNatural language processing, thus, enables computers to understand and process human languages and presents the enormous potential of building intelligent machines capable of understanding human input in their own languages.\nIn this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial. The post is targeted at beginners who are just starting to gain first-hand experience with natural language processing. # Basics of Natural Language Processing in Python Natural langauge processing enables computer to human languages. It combines linguistic with statistical modeling. In this post, we will become familiar with basics of nlp with Python. We will use NLTK library for the tutorial.\nFront image by Andrea De Santis on Unsplash."
  },
  {
    "objectID": "posts/post-with-code/basics_nlp.html#installation",
    "href": "posts/post-with-code/basics_nlp.html#installation",
    "title": "Basics of Natural Language Processing in Python",
    "section": "Installation",
    "text": "Installation\nFirst we need to install nltk library. The following command can be used to do that.\n\npip install nltk"
  },
  {
    "objectID": "posts/post-with-code/basics_nlp.html#basic-concepts",
    "href": "posts/post-with-code/basics_nlp.html#basic-concepts",
    "title": "Basics of Natural Language Processing in Python",
    "section": "Basic concepts",
    "text": "Basic concepts\nNow, we will get familiar with the basic concepts of natural language processing. This processing takes place through multiple steps. With each step, a higher level of abstraction is achieved. Let‚Äôs understand some basic steps first.\nWhen a text is preseted to a computer, it only sees the text as a sequence of characters; So, the first step focuses on breaking the character‚Äôs sequence into sentences, and then each sentence into words. This step is known as Tokenization.\nNext, the sentence structure is understood from a grammatical point of view. This involves identifying nouns, verbs, objects, etc. This step is known as Parts-of-Speech Tagging.\nOnce the grammatical relavant tags are identified for each word in the sentence, the next step tries to apply a transformation which replaces words with their base form. For example, transforming running into run. This step is known as Stemming/Lemmatization. We will later discuss the differences between these two.\nThe final step converts the text data into numbers for the computer‚Äôs usage. This step is known as Vectorization.\nNow we will cover these topics one by one. The list of topics is provided below as well.\n\nTokenization\nParts of Speech Tagging\nStemming/Lemmatization\nVectorization\n\n\nTokenization\nLet‚Äôs start with tokenization, the first step in the process. The tokenization step simply breaks down text data into smaller units for analysis purposes such as sentences, words, numbers, etc. These units are also known as tokens.\nThe following program performs tokenization, first breaking the text into a group of sentences, and second, breaking each sentence into a group of words.\n\nfrom nltk import word_tokenize, sent_tokenize\n\ntext = \"\"\"This post offers basics of natural langauge processing (NLP) in Python. \n    NLP enables computer to human languages. It combines linguistic with statistical modeling.\n    \"\"\"\n\n# Breaking the text data into sentences\nsentences = sent_tokenize(text)\nprint('Sentences:\\n',sentences)\n\n# Breaking each sentece into words\nwords = [word_tokenize(sentence) for sentence in sentences]\nprint('Words:\\n',words)\n\nSentences:\n ['This post offers basics of natural langauge processing (NLP) in Python.', 'NLP enables computer to human languages.', 'It combines linguistic with statistical modeling.']\nWords:\n [['This', 'post', 'offers', 'basics', 'of', 'natural', 'langauge', 'processing', '(', 'NLP', ')', 'in', 'Python', '.'], ['NLP', 'enables', 'computer', 'to', 'human', 'languages', '.'], ['It', 'combines', 'linguistic', 'with', 'statistical', 'modeling', '.']]\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou need to run nltk.download('punkt') only once.\n\n\n\n\nPOS tagging\nLet‚Äôs move to now understanding grammatical structure of sentences. The step involves identifying whether a word in the sentence is noun, verb, adverb, etc. It is known as Parts-of-Speech or POS tagging.\nLet‚Äôs see our first example of tagging.\n\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk import pos_tag\nimport nltk\n\n# Uncomment the below statement if you get an error of tagger not found\n# nltk.download('averaged_perceptron_tagger')\n\n# using only word tokenization because there is only one sentence in the text data.\nwords = word_tokenize('Estonia is a leading country in digital space.')\n\n# applying parts-of-speech tagging\ntags = pos_tag(words)\n\nprint(tags)\n\n[('Estonia', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'JJ'), ('country', 'NN'), ('in', 'IN'), ('digital', 'JJ'), ('space', 'NN'), ('.', '.')]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can check the meaning of each tag using ntlk.help.upenn_tagset() function.\n\n\n\n# checking the meaning of NN\nnltk.help.upenn_tagset('NN')\n\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\n\n\nThe result is provided in a form of list of tuples. Each tuple contains a word and corresponding word category (e.g., VBZ for verb). This tuple is also known as tagged token. You can read about it in more details here.\nThe complete list is given below.\n\n\n\n\n\n\n\nTag\nDescription\n\n\n\n\nCC\ncoordinating conjunction\n\n\nCD\ncardinal digit\n\n\nDT\ndeterminer\n\n\nEX\nexistential there (like: ‚Äúthere is‚Äù . think of it like ‚Äúthere exists‚Äù)\n\n\nFW\nforeign word\n\n\nIN\npreposition/subordinating conjunction\n\n\nJJ\nadjective ‚Äòbig‚Äô\n\n\nJJR\nadjective, comparative ‚Äòbigger‚Äô\n\n\nJJS\nadjective, superlative ‚Äòbiggest‚Äô\n\n\nLS\nlist marker\n\n\nMD\nmodal could, will\n\n\nNN\nnoun, singular ‚Äòdesk‚Äô\n\n\nNNS\nnoun plural ‚Äòdesks‚Äô\n\n\nNNP\nproper noun, singular ‚ÄòHarrison‚Äô\n\n\nNNPS\nproper noun, plural ‚ÄòAmericans‚Äô\n\n\nPDT\npredeterminer ‚Äòall the kids‚Äô\n\n\nPOS\npossessive ending parent\n\n\nPRP\npersonal pronoun I, he, she\n\n\nPRP$\npossessive pronoun my, his, hers\n\n\nRB\nadverb very, silently,\n\n\nRBR\nadverb, comparative better\n\n\nRBS\nadverb, superlative best\n\n\nRP\nparticle give up\n\n\nTO,\nto go ‚Äòto‚Äô the store.\n\n\nUH\ninterjection, errrrrrrrm\n\n\nVB\nverb, base form take\n\n\nVBD\nverb, past tense took\n\n\nVBG\nverb, gerund/present participle taking\n\n\nVBN\nverb, past participle taken\n\n\nVBP\nverb, sing. present\n\n\nVBZ\nverb, third person sing. present takes\n\n\nWDT\nwh-determiner which\n\n\nWP\nwh-pronoun who, what\n\n\nWP$\npossessive wh-pronoun whose\n\n\nWRB\nwh-abverb where, when\n\n\n\n\n\nStemming and Lemmatization\nThis step transforms a word into its root word or base form. For example, car, cars, car‚Äôs all share a common root word car. In the linguistic field, such words are known as words with inflection endings or derivationally related words. There is a nice post which you can refer to understand more about it and also about morphological analysis.\nHere we are briefly discussing inflection and derivational forms of words.\nInflection forms These word forms are used to distinct tenses, person, gender, etc. For example, words like go, going, gone, goes. If you notice these words have different endings. These all are called inflection endings in linguistic field.\n\n\n\n\n\n\nTip\n\n\n\nThe words with inflection endings do not have a separate entry in the dictionary. You will find all words with inflection endings under a single entry i.e., go.\n\n\nDerivational form\nThese word forms are derived from the rood words and create a new meaning. For example, react and actor both are derived from the word act.\n\n\n\n\n\n\nTip\n\n\n\nThe words with derivational forms have a separate entry in the dictionary. You will find a separate entry in the dictionary for act, react, and actor.\n\n\nNow, as we have a preliminary understanding of different forms of words, we next move to the stemming and lemmatization. These are two techniques to transform a word from its inflection form (and sometimes derivational form) to the base form.\n\n\nStemming\nStemming is the technique which simply chops off the ending of a word to obtain its base form. For example, removing ing from eating to obain the base form i.e., eat. By default NLTK uses a rule-based stemmer (i.e., Porter Stemmer). There are other stemmer as well. You can check this page for more information on different stemmers.\nLet‚Äôs take a look at the following code which perform stemming.\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n# Uncomment the following statement if running it first time\n#nltk.download('wordnet')\n\n\n# Initialize Python porter stemmer\nps = PorterStemmer()\n\n# Tokenize the text data\nwords = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n\n# Printing the data with its base form after stemming operation\nfor word in words:\n    print('Word:{:10} Stem:{}'.format(word,ps.stem(word)))\n\nWord:Estonia    Stem:estonia\nWord:is         Stem:is\nWord:a          Stem:a\nWord:leading    Stem:lead\nWord:country    Stem:countri\nWord:in         Stem:in\nWord:digital    Stem:digit\nWord:space      Stem:space\nWord:,          Stem:,\nWord:and        Stem:and\nWord:on         Stem:on\nWord:its        Stem:it\nWord:way        Stem:way\nWord:to         Stem:to\nWord:become     Stem:becom\nWord:the        Stem:the\nWord:leader     Stem:leader\nWord:.          Stem:.\n\n\nIn the output, we can see that words leading, country, digital are transformed into lead, countri, and digit, respectively. Now, we will move to Lemmatization.\n\n\nLemmatization\nLemmatization is an another technique which also perform a similar task as Stemming i.e., transforming words into its base forms. However, it differs in its approach. Lemmatization uses morphological analysis to achieve the goal. The morphological analysis means understanding of words and their parts.\nYou can refer to this post to gain more information on different lemmatization approaches in python.\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Lemmatizer \nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Tokenization\nwords = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n\nfor word in words:\n    print('Word:{:10} Lemma:{} Tag:{}'.format(word,wordnet_lemmatizer.lemmatize(word),wordnet_lemmatizer.lemmatize(word)))\n\nWord:Estonia    Lemma:Estonia\nWord:is         Lemma:is\nWord:a          Lemma:a\nWord:leading    Lemma:leading\nWord:country    Lemma:country\nWord:in         Lemma:in\nWord:digital    Lemma:digital\nWord:space      Lemma:space\nWord:,          Lemma:,\nWord:and        Lemma:and\nWord:on         Lemma:on\nWord:its        Lemma:it\nWord:way        Lemma:way\nWord:to         Lemma:to\nWord:become     Lemma:become\nWord:the        Lemma:the\nWord:leader     Lemma:leader\nWord:.          Lemma:.\n\n\n[nltk_data] Downloading package wordnet to /Users/htk/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\n\n\n\n\n\nNo change after lemmatization\n\n\n\nIf you notice in the results there are no changes after applying lemmatization. The reason is that if the word can not be found in WordNet (publicly awailable English dataset) then the word remain unchanged. It can be corrected by providing the pos tag of the word when calling lemmatize function.\n\n\nNow, we will supply pos tag of each word when calling lemmatize function. However, the function only takes a single character for the pos tag. For example, n for nouns, v for verbs, a for adjectives, and r for adverbs.\nSo, we need to prepare a mapping which translates pos tags, obtained from nltk.pos_tag() function, into a, r, n, v (depending on the tag).\nWe know from the POS tags table above that tags for adjectives starts from ‚ÄòJ‚Äô. So what we can do is, we can take the first character of pos-tag and determine which tag to supply in lemmatize() function.\n\nfrom nltk import pos_tag\n\ndef get_pos(word):\n    # the function returns a list with one tagged tuple, e.g., [('riding','VBD')]\n    tagged_tuple_list = pos_tag([word])\n    \n    # fetching the item in the list and then the tag in tuple\n    tagged_tuple = tagged_tuple_list[0]  # the first index will fetch the fist item in the list\n    \n    # fetching the tag from the tagged tuple\n    tag = tagged_tuple[1]   # the index will fetch the tag (e.g., 'VBD')\n    \n    # extracting the first character\n    tag_char = tag[0]\n    \n    # all these three statement can be combined into a single statement given below\n    # tag_char = pos_tag([word])[0][1][0]\n    \n    # Now we will create a mapping\n    pos_to_lemma_tag = {\n        'J': 'a',\n        'N': 'n',\n        'R': 'r',\n        'V': 'v'\n    }\n    \n    # we will return the tag for usage in lemmatize function\n    return pos_to_lemma_tag.get(tag_char,'n')   # get function will return n if the tag is something else\n\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Lemmatizer \nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Tokenization\nwords = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n\nfor word in words:\n    print('Word:{:10} Lemma:{}'.format(word,\n                                       wordnet_lemmatizer.lemmatize(word,get_pos(word))))\n    \n\nWord:Estonia    Lemma:Estonia\nWord:is         Lemma:be\nWord:a          Lemma:a\nWord:leading    Lemma:lead\nWord:country    Lemma:country\nWord:in         Lemma:in\nWord:digital    Lemma:digital\nWord:space      Lemma:space\nWord:,          Lemma:,\nWord:and        Lemma:and\nWord:on         Lemma:on\nWord:its        Lemma:it\nWord:way        Lemma:way\nWord:to         Lemma:to\nWord:become     Lemma:become\nWord:the        Lemma:the\nWord:leader     Lemma:leader\nWord:.          Lemma:.\n\n\n\nIt worked now like a charm :-)\n\n\n\nVectorization\nNow, we will move to the final step in the aforementioned list of preprocessing steps in natural language processing. This final step is vectorization step. This step translates the text into numbers so that computers can use them for further anlaysis.\nThere are multiple techinques of vectorization. In this post, we are going to discuss two basics techniques techniques: count vectorization, and tf-idf vectorization.\n\nCount Vectorization\nIn this technique, the input text is first broken down into a set of unique words, and next, each word is assigned a number which represents the frequency of that word.\nLet‚Äôs see a working example. For the example, we will use CountVectorizer from scikit-learn.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# input data\ninput_text = [\"In this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial.\"] \n\n# initialization\nvect = CountVectorizer()\n\n# applying count vectorization on the text\nresult = vect.fit_transform(input_text)\n\n# printing vocubulary with frequency\nprint('Shape:',result.shape, '\\n Vector:',result.toarray())\n\nShape: (1, 20) \n Vector: [[1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 2 2 2]]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe result is a single vector of length 20. In out input, we only had a single sentence, therefore, one got a single vector. In case of multiple sentences, we get one vector for each sentence.\n\n\n\n\nTF-IDF\nThe next vectorization technique is TF-IDF (Term Frequency- Inverse Document Frequency). Let‚Äôs understand their meaning. \nTerm Frequency (TF) This frequency counts the number of times a word occurs in the document.\nInverse Document Frequency This is the inverse of how many documents contains the specified term.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# input data\ninput_text = [\"This is an amazing field with huge potential of building intelligent machine.\",\n              \"Those machine would be able to transform people's life.\",\n              \"This transformation would significantly improve the quality of life.\"]\n# TF-IDF intialization\ntf = TfidfVectorizer()\n\n# applying vectorizer\nresult = tf.fit_transform(input_text)\n\n# print results\nprint('Shape:',result.shape, '\\n Vector:',result.toarray())\n\nShape: (3, 25) \n Vector: [[0.         0.30520733 0.30520733 0.         0.30520733 0.30520733\n  0.30520733 0.         0.30520733 0.30520733 0.         0.23211804\n  0.23211804 0.         0.30520733 0.         0.         0.\n  0.23211804 0.         0.         0.         0.         0.30520733\n  0.        ]\n [0.35955412 0.         0.         0.35955412 0.         0.\n  0.         0.         0.         0.         0.27345018 0.27345018\n  0.         0.35955412 0.         0.         0.         0.\n  0.         0.35955412 0.35955412 0.35955412 0.         0.\n  0.27345018]\n [0.         0.         0.         0.         0.         0.\n  0.         0.36977238 0.         0.         0.28122142 0.\n  0.28122142 0.         0.         0.36977238 0.36977238 0.36977238\n  0.28122142 0.         0.         0.         0.36977238 0.\n  0.28122142]]\n\n\n\nYou can check [this blog post](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp) on further information on vectorization.\n\n\nReferences\n1. https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n2. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n3. https://www.nltk.org/book/\n4. Morphological analysis: https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx\n5. https://www.datacamp.com/tutorial/stemming-lemmatization-python\n6. https://pianalytix.com/countvectorizer-in-nlp/#:~:text=CountVectorizer%20means%20breaking%20down%20a,data%20needs%20to%20be%20vectorized."
  },
  {
    "objectID": "posts/post-with-code/data-anonymization.html",
    "href": "posts/post-with-code/data-anonymization.html",
    "title": "Introduction to Data Anonymization",
    "section": "",
    "text": "You most likely have heard the term ‚Äúdata anonymization‚Äù if you are involved with data directly or indirectly. In this post, we will learn about this crucial part of the data processing pipeline, targetting data privacy at its core.\nBut wait a minute. Let‚Äôs address this question first ‚Äúwhy does it matter?‚Äù before delving into data anonymization."
  },
  {
    "objectID": "posts/post-with-code/data-anonymization.html#why-does-data-anonymization-matter",
    "href": "posts/post-with-code/data-anonymization.html#why-does-data-anonymization-matter",
    "title": "Introduction to Data Anonymization",
    "section": "Why does data anonymization matter?",
    "text": "Why does data anonymization matter?\nWell, to start with, data anonymization matters to honor people‚Äôs Right to Privacy, a fundamental human right, included in the universal declaration of human rights by the United Nations (article 12).\n\nNo one shall be subjected to arbitrary interference with his privacy, family, home, or correspondence, nor to attacks upon his honor and reputation. Everyone has the right to the protection of the law against such interference or attacks.\n\nThis article, though, does not explicitly mention data privacy but it is inherent in the term privacy.\nIn the current digital age we are living in today, we are increasingly contributing, knowingly or unknowingly, to data collection. Some of those data are highly personal (e.g., credit card number, phone number) ‚Äìalso referred to as personal data‚Äì making data privacy a paramount concern.\n\n\n\n\n\n\nPersonal Identifiable Information (PII)\n\n\n\nA formal definition of Personal data is any information concerning the identified person or identifiable living person (GDPR 2016, Indian Digital Data Protection Act 2023). This is also referred to as Personal Identifiable Information (PII) (Refer to this post for more detailed information on PII). Some examples are name, email address, and passport number.\n\n\nThe first and foremost reason why data anonymization matters? Is because data anonymization helps to protect personal data to respect people‚Äôs privacy and, thus, upholds the fundamental Right to Privacy. However, we don‚Äôt live in an perfact world where people always do what is ethical and right (In that case we would not have needed laws at all). So to ensure the protection of personal data, data privacy regulations have been drafted and integrated in constitutions across the world (refer to this link for more information on data privacy laws in different countries). For example, GDPR in European countries, Digital Data Protection Act in India, Personal Information Protection Law in China, etc. **\nThe second reason why we should care for data anonymization is that it helps in adhering to data-privacy regulations and legislation violations which could not only result in monetary loss but also loss of customer trust.\n\nNon-compliance with data-privacy legislation could cost a fortune. Take the example of 746 million euro penalty on Amazon for not processing personal data as per GDPR**). So to avoid such a situation and also to respect people‚Äôs privacy, we need a way that can strike a balance between the utility of data and privacy preservation. Here, we have data anonymization."
  },
  {
    "objectID": "posts/post-with-code/data-anonymization.html#data-anonymization",
    "href": "posts/post-with-code/data-anonymization.html#data-anonymization",
    "title": "Introduction to Data Anonymization",
    "section": "Data anonymization",
    "text": "Data anonymization\nData anonymization removes personal information from the data without degrading the utility of that data for the objective at hand, thus, greatly reducing the risks associated with data privacy.\n\n\n\n\n\n\nTip\n\n\n\nMore importantly, data privacy regulations‚Äîsuch as the GDPR‚Äîdo not apply to properly anonymized data, allowing organizations to operate without compliance concerns (provided the anonymization is conducted correctly).\n\n\nData anonymization techniques can be broadly categorized into two main types: anonymization and pseudonymization.\nThe following section explores both approaches, highlights their key differences, and illustrates their applications with real-world examples.\n\nAnonymization v/s Pseudonymization\nData anonymization and pseudonymization are both methods of removing personal information, but they differ in two key ways:\n\nThe level of anonymity they provide\nWhether the process can be reversed\n\nThe figure below highlights the main differences between these two approaches.‚Äù \n\nExample\nLet‚Äôs take an example to understand it further. Consider the following table, which consists of Personal Identifiable Information (PII).\nOriginal data\n\n\n\nName\nGender\nDate of Birth\nZipcode\n\n\n\n\nJohn Smith\nMale\n1985-03-15\n90210\n\n\nEmily Davis\nFemale\n1990-07-22\n10001\n\n\nMichael Lee\nMale\n1978-11-05\n30301\n\n\n\n Pseudonymized data\nPseudonymization replaces direct personal identifiers (like names) with artificial keys or pseudonyms. The mapping between original identifiers and pseudonyms is preserved, enabling valuable use cases such as targeted marketing campaigns while still protecting raw personal data.\n\n\n\nPseudonymized Name\nGender\nDate of Birth\nZipcode\n\n\n\n\nUser_1A7B\nMale\n1985-03-15\n90210\n\n\nUser_2C8D\nFemale\n1990-07-22\n10001\n\n\nUser_3E6F\nMale\n1978-11-05\n30301\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe mapping from the original PII to the pseudonymized version is reversible which means individuals still can be re-identified.\n\n\n Anonymized data\nIn the case of an anonymized version of data, the mapping is irreversible. For example, names are replaced with alternate strings without retaining any information on the mapping which means we don‚Äôt have any means of converting back the anonymized ID to original PII using only the information available in the table.\n\n\n\nAnonymized ID\nGender\nDate of Birth\nZipcode\n\n\n\n\nANON_001\nMale\n1985-03-15\n90210\n\n\nANON_002\nFemale\n1990-07-22\n10001\n\n\nANON_003\nMale\n1978-11-05\n30301\n\n\n\nHowever, this de-indentified data along with some other publicly available information could led to re-identification.\n\n\n\n\n\n\nImportant\n\n\n\nThe table though anonymized personal identifier but re-identification is still possible using some additional information.\nFor example, Professor Lataney Sweeney (who also proposed the k-anonymity method) showed that ‚Äú87% of the U.S. Population are uniquely identified by {date of birth, gender, ZIP}‚Äù. This now really makes us think again about the risks associated with our anonymized version of data.\n\n\nTo mitigate re-identification risks in supposedly ‚Äòanonymized‚Äô data, robust techniques like k-anonymity (discussed earlier) and generalization have been developed. When properly implemented, these methods enable correct anonymization‚Äîensuring personal data cannot be traced back to individuals, as illustrated in the example below.\n\n\n\nAnonymized ID\nGender\nAge Group\nGeneralized Zipcode\n\n\n\n\nANON_001\nMale\n35-44\n90XXX\n\n\nANON_002\nFemale\n30-34\n10XXX\n\n\nANON_003\nMale\n45-54\n30XXX\n\n\n\n\n\nIn the upcoming posts, we‚Äôll peel back the layers of data anonymization!\nStay tuned for more privacy-preserving insights! üîçüîí"
  },
  {
    "objectID": "posts/post-with-code/data-anonymization.html#references",
    "href": "posts/post-with-code/data-anonymization.html#references",
    "title": "Introduction to Data Anonymization",
    "section": "References",
    "text": "References\n\nWhat is Personally Identifiable Information (PII)? | IBM\nProtection of personal data and privacy\nData protection laws - Data Protection Laws of the World\nhttps://ega.ee/wp-content/uploads/2022/06/eGov-Legal-Framework-GDPR-presentation-06.06.2022-KSL.pdf\nhttps://www.meity.gov.in/static/uploads/2024/06/2bf1f0e9f04e6fb4f8fef35e82c42aa5.pdf\nhttps://medium.com/mercedes-benz-techinnovation-blog/data-privacy-anonymization-and-pseudonymization-part-1-6b145459a3bf"
  },
  {
    "objectID": "posts/post-with-code/mqtt.html",
    "href": "posts/post-with-code/mqtt.html",
    "title": "Setting up MQTT server and Client",
    "section": "",
    "text": "This post offers an introduction to the MQTT (Message Queuing Telemetry Transport) protocol [1] and also demonstrates its usage with an example in Python (Just for info: telemetry means the collection of measurement data from a remote location and its transmission wiki-link."
  },
  {
    "objectID": "posts/post-with-code/mqtt.html#what-is-mqtt",
    "href": "posts/post-with-code/mqtt.html#what-is-mqtt",
    "title": "Setting up MQTT server and Client",
    "section": "What is MQTT?",
    "text": "What is MQTT?\n\nMQTT is a communication protocol which is actually developed for low-energy devices to transmit data with the low-bandwidth network. It is a lightweight protocol built on the top of TCP/IP.\n\n\n\n\n\n\nTip\n\n\n\nLightweight: In software/programs, lightweight specify the characteristic of low memory and CPU usage.\n\n\nMQTT allows simple and efficient data transmission from sensors to IoT (Internet of Things) networks. Additionally, it also enables the easy integration of new sensors to the IoT network.\n\n\nMQTT Terminology\n\nIn MQTT, there are five main terminology\n\n\n\nBroker: It is the middleware between data sender (publisher) and data receiver (subscriber). You can consider it as a server which receives data from sender and forward it to the receiver.\n\n\nPublisher: In simple terms, you can think of a device which needs to send data to other parties. This device could be a sensor, laptop, and Raspberry pi board.\n\n\nSubscriber: It is the receiver of data sent by the publisher.\n\n\nTopic: Publisher and Subscriber do not know each other directly. When the publisher sends some data to Broker, it associates that data with a topic . The topic is a string in the format of hierarchy e.g.¬†sensor/room-1/temprature (it is just an example) where ‚Äò/‚Äô is level separator and each string represents a level. You can define it as per your requirement. When the subscriber connects to the Broker, it specifies the topic in which it is interested in. Then, broker forward published messages to their corresponding subscribers on the basis of the topic. Here, you need to understand two operators + and #. Before jumping to these operators, let‚Äôs assume a scenario where we have four temperature sensors in four rooms. These sensors are sending their data with the following topics\n\n\n\n\n\n\nhome/bedroom/temperature\n\n\nhome/kitchen/temperature\n\n\nhome/dining-room/temperature\n\n\nhome/study-room/temperature\nNow, in order to receive data from all four rooms, subscribers need to subscribe to the above topics. One way of doing that is to subscribe to each topic separately. Another way is to use + operator and simply subscribe to the topic home\\+\\temprature . + operator here matches any single level ( + is known as a single-level wildcard). Therefore, home+will match any topic with three-level hierarchy starting with home¬† and ending with . Coming to second operator # which matches multiple levels in the topic. For instance, home\\# will match all the aforementioned four topics.\nSome examples Let‚Äôs say we have multiple sensors which are transmitting data with following topics\n\n\n\n\n\nschool\\class-1\\group-1\\audio\nschool\\class-1\\group-2\\audio\nschool\\class-2\\group-1\\audio\nschool\\class-2\\group-2\\audio\n\n\n\n\n\n\n\nTip\n\n\n\nWhich topic to subscribe to receive all data from class-1 ? school\\class-1\\#\nWhich topic to subscribe to receive data from group-1? school\\+\\group-1\\audio\nWhich topic to subscribe to receive data from entire school? school\\#\n\n\n\n# operator can appear only once in a topic expression e.g.¬†school## is invalid. A topic expression # matches every topic hence subscriber to this topic will receive every message.\n\n\n\n\nMessage: It is simply the data which needs to be sent.\n\n\n\n\n\n\nDemonstration of MQTT in Python\n\nNow, we will see a python example of using MQTT protocol for transmitting data. We will use a python package [paho-mqtt](https://pypi.org/project/paho-mqtt/ ‚Äú‚Äù target = ‚Äú‚Äú_blank) for creating publisher and subcriber. You can install it using following command\n pip install paho-mqtt \nNext, we need to set up a Broker. There are numerous option of this. You can either use a cloud-based broker or you can install a broker on a server in your network. There are multiple cloud services are available with can be used ([Complete list of broker servers](https://github.com/mqtt/mqtt.github.io/wiki/servers ‚Äú‚Äù target = ‚Äú‚Äú_blank)). I would like to mention [MaqiaTTo](https://www.maqiatto.com ‚Äú‚Äù target = ‚Äú‚Äú_blank) which is a free cloud-based MQTT broker. It can be used for testing purpose. In this tutorial, however, we are going to set-up a broker in the network. We will use [Mosquitto](https://mosquitto.org/ ‚Äú‚Äù target = ‚Äú‚Äú_blank) broker server.\n\n\nSetting up Mosquitto Broker\n\nMosquitto is an open-source MQTT message broker. Following are the instructions for installing it on your systems.\n\n\n\nWindows users: Download and install 64bit-version, 32bit-version(If you are not sure then install 32bit-version)\n\n\nUbuntu Users: Run following commands\nsudo apt-add-repository ppa:mosquitto- dev/mosquitto-ppa\nsudo apt-get update \n\n\n\n\n\nMac Users: First install brew package manager using following command\n/usr/bin/ruby \\\n -e \"\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\" \nNext, install Mosquitto\nbrew install mosquitto \n\n\n\n\nStarting Mosquitto Server\nNow, we are going to start our MQTT broker. Run the following command [Mac users]\n/usr/local/sbin/mosquitto -c /usr/local/etc/mosquitto/mosquitto.conf \n\n\nOn ubuntu following command can be used to start or stop Mosquitto\nsudo systemctl (start|stop) mosquitto\n\n\n\n\n\nWriting Publisher and Subscriber codes\n\nNow, we will develop our publisher and subscriber. Following are the steps for developing publisher and subscriber in Python.\n\n\n\nCreate client\n\n\nConnect to broker\n\n\nPublish/subscribe message\n\n\nConnect callback function\n\n\nRun the loop\n\n\nDisconnect\n\n\n\nLet‚Äôs understand these steps in details. The first step is to create a client. For this, we will create a Client object from paho-MQTT python package.\nThe second step connects to the broker. In our case, we are running the broker on the same machine, therefore, we specify the broker‚Äôs IP as 127.0.0.1 and port as 1883 (default port for Mosquitto broker).\nIn third step, you can publish or subcribe using publish() and subscribe() function.\nNext, we need to write callback functions. It needs a bit of explanation. Callback functions are functions which are executed on the occurrence of particular events. Followings are the table showing the event and their corresponding callback function\n\n\n\n\n\nEvent\nCallback Function\n\n\nConnection ACK\non_connect\n\n\nDis-connection ACK\non_disconnect\n\n\nPublish ACK\non_publish\n\n\nSubcription ACK\non_subcribe\n\n\nUn-subcription ACK\non_unsubcribe\n\n\nMessage received\non_message\n\n\n\n\n\nLet‚Äôs take one example to understand it. When a client connects to the broker, the broker sends an ACK (acknowledgment) to the client. This event triggers the execution of a callback function on_connect .\n\n\nRunning a loop: Why we need to start a loop?\nWhen a publisher sends messages to the broker or subscriber receive messages from the broker, these messages are first stored in the buffer. Now, in order to process all messages (either for sending or receiving), we need to write a loop manually. Thanks to paho-MQTT , it provides three functions for the same purpose, therefore, we don‚Äôt need to write message processing loop. These functions are as following\n\n\n\n\n\n\nTip\n\n\n\n\n\nloop(): When you call this function, it will process any pending message sending or receiving action. This function waits for a particular time (you can specify using timeout parameter) for processing buffer for reading or sending a message. After, that its execution completes. Therefore, if you plan to use this function you need to call it regularly.\n\n\nloop_forever(): This function call results in indefinite execution of your program. This function automatically reconnects to the broker in case of disconnection. This function is blocking type function (you can understand it as an infinite for loop) and it returns when you disconnect with the broker.\n\n\nloop_start() & loop_stop(): loop_start() function starts a new background thread and that thread regularly execute loop() function. You can stop this background thread using loop_stop() function.\n\n\n\n\n\n\n\nCoding Publisher\n\nAs we have setup our broker server, now we move towards writing publisher code.\n\nimport paho.mqtt.client as MQTT\n\n#  function\ndef connect_msg():\n    print('Connected to Broker')\n\n# function\ndef publish_msg():\n    print('Message Published')\n\n# Creating client\nclient = mqtt.Client(client_id='publisher-1')\n\n# Connecting callback functions\nclient.on_connect = connect_msg\nclient.on_publish = publish_msg\n\n# Connect to broker\nclient.connect(\"\"127.0.0.1\"\",1883)\n\n# if you experience Socket error then replace above statement with following one\n# client.connect(\"\"127.0.0.1\"\",1883,60)\n\n# Publish a message with topic\nret= client.publish(\"\"house/light\"\",\"\"on\"\")\n\n# Run a loop\nclient.loop()\nThe above code publishes a message on with topic house/light. In this code, we have written two functions connect_msg and publish_msg (you can use any names for functions). We connected these functions to callback functions using client.on_connect = connect_msg . We specified that connect_msg function is callback function and it will be called when the Connection ACK event occurs (events and their callbacks are given in the above table). In this program, we used the loop function which process pending action (sending or receiving) and then returns.\n\n\nCoding Subscriber\n\n\nAs we have our publisher with topic housetopic, we now develop our subscriber for the same topic.\nimport paho.mqtt.client as MQTT #import the client\n\n# Function to process recieved message\ndef process_message(client, userdata, message):\n    print(\"\"message received \"\" ,str(message.payload.decode(\"\"utf-8\"\")))\n    print(\"\"message topic=\"\",message.topic)\n    print(\"\"message qos=\"\",message.qos)\n    print(\"\"message retain flag=\"\",message.retain)\n\n\n\n# Create client\nclient = mqtt.Client(client_id=\"\"subscriber-1\"\")\n\n# Assign callback function\nclient.on_message = process_message\n\n# Connect to broker\nclient.connect(broker_address,1883,60)\n\n# Subscriber to topic\nclient.subscribe(\"\"house/light\"\")\n\n# Run loop\nclient.loop_forever() \n\n\nExecution Results\n\n\n\nReferences\n\n\n\nMQTT Version 5.0. Edited by Andrew Banks, Ed Briggs, Ken Borgendale, and Rahul Gupta. 07 March 2019. OASIS Standard. https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html. Latest version: https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html.\n\n\n\n‚Äù"
  },
  {
    "objectID": "posts/post-with-code/A primer on data distribution fundamentals .html",
    "href": "posts/post-with-code/A primer on data distribution fundamentals .html",
    "title": "A primer on discrete data distribution fundamentals",
    "section": "",
    "text": "This post offers an introduction to discrete data distributions which are essential to know for every data scientist. The post starts by explaining what does it mean by data distribution and then dives deep into the following six data distributions with real world examples:"
  },
  {
    "objectID": "posts/post-with-code/A primer on data distribution fundamentals .html#what-is-data-distribution",
    "href": "posts/post-with-code/A primer on data distribution fundamentals .html#what-is-data-distribution",
    "title": "A primer on discrete data distribution fundamentals",
    "section": "What is data distribution?",
    "text": "What is data distribution?\nLet‚Äôs now understand what is the meaning of data distribution. In data science projects, we work on different types of data and those data come from somewhere. For example, a dataset containing salary information of employees comes from a population of employees working at a particular company. In the dataset, there would be some values occuring quite frequently while some rarely. Data distribution enables an understanding of such likelihood of data values.\nLet‚Äôs take an example of such dataset.\n    # salary in thousands\n    salary = [1500,1200,1500,1300,1500,2100,2800,2500,1500,2100]\n\nIn the above example, we can see that the most frequently occuring salary is 1500. In other terms, there is a higher likelihood that a person chosen from the company most likely to have a salary of 1500. This is what we aim to study by data distribution, i.e., how the values are distributed or what are their likelihood?\nFigure Figure¬†1 below shows the frequency count (and normalized frequency count) for each unique salary in the dataset.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# counting frequencies\ncount_freq = dict(Counter(salary))\n\n# salary data\nsalary = [1500,1200,1500,1300,1500,2100,2800,2500,1500,2100]\n\n\n\nCode\n# plotting distribution\nplt.bar(count_freq.keys(),count_freq.values(),width=15)\nplt.title('Frequency count')\nplt.show()\n# plotting distribution\nplt.bar(count_freq.keys(),[item/len(salary) for item in count_freq.values()],width=15)\nplt.title('Probability')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) frequency\n\n\n\n\n\n\n\n\n\n\n\n(b) percentage/probability\n\n\n\n\n\n\n\nFigure¬†1: Frequency/precentage distribution\n\n\n\nIf we are asked what percentage of times a salary of 1500 occurred in the dataset then we can answer 40% (Why: 4 out of 10 salaries were 1500)\nWith the above figures, we can answer questions like\n\nWhat are the chances of a person having salary less than 1500 euros?\nWhat are the chances of a personal having a salary of 2800 euros?\n\nThese are just a few examples. Data distribution helps to answer such questions.\nIn a simple term, data distribution is a function which maps the value occuring in the dataset to its corresponding likelihood of appearing in the dataset.\nFor example, \\[P(1500) = .4\\]\n\n\n\n\n\n\nImportant\n\n\n\n   Once we have distribution function we do not need the dataset to answer questions like those mentioned above.     \n\n\nWe have several well defined data distribution functions which can be utilized to model the data. These distributions are divided into two categories based on values of data, i.e., Discrete and Continuous.\nIn this blog post, we will cover six discrete data distributions, i.e., Bernoulli, Binomial, Geometric, Negative Binomial, Hypergeometric, and Piosson.\nBefore diving into different discrete distributions, we need to first understand a bit about Random Variable. In data modeling problems, we are often interested in a particular phenomenon/event, e.g., salary in our previous example. We model such events using random variables denoted as \\(X\\).\n\n\n\n\n\n\nImportant\n\n\n\nA random variable is not like a typical variable instead it is a mapping that maps the phenomenon/event of our interest into a real number.\n\n\nRandom variables can take any values from the sample space (i.e., all different salaries of employees, e.g., \\(X=1500\\))\nIn the following sections, you would often see the notation \\(P(X=x)\\) that represents the probability of random variable \\(X\\) to have a value \\(x\\)."
  },
  {
    "objectID": "posts/concepts/pca.html",
    "href": "posts/concepts/pca.html",
    "title": "Understanding Principal Component Analysis under the hood",
    "section": "",
    "text": "Let‚Äôs assume that you have a dataset with a higher number of attributes and you are thinking Is there any way to compress this information into a smaller number of attributes.\nWell, dimensionality reduction methods offer that functionality. Among many dimensionality reduction methods, PCA (Principal Component Analysis) is widely used and this post introduction to PCA and its working.\nLet‚Äôs start with a simple dataset with two attributes, \\(x\\), and \\(y\\) which we need to reduce from two attributes to one. \n\n\n\n\n\n$x$\n$y$\n\n\n10\n5\n\n\n12\n5\n\n\n16\n6\n\n\n20\n7\n\n\n19\n5\n\n\n\n\n\n\n\n\nThe simplest approach is to select one of the attributes. Let‚Äôs say we are selecting \\(x\\). In other words, we are taking the projection of the data points on X-axis (Projection: simply drawing a perpendicular line from data points to the x-axis and taking that corresponding value). For example, in the following diagram, we have taken the projection of data points along the x-axis.\nSo here we have two options, we can take the projection of data points to either x-axis or y-axis. Our aim is to select one which offers more information about the data. To measure it, we can use variance which computes spread of data or in other words how the values are different from their mean value. So if you have an attribute with zero variance that means all values in that attribute are same. Variance can be computed using the following formula \\[ \\sigma^2 = \\sum\\limits_{i=1}^N (X -\\mu)^2 \\] where: \\(X\\) is the set of data points \\(\\mu\\) is the mean \\(N\\) is the number of data points in \\(X\\)\n\n\n\n\nSo, now if we look at our options (projection along x-axis or y-axis) then we find x-axis as a better option due to the larger variance shown in below figure\n\n\n\n\n\n\n\n\n\nDo we really have only these two options?\n\n\n\nNo, there are infinite number of possibilities (How: draw any line and take the projection of data points on that line in a similar manner as we did with x/y axis). In this case, if we can find a line which gives maximum variance of the data compared to other possible options then it can be used at the place of \\(x\\) and \\(y\\) attributes.\n\n\n\nPrincipal Component Analysis (PCA)\n\nAs we discussed in the above section the infinite possibilities for our two-dimensional data, PCA finds the one which offers maximum variance. Let‚Äôs go deeper into the mathematics of PCA.\n\n\n\n\nUnderstanding Mathematics behind PCA\n\n\nFirst, we are going to write the problem statement (finding a direction/vector/line which offers a maximum variance of projected data) of PCA into mathematics format. First, we need to see how to represent the projection. A projection of a data point along with a line can be computed using \\(dot\\) product. Let‚Äôs say we want to compute the projection of the first data point (10,5) on the x-axis.\nLet‚Äôs represent our data point as a vector \\(\\vec{x_1}\\). A vector has two properties- direction and magnitude (more info). The unit vector in the direction of x-axis and y-axis is represented by \\(\\hat{i}\\) and \\(\\hat{j}\\), respectively. Every vector then denoted by number of units in the direction of x-axis and y-axis. Our data point (10,5) can be represented as 10 units in x-axis direction and 5 units in y-axis direction. The dot product between \\(\\vec{x_1}\\) and \\(\\hat{i}\\) will give the projection over the x-axis.\n\n\n\n\n\\[= (10\\hat{i},5\\hat{j}) . (\\hat{i},0)\\]\n\n\n\n\n\\[= 10\\]\n\n\n\n\n\nIn simpler terms, if you have two vectors or list of numbers \\((a_1,a_2,a_3)\\) and\\((b_1,b_2,b_3)\\) then their dot product will be \\(a_1*b_1+a_2*b_2+a_3*b_3\\). It can be written in matrix form as following \\[\n      dot(A,B)=\\begin{bmatrix}\n      a_1 & a_2 & a_3\\\n      \\end{bmatrix}\\begin{bmatrix}\n      b_1 \\\\\n      b_2 \\\\\n      b_3 \\\\\n      \\end{bmatrix}\n     = A^TB\n  \\] here, \\(A^T\\) is transpose of \\(A\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nA dot product of \\(\\vec{A}\\) with its own gives you \\(A^2\\).\n\n\n\\[\n  A^2 = a_1^2+a_2^2+a_3^2 \\\\\n  = a_1*a_1+a_2*a_2+a_3*a_3\n  = A^TA\n  \\]\n\nNow coming back to PCA problem statement, let‚Äôs denote a direction (/vector/line) \\(\\vec{L}\\). So projection of our dataset \\(X\\) can be written as \\[\nX_{new}=X^TL\n\\] here, \\(X_{new}\\) represents the new values obtained after projection of \\(X\\) over \\(L\\). To ease the understanding of next step, let‚Äôs assume we transformed \\(X_{new}\\) in such a way that it has zero mean (we can do that by simply replacing every value in \\(X_{new}\\) by \\(X_{new}-Mean)\\). Next, we compute the variance of \\(X_{new}\\).\n\\[\nvar(X_{new}) = (X_{new}-0)^2 \\\\\n= X_{new}^2 \\\\\n= X_{new}^TX_{new}\n= (X^TL)^T(X^TL)\n= (L^TXX^TL)\n= (L^T  \\sum L)\n\\]\n\nhere, \\(\\sum\\) is covariance matrix of \\(X_{new}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nRule used: \\((AB)^T = B^TA^T\\) and \\((A^T)^T = A\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe add a constraint that L must be a unit vector which means \\(L^TL=1\\).\nWhy: In a single direction there can be infinite possibilities for example \\(2\\hat{i}+1\\hat{j}\\), \\(4\\hat{i}+2\\hat{j}\\), \\(\\hat{i}+.5\\hat{j}\\) all vectors are in the same direction. Therefore, to avoid it, we put a constraint that we will check only a single vector in a direction and that one with unit vector (to avoid large values).\n\n\nNow, our problem is to find \\(L\\) which maximizes \\(var(X_{new})\\) with constraint \\(L^TL=1\\) (constrain of unit vector). (also known as constrained maximization problem)\nTo formulate this problem, we will use Lagrange Multiplier. Our problem can be written as follows \\[\n\\max_{L} = (L^T\\sum L)-\\lambda(L^TL-1)\n\\]\n\nTo solve it, we will differentiate it with respect to \\(L\\) and then equate it to zero. As we seen above that \\(L^TL=L^2\\), therefore, differentiating it gives us \\(2L\\). \\[\n\\sum L - \\lambda (L) = 0\n\\]\n\n\n\n\n\\[ \\sum L = \\lambda L \\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above equation is actually stating that \\(L\\) must be eigenvector of covariance matrix of \\(X\\).\nA brief about eigen vector\n\n\n\n\n\nLet‚Äôs consider a matrix as a system of transformation. When it is multiplied with a vector that vector gets transformed into a new vector. Let‚Äôs take an example. We have a matrix \\[ M =\n    \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\] We will multiply (or apply) it on a vector \\(A = [2,3]\\). The multiplication of \\(M\\) and \\(A\\) will give \\[\n  \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\begin{bmatrix}\n      3 \\\\\n      2  \\\\\n      \\end{bmatrix}\n  = \\begin{bmatrix}\n      -2 \\\\\n      -5 \\\\\n      \\end{bmatrix}\n  \\] As it‚Äôs shown in the below figure that point (3,2) transformed into new point (-2,-5). Now, if we multiply \\(B = [1,1]\\) with M then we will get result in the same direction.\n \\[\n  \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\begin{bmatrix}\n      1 \\\\\n      1  \\\\\n      \\end{bmatrix}\n  = \\begin{bmatrix}\n      -2 \\\\\n      -1 \\\\\n      \\end{bmatrix}\n  = -2\\begin{bmatrix}\n      1 \\\\\n      1 \\\\\n      \\end{bmatrix}\n  \\]\nTherefore vector \\(B\\) is eigen vector for the matrix in above example.\n\n\n\n\nComing back to our last equation \\(\\sum L = \\lambda L\\), now the question is which eigenvector (as there can be many eigen vectors of a matrix) to use. Let‚Äôs have a look on our variance \\((L^T\\sum L)\\) which we want to maximize.\n\n\\[\n= (L^T\\sum L)\n= (L^T\\lambda L)\n= \\lambda(L^T L)\n= \\lambda\n\\] \n\n\n\n\n\n\nTip\n\n\n\nSo, to maximize the variance, we need to take eigenvector with maximum eigen value (\\(\\lambda\\)). Here is our first vector, one with highest Eigen value.\n\n\nUsing this result as a basis, we then have the following steps for PCA\n\n\n\n\n\nCompute Covariance matrix (\\(\\sum\\)) of \\(X\\)\n\n\nSubtract \\(X_\\mu\\) (mean) from \\(X\\)\n\n\nCompute eigenvector \\(\\sum\\)\n\n\nReorder the eigen vectors according to their corresponding eigen value\n\n\nProject data set on those eigenvectors beginning from the start (as the first eigenvector has the highest eigenvalue, second eigenvector with second-highest, and so on)\n\n\n\n\n\nKey points\n\n\n\n\n\nPCA is a unsupvervised machine learning algorithm.\n\n\nPCA dimensions are linear combinations of original attributes. Therefore, it is a linear DR method. However, there are variants of PCA (e.g.¬†kernelPCA) which offers non-linearity feature.\n\n\nUneven data range of attributes can influence the PCA results, therefore, standardize your data before applying PCA.\n\n\n\n\n\n\nExample\nPython‚Äôs library scikit-learn has numerous inbuilt functions for dimensionality reduction. In this example, we will see how to use that function. First, we need to import the packages\n\n\n\n# For loading iris dataset\nfrom sklearn import datasets\n\n# For standardizing our data\nfrom sklearn.preprocessing import StandardScaler\n\n# For PCA\nfrom sklearn import decomposition\n\n\n# Load your dataset here\niris = datasets.load_iris()\n\n# In the next step, we will standardize our data.&lt;/p&gt;\n\n# Creating Standard Scaler\nscaler = StandardScaler()\n\n# Fitting iris data to a scaler\nscaler.fit(iris)\n\n# Transform the data into standardised form\nnew_iris = scaler.transform(iris)\n\n# Create PCA object\npca = decomposition.PCA(n_components=3)\n\n# Fitting data to PCA\npca.fit(new_iris)\n\n# Computing new dimensions\ndr_iris = pca.transform(new_iris)\nLet‚Äôs check now how much variance offered by new dimensions. There is an attribute of PCA class in sklearn library: explained_variance_ratio_ which offer this information.\nprint(PCA.explained_variance_ratio_)\n# Output\n# array([0.72962445, 0.22850762, 0.03668922])&lt;/code&gt;&lt;/pre&gt;\nAs it can be seen that first two principal components offered in total around 94% variance o f original data.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/cotrack.html#project-overview",
    "href": "projects/cotrack.html#project-overview",
    "title": "Collaboration prediction in real-time",
    "section": "Project overview",
    "text": "Project overview\nThis project is culmination of my PhD work. The project‚Äôs goal was to allow teachers to monitor group‚Äôs activities in their classrooms where students collborating face-to-face using some digital technology."
  },
  {
    "objectID": "projects/cotrack.html#best-demo-award-at-lak23-conference-tx-usa",
    "href": "projects/cotrack.html#best-demo-award-at-lak23-conference-tx-usa",
    "title": "Collaboration prediction in real-time",
    "section": "üèÜ Best Demo Award at LAK‚Äô23 Conference, TX, USA",
    "text": "üèÜ Best Demo Award at LAK‚Äô23 Conference, TX, USA\nThe app recieved Best Demonstration Award at the prestigious Learning Analytics & Knowledge conference organized in USA in 2023.\n\nResearch Paper\nThe findings from the usability analysis of the app were published in Learning Analytics & Knowledge conference organized at Kyoto, Japan in 2024. You can access the paper here\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nDeveloped django app to create collaborative learning activities using Etherpad\nIntegrated Google Speech-to-Text API to convert audio into speech data in real-time\nAdded a recording component to record audio and video data of the participant for post-hoc analysis\nIntegrated collaboration prediction component for real-time prediction of collaboration quality of the group"
  },
  {
    "objectID": "projects/cotrack.html#skills-applied",
    "href": "projects/cotrack.html#skills-applied",
    "title": "Collaboration prediction in real-time",
    "section": "Skills Applied",
    "text": "Skills Applied\nPython Programming, Web Development, DevOps, MLOps,"
  },
  {
    "objectID": "projects/cotrack.html#libraries-used",
    "href": "projects/cotrack.html#libraries-used",
    "title": "Collaboration prediction in real-time",
    "section": "Libraries Used",
    "text": "Libraries Used\ndjango,pandas,matplotlib,numpy,scikit-learn"
  },
  {
    "objectID": "projects/rpi.html#project-overview",
    "href": "projects/rpi.html#project-overview",
    "title": "Group Conversation Analytics using Raspberry Pi",
    "section": "Project overview",
    "text": "Project overview\nDeveloped during my first-year PhD research, this system captures and visualizes speaking dynamics in face-to-face group conversations. The interactive dashboard provides participants with real-time feedback about their conversation patterns, enabling more balanced and productive discussions.\n\nTechnical Implementation\nThe solution combines a Raspberry Pi with a ReSpeaker microphone array to create an edge-computing prototype that processes vocal interactions in real time. The system captures two key dimensions of group communication:\n\nVocal Activity Detection\n\nAnalyzes voice energy signals to identify speaking segments\n\n\n\nSpatial Audio Analysis\n\nCalculates direction-of-arrival (DoA) using beamforming techniques\nMaps speakers to physical positions around the discussion table\nTracks speaking turns and participant engagement\n\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nDeveloped a Python library for processing timestamped directional audio data.\nBuilt an interactive dashboard (web-based) to visualize speaking patterns."
  },
  {
    "objectID": "projects/rpi.html#skills-applied",
    "href": "projects/rpi.html#skills-applied",
    "title": "Group Conversation Analytics using Raspberry Pi",
    "section": "Skills Applied",
    "text": "Skills Applied\nPython, Raspberry Pi, Signal Processing, Web Development, IoT Prototyping"
  },
  {
    "objectID": "projects/rpi.html#libraries-used",
    "href": "projects/rpi.html#libraries-used",
    "title": "Group Conversation Analytics using Raspberry Pi",
    "section": "Libraries Used",
    "text": "Libraries Used\npandas,matplotlib,networkx,numpy, dash"
  },
  {
    "objectID": "projects/rpi.html#prototype-demo",
    "href": "projects/rpi.html#prototype-demo",
    "title": "Group Conversation Analytics using Raspberry Pi",
    "section": "Prototype Demo",
    "text": "Prototype Demo"
  },
  {
    "objectID": "projects/rpi.html#dashboard",
    "href": "projects/rpi.html#dashboard",
    "title": "Group Conversation Analytics using Raspberry Pi",
    "section": "Dashboard",
    "text": "Dashboard"
  },
  {
    "objectID": "projects/genome.html#project-overview",
    "href": "projects/genome.html#project-overview",
    "title": "Genome sequencing data processing pipelines",
    "section": "Project overview",
    "text": "Project overview\nThis project goal was to establish genome sequencing data processing pipelines to process raw sequencing reads (i.e., FASTQ reads) into microbacterial compositional data. It was a part of a larger project aiming at building a machine learning classifier for colorectal cancer using fecal sample."
  },
  {
    "objectID": "projects/genome.html#key-features",
    "href": "projects/genome.html#key-features",
    "title": "Genome sequencing data processing pipelines",
    "section": "Key Features",
    "text": "Key Features\n\nEnd-to-End Automation: Fully automated processing from raw sequencing data to analytical results\nReproducible Workflows: Pipeline built with Snakemake for reproducible bioinformatics analysis\nComprehensive Analysis: Produces taxonomic profiles and microbial composition data\nIntegration Ready: Output formatted for downstream machine learning applications\nMachine Learning Modeling: Microbial compositional data analyzed using LASSO model to predict colorectal cancer.\n\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nDeveloped a fully automated upstream pipeline and downstream pipeline featuring\n\nAutomated quality control and adapter trimming\nTaxonomic classification using QIIME2\nGeneration of microbial abundance profiles\nExploratory analysis of microbial abundance profiles\nBuilding of a LASSO model for colorectal cancer detection"
  },
  {
    "objectID": "projects/genome.html#skills-applied",
    "href": "projects/genome.html#skills-applied",
    "title": "Genome sequencing data processing pipelines",
    "section": "Skills Applied",
    "text": "Skills Applied\nPython, R, Signal Processing"
  },
  {
    "objectID": "projects/genome.html#toollibraries-used",
    "href": "projects/genome.html#toollibraries-used",
    "title": "Genome sequencing data processing pipelines",
    "section": "Tool/Libraries Used",
    "text": "Tool/Libraries Used\npandas,matplotlib,ggplot2,qiime2,snakemake"
  },
  {
    "objectID": "projects/bayesian.html#project-overview",
    "href": "projects/bayesian.html#project-overview",
    "title": "Bayesian modeling of students‚Äô mathematics skills",
    "section": "Project overview",
    "text": "Project overview\nThis Django project analyzes students‚Äô interaction data stored in xAPI statements from the Vara learning platform. The dataset captures various key learning features including:\nNumber of attempts per activity\nHint usage patterns\nScore achievement\nTime spent on activities\nThe system also incorporates a mapping of each learning activity to specific mathematics skills aligned with a 9th Standard Algebra curriculum.\n\nTechnical Implementation\nThe solution combines a processing pipeline and dashboards for teachers and students.\n\nGithub source code\nDocumentation\n\n\n\nResearch Paper (Poster)\nSome parts of this project‚Äôs results are presented in a Poster paper at the presitigious conference of Learning Analytics & Knowledge organized in Dublin, Ireland in 2025. You can access the paper here\n\n\n\n\n\n\n‚ö° Key Contributions\n\n\n\n\nPreprocessed interaton data for student modeling\nDeveloped open learner models for students using Bayesian Inference\nBuilt dashboards to visualize learner model at individual and class levels"
  },
  {
    "objectID": "projects/bayesian.html#skills-applied",
    "href": "projects/bayesian.html#skills-applied",
    "title": "Bayesian modeling of students‚Äô mathematics skills",
    "section": "Skills Applied",
    "text": "Skills Applied\nPython Programming, Web Development, DevOps, Bayesian Modeling"
  },
  {
    "objectID": "projects/bayesian.html#libraries-used",
    "href": "projects/bayesian.html#libraries-used",
    "title": "Bayesian modeling of students‚Äô mathematics skills",
    "section": "Libraries Used",
    "text": "Libraries Used\npgmpy,pandas,matplotlib,numpy,dash"
  },
  {
    "objectID": "articles/2022/frontiers.html",
    "href": "articles/2022/frontiers.html",
    "title": "Teacher AI-Supported Pedagogical Actions in Collaborative Learning Coregulation: A Wizard-of-Oz Study",
    "section": "",
    "text": "Kasepalu, R., Prieto, L. P., Ley, T., & Chejara, P. (2022, February). Teacher artificial intelligence-supported pedagogical actions in collaborative learning coregulation: A wizard-of-oz study. In Frontiers in Education (Vol. 7, p.¬†736194). Frontiers Media SA."
  },
  {
    "objectID": "articles/2022/frontiers.html#citation-apa-7",
    "href": "articles/2022/frontiers.html#citation-apa-7",
    "title": "Teacher AI-Supported Pedagogical Actions in Collaborative Learning Coregulation: A Wizard-of-Oz Study",
    "section": "",
    "text": "Kasepalu, R., Prieto, L. P., Ley, T., & Chejara, P. (2022, February). Teacher artificial intelligence-supported pedagogical actions in collaborative learning coregulation: A wizard-of-oz study. In Frontiers in Education (Vol. 7, p.¬†736194). Frontiers Media SA."
  },
  {
    "objectID": "articles/2022/frontiers.html#abstract",
    "href": "articles/2022/frontiers.html#abstract",
    "title": "Teacher AI-Supported Pedagogical Actions in Collaborative Learning Coregulation: A Wizard-of-Oz Study",
    "section": "Abstract",
    "text": "Abstract\nOrchestrating collaborative learning (CL) is difficult for teachers as it involves being aware of multiple simultaneous classroom events and intervening when needed. Artificial intelligence (AI) technology might support the teachers‚Äô pedagogical actions during CL by helping detect students in need and providing suggestions for intervention. This would be resulting in AI and teacher co-orchestrating CL; the effectiveness of which, however, is still in question. This study explores whether having an AI assistant helping the teacher in orchestrating a CL classroom is understandable for the teacher and if it affects the teachers‚Äô pedagogical actions, understanding and strategies of coregulation. Twenty in-service teachers were interviewed using a Wizard-of-Oz protocol. Teachers were asked to identify problems during the CL of groups of students (shown as videos), proposed how they would intervene, and later received (and evaluated) the pedagogical actions suggested by an AI assistant. Our mixed-methods analysis showed that the teachers found the AI assistant useful. Moreover, in multiple cases the teachers started employing the pedagogical actions the AI assistant had introduced to them. Furthermore, an increased number of coregulation methods were employed. Our analysis also explores the extent to which teachers‚Äô expertise is associated with their understanding of coregulation, e.g., less experienced teachers did not see coregulation as part of a teacher‚Äôs responsibility, while more experienced teachers did."
  },
  {
    "objectID": "articles/2022/cscl.html",
    "href": "articles/2022/cscl.html",
    "title": "Co-designing a multimodal dashboard for collaborative analytics",
    "section": "",
    "text": "Chejara, P., Kasepalu, R., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Prieto, L. P. (2022). Co-Designing a Multimodal Dashboard for Collaborative Analytics. In Proceedings of the 15th International Conference on Computer-Supported Collaborative Learning (CSCL) (pp.¬†577-578). International Society of the Learning Sciences."
  },
  {
    "objectID": "articles/2022/cscl.html#citation-apa-7",
    "href": "articles/2022/cscl.html#citation-apa-7",
    "title": "Co-designing a multimodal dashboard for collaborative analytics",
    "section": "",
    "text": "Chejara, P., Kasepalu, R., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Prieto, L. P. (2022). Co-Designing a Multimodal Dashboard for Collaborative Analytics. In Proceedings of the 15th International Conference on Computer-Supported Collaborative Learning (CSCL) (pp.¬†577-578). International Society of the Learning Sciences."
  },
  {
    "objectID": "articles/2022/cscl.html#abstract",
    "href": "articles/2022/cscl.html#abstract",
    "title": "Co-designing a multimodal dashboard for collaborative analytics",
    "section": "Abstract",
    "text": "Abstract\nThe understanding of collaboration quality is crucial for teachers to become aware of the activities going on in the groups and also for identifying groups in need to offer support in CSCL (Computer-Supported Collaborative Learning). Multimodal data captured during CSCL activity in the classroom can facilitate a holistic understanding of collaboration behavior. However, part of the problem is an adequate graphical representation of multimodal data that can help teachers to understand the collaboration and its related sub-constructs (e.g., argumentation). This aspect has been scarcely investigated in CSCL research using multimodal data. This paper presents a study with 58 participants co-designing a dashboard using multimodal data to aid teachers monitoring collaboration and supporting students in the classroom. According to our findings, teachers‚Äô preferences include: abstract representation over quantitative measures (e.g., showing the group‚Äôs written contribution with a pen icon and an amount of writing as size of pen), quantitative details on request, and being notified when problems are identified."
  },
  {
    "objectID": "articles/2025/lak_poster.html",
    "href": "articles/2025/lak_poster.html",
    "title": "Supporting Student-Centered Learning with Flexible Learning Trajectories and Open Learner Models",
    "section": "",
    "text": "Chejara, P., Tammets, K., Laanpere, M., Volt, A., Tammets, P., & Savitski, P. (2025). Supporting Student-Centered Learning with Flexible Learning Trajectories and Open Learner Models. Companion Proceedings 15th International Conference on Learning Analytics & Knowledge. https://doi.org/10.13140/RG.2.2.34284.68489"
  },
  {
    "objectID": "articles/2025/lak_poster.html#citation-apa-7",
    "href": "articles/2025/lak_poster.html#citation-apa-7",
    "title": "Supporting Student-Centered Learning with Flexible Learning Trajectories and Open Learner Models",
    "section": "",
    "text": "Chejara, P., Tammets, K., Laanpere, M., Volt, A., Tammets, P., & Savitski, P. (2025). Supporting Student-Centered Learning with Flexible Learning Trajectories and Open Learner Models. Companion Proceedings 15th International Conference on Learning Analytics & Knowledge. https://doi.org/10.13140/RG.2.2.34284.68489"
  },
  {
    "objectID": "articles/2025/lak_poster.html#abstract",
    "href": "articles/2025/lak_poster.html#abstract",
    "title": "Supporting Student-Centered Learning with Flexible Learning Trajectories and Open Learner Models",
    "section": "Abstract",
    "text": "Abstract\nFlexible learning caters to each individual‚Äôs learning needs, offering a solution to provide personalized learning while preserving the student‚Äôs agency. However, effective utilization of this approach requires students to have some basic understanding of dependencies among learning contents, and the ability to track and assess their learning (i.e., metacognitive skill). Our research aims to address these challenges with a novel approach built upon an expert-designed domain model, flexible instructional trajectories, and open learner models (OLMs) to support students in flexible learning. We present our ongoing research work on leveraging log data to construct OLMs to provide meaningful insights into students‚Äô learning processe"
  },
  {
    "objectID": "articles/2024/jla.html",
    "href": "articles/2024/jla.html",
    "title": "Impact of data noise on the performance of supervised machine learning models using multimodal data to estimate collaboration quality",
    "section": "",
    "text": "Chejara, P., Prieto, L. P. ., Dimitriadis, Y., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., Kasepalu, R. ., & Shankar, S. K. . (2024). The Impact of Attribute Noise on the Automated Estimation of Collaboration Quality Using Multimodal Learning Analytics in Authentic Classrooms. Journal of Learning Analytics, 11(2), 73-90. https://doi.org/10.18608/jla.2024.8253"
  },
  {
    "objectID": "articles/2024/jla.html#citation-apa-7",
    "href": "articles/2024/jla.html#citation-apa-7",
    "title": "Impact of data noise on the performance of supervised machine learning models using multimodal data to estimate collaboration quality",
    "section": "",
    "text": "Chejara, P., Prieto, L. P. ., Dimitriadis, Y., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., Kasepalu, R. ., & Shankar, S. K. . (2024). The Impact of Attribute Noise on the Automated Estimation of Collaboration Quality Using Multimodal Learning Analytics in Authentic Classrooms. Journal of Learning Analytics, 11(2), 73-90. https://doi.org/10.18608/jla.2024.8253"
  },
  {
    "objectID": "articles/2024/jla.html#abstract",
    "href": "articles/2024/jla.html#abstract",
    "title": "Impact of data noise on the performance of supervised machine learning models using multimodal data to estimate collaboration quality",
    "section": "Abstract",
    "text": "Abstract\nMultimodal learning analytics (MMLA) research has shown the feasibility of building automated models of collaboration quality using artificial intelligence (AI) techniques (e.g., supervised machine learning (ML)), thus enablingthe development of monitoring and guiding tools for computer-supported collaborative learning (CSCL). However, the practical applicability and performance of these automated models in authentic settings remains largely an under-researched area. In such settings, the quality of data features or attributes is often affected by noise, which is referred to as attribute noise. This paper undertakes a systematic exploration of the impact of attribute noise on the performance of different collaboration-quality estimation models. Moreover, we also perform a comparative analysis of different ML algorithms in terms of their capability of dealing with attribute noise. We employ four ML algorithms that have often been used for collaboration-quality estimation tasks due to their high performance: random forest, naive Bayes, decision tree, and AdaBoost. Our results show that random forest and decision tree outperformed other algorithms for collaboration-quality estimation tasks in the presence of attribute noise. The study contributes to the MMLA (and learning analytics (LA) in general) and CSCL fields by illustrating how attribute noise impacts collaboration-quality model performance and which ML algorithms seem to be more robust to noise and thus more likely to perform well in authentic settings. Our research outcomes offer guidance to fellow researchers and developers of (MM)LA systems employing AI techniques with multimodal data to model collaboration-related constructs in authentic classroom settings."
  },
  {
    "objectID": "articles/2023/jcal.html",
    "href": "articles/2023/jcal.html",
    "title": "Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‚Äêbased learning analytics",
    "section": "",
    "text": "Ley, T., Tammets, K., Pishtari, G., Chejara, P., Kasepalu, R., Khalil, M., ‚Ä¶ & Wasson, B. (2023). Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‚Äêbased learning analytics. Journal of Computer Assisted Learning, 39(5), 1397-1417."
  },
  {
    "objectID": "articles/2023/jcal.html#citation-apa-7",
    "href": "articles/2023/jcal.html#citation-apa-7",
    "title": "Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‚Äêbased learning analytics",
    "section": "",
    "text": "Ley, T., Tammets, K., Pishtari, G., Chejara, P., Kasepalu, R., Khalil, M., ‚Ä¶ & Wasson, B. (2023). Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‚Äêbased learning analytics. Journal of Computer Assisted Learning, 39(5), 1397-1417."
  },
  {
    "objectID": "articles/2023/jcal.html#abstract",
    "href": "articles/2023/jcal.html#abstract",
    "title": "Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‚Äêbased learning analytics",
    "section": "Abstract",
    "text": "Abstract\nWith increased use of artificial intelligence in the classroom, there is now a need to better understand the complementarity of intelligent learning technology and teachers to produce effective instruction. Objective. The paper reviews the current research on intelligent learning technology designed to make models of student learning and instruction transparent to teachers, an area we call model-based learning analytics. We intended to gain an insight into the coupling between the knowledge models that underpin the intelligent system and the knowledge used by teachers in their classroom decision making."
  },
  {
    "objectID": "articles/2023/jucs.html",
    "href": "articles/2023/jucs.html",
    "title": "CIMLA: A Modular and Modifiable Data Preparation, Organization, and Fusion Infrastructure to Partially Support the Development of Context-aware MMLA Solutions.",
    "section": "",
    "text": "Shankar, S. K., Ruiz-Calleja, A., Prieto, L. P., Rodr√≠guez-Triana, M. J., Chejara, P., & Tripathi, S. (2023). CIMLA: A Modular and Modifiable Data Preparation, Organization, and Fusion Infrastructure to Partially Support the Development of Context-aware MMLA Solutions. JUCS: Journal of Universal Computer Science, (3)."
  },
  {
    "objectID": "articles/2023/jucs.html#citation-apa-7",
    "href": "articles/2023/jucs.html#citation-apa-7",
    "title": "CIMLA: A Modular and Modifiable Data Preparation, Organization, and Fusion Infrastructure to Partially Support the Development of Context-aware MMLA Solutions.",
    "section": "",
    "text": "Shankar, S. K., Ruiz-Calleja, A., Prieto, L. P., Rodr√≠guez-Triana, M. J., Chejara, P., & Tripathi, S. (2023). CIMLA: A Modular and Modifiable Data Preparation, Organization, and Fusion Infrastructure to Partially Support the Development of Context-aware MMLA Solutions. JUCS: Journal of Universal Computer Science, (3)."
  },
  {
    "objectID": "articles/2023/jucs.html#abstract",
    "href": "articles/2023/jucs.html#abstract",
    "title": "CIMLA: A Modular and Modifiable Data Preparation, Organization, and Fusion Infrastructure to Partially Support the Development of Context-aware MMLA Solutions.",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics (MMLA) solutions aim to provide a more holistic\npicture of a learning situation by processing multimodal educational data. Considering contex- tual information of a learning situation is known to help in providing more relevant outputs to\neducational stakeholders. However, most of the MMLA solutions are still in prototyping phase and dealing with different dimensions of an authentic MMLA situation that involve multiple cross-disciplinary stakeholders like teachers, researchers, and developers. One of the reasons behind still being in prototyping phase of the development lifecycle is related to the challenges that software developers face at different levels in developing context-aware MMLA solutions. In this paper, we identify the requirements and propose a data infrastructure called CIMLA. It includes different data processing components following a standard data processing pipeline and considers contextual information following a data structure. It has been evaluated in three authentic MMLA scenarios involving different cross-disciplinary stakeholders following the Software Architecture Analysis Method. Its fitness was analyzed in each of the three scenarios and developers were interviewed to assess whether it meets functional and non-functional requirements. Results showed that CIMLA supports modularity in developing context-aware MMLA solutions and each of its modules can be reused with required modifications in the development of other solutions. In the future, the current involvement of a developer in customizing the configuration file to consider contextual information can be investigated."
  },
  {
    "objectID": "articles/2023/bjet.html",
    "href": "articles/2023/bjet.html",
    "title": "Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., Kasepalu, R., Chounta, I. A., & Schneider, B. (2023, August). Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics. In European Conference on Technology Enhanced Learning (pp.¬†60-74). Cham: Springer Nature Switzerland."
  },
  {
    "objectID": "articles/2023/bjet.html#citation-apa-7",
    "href": "articles/2023/bjet.html#citation-apa-7",
    "title": "Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Rodr√≠guez-Triana, M. J., Ruiz-Calleja, A., Kasepalu, R., Chounta, I. A., & Schneider, B. (2023, August). Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics. In European Conference on Technology Enhanced Learning (pp.¬†60-74). Cham: Springer Nature Switzerland."
  },
  {
    "objectID": "articles/2023/bjet.html#abstract",
    "href": "articles/2023/bjet.html#abstract",
    "title": "Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics researchers have explored relationships between collaboration quality and multimodal data. However, the current state-of-art research works have scarcely investigated authentic settings and seldom used video data that can offer rich behavioral information. In this paper, we present our findings on potential indicators for collaboration quality and its underlying dimensions such as argumentation, and mutual understanding. We collected multimodal data (namely, video and logs) from 4 Estonian classrooms during authentic computer-supported collaborative learning activities. Our results show that vertical head movement (looking up and down) and mouth region features could be used as potential indicators for collaboration quality and its aforementioned dimensions. Also, our results from clustering provide indications of the potential of video data for identifying different levels of collaboration quality (e.g., high, low, medium). The findings have implications for building collaboration quality monitoring and guiding systems for authentic classroom settings."
  },
  {
    "objectID": "articles/2023/lak1.html",
    "href": "articles/2023/lak1.html",
    "title": "How to build more generalizable models for collaboration quality? Lessons learned from exploring multi-contexts audio-log datasets using Multimodal Learning Analytics",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., Kasepalu, R. & Shankar, S. K., (2023). How to build more generalizable models for collaboration quality? Lessons learned from exploring multi-contexts audio-log datasets using multimodal learning analytics. In the 13th International Learning Analytics and Knowledge Conference (LAK23) (pp.¬†111-121). ACM."
  },
  {
    "objectID": "articles/2023/lak1.html#citation-apa-7",
    "href": "articles/2023/lak1.html#citation-apa-7",
    "title": "How to build more generalizable models for collaboration quality? Lessons learned from exploring multi-contexts audio-log datasets using Multimodal Learning Analytics",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., Kasepalu, R. & Shankar, S. K., (2023). How to build more generalizable models for collaboration quality? Lessons learned from exploring multi-contexts audio-log datasets using multimodal learning analytics. In the 13th International Learning Analytics and Knowledge Conference (LAK23) (pp.¬†111-121). ACM."
  },
  {
    "objectID": "articles/2023/lak1.html#abstract",
    "href": "articles/2023/lak1.html#abstract",
    "title": "How to build more generalizable models for collaboration quality? Lessons learned from exploring multi-contexts audio-log datasets using Multimodal Learning Analytics",
    "section": "Abstract",
    "text": "Abstract\nMultimodal learning analytics (MMLA) research for building collaboration quality estimation models has shown significant progress. However, the generalizability of such models is seldom addressed. In this paper, we address this gap by systematically evaluating the across-context generalizability of collaboration quality models developed using a typical MMLA pipeline. This paper further presents a methodology to explore modelling pipelines with different configurations to improve the generalizability of the model. We collected 11 multimodal datasets (audio and log data) from face-to-face collaborative learning activities in six different classrooms with five different subject teachers. Our results showed that the models developed using the often-employed MMLA pipeline degraded in terms of Kappa from Fair (.20 &lt; Kappa &lt; .40) to Poor (Kappa &lt; .20) when evaluated across contexts. This degradation in performance was significantly ameliorated with pipelines that emerged as high-performing from our exploration of 32 pipelines. Furthermore, our exploration of pipelines provided statistical evidence that often-overlooked contextual data features improve the generalizability of a collaboration quality model. With these findings, we make recommendations for the modelling pipeline which can potentially help other researchers in achieving better generalizability in their collaboration quality estimation models."
  },
  {
    "objectID": "articles/2021/ectel.html",
    "href": "articles/2021/ectel.html",
    "title": "Exploring the triangulation of dimensionality reduction when interpreting multimodal learning data from authentic settings",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Shankar, S. K. (2019, Sept). Exploring the triangulation of dimensionality reduction when interpreting multimodal learning data from authentic settings. In the 14th European Conference on Technology Enhanced Learning (EC-TEL) (pp.¬†664-667). Springer, Cham. https://doi.org/10.1007/978-3-030-29736-7_62"
  },
  {
    "objectID": "articles/2021/ectel.html#citation-apa-7",
    "href": "articles/2021/ectel.html#citation-apa-7",
    "title": "Exploring the triangulation of dimensionality reduction when interpreting multimodal learning data from authentic settings",
    "section": "",
    "text": "Chejara, P., Prieto, L. P., Ruiz-Calleja, A., Rodr√≠guez-Triana, M. J., & Shankar, S. K. (2019, Sept). Exploring the triangulation of dimensionality reduction when interpreting multimodal learning data from authentic settings. In the 14th European Conference on Technology Enhanced Learning (EC-TEL) (pp.¬†664-667). Springer, Cham. https://doi.org/10.1007/978-3-030-29736-7_62"
  },
  {
    "objectID": "articles/2021/ectel.html#abstract",
    "href": "articles/2021/ectel.html#abstract",
    "title": "Exploring the triangulation of dimensionality reduction when interpreting multimodal learning data from authentic settings",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics (MMLA) has sparked researcher interest in investigating learning in real-world settings by capturing learning traces from multiple sources of data. Though multimodal data offers a more holistic picture of learning, its inherent complexity makes it difficult to understand and interpret. This paper illustrates the use of dimensionality reduction (DR) to find a simple representation of multimodal learning data collected from co-located collaboration in authentic settings. We employed multiple DR methods and used triangulation to interpret their result which in turn provided a more simplistic representation. Additionally, we also show how unexpected events in authentic settings (e.g., missing data) can affect the analysis results."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Generating and Performing Automated Test for Snakemake Workflow\n\n\n\n\n\n\n\n\nMay 30, 2025\n\n\nPankaj Chejara\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Data Anonymization\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\nPankaj Chejara\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nWriting your first Snakemake rule: A quick startup guide for Snakemake\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nPankaj Chejara\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nA primer on discrete data distribution fundamentals\n\n\n\n\n\n\n\n\nNov 7, 2024\n\n\nPankaj Chejara\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\nHow to Successfully host your Dash application on Render\n\n\n\n\n\n\n\n\nJun 1, 2024\n\n\nPankaj Chejara\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nGuide to train word embeddings from scratch\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nPankaj Chejara\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to word embeddings with hands-on exercises\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nPankaj Chejara\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding a surname classifier using PyTorch\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nPankaj Chejara\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nHands-on experience with llama2 with Python: Building a simple Q&A app\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nPankaj Chejara\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nSentiment prediction using Pytorch\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nPankaj Chejara\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nBasics of Natural Language Processing in Python\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nPankaj Chejara\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Regular Expressions\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nPankaj Chejara\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nLocation data visualization on Map using Python\n\n\n\n\n\n\n\n\nSep 23, 2023\n\n\nPankaj Chejara\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python Dash Framework\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nPankaj Chejara\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nComputing inter-rater aggrement scores using Python\n\n\n\n\n\n\n\n\nMay 21, 2023\n\n\nPankaj Chejara\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Gramian Angular Field\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nPankaj Chejara\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nR tutorial on accessing, filtering, aggregating and plotting data\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\nPankaj Chejara\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nSetting up MQTT server and Client\n\n\n\n\n\n\n\n\nJan 13, 2022\n\n\nPankaj Chejara\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nStatistical tests in R\n\n\n\n\n\n\n\n\nJun 22, 2021\n\n\nPankaj Chejara\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python‚Äôs Pandas API\n\n\n\n\n\n\n\n\nMay 17, 2021\n\n\nPankaj Chejara\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Principal Component Analysis under the hood\n\n\n\n\n\n\n\n\nApr 22, 2020\n\n\nPankaj Chejara\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nFacial feature extraction using OpenFace\n\n\n\n\n\n\n\n\nMar 28, 2020\n\n\nPankaj Chejara\n\n2 min\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "talks/seminar.html",
    "href": "talks/seminar.html",
    "title": "Classroom collaboration analytics",
    "section": "",
    "text": "I gave this talk (remote) in a joint Learning Analytics course run by KTH Royal Institute of Technology and University of Bergen coordinated by Prof.¬†Daniel Spikol. In this talk, I discussed approaches for building automated models for predicting collaboration quality.\nPresentation slides:  Slides\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/lak25.html",
    "href": "talks/lak25.html",
    "title": "Teacher-AI complementarity: From design to implementation and reflection",
    "section": "",
    "text": "This talk was at the Hybrid Intelligence: Human-AI Collaboration and Learning workshop at 15th International Conference of Learning Analytics & Knowledge organized by Trinity college, Dublin, Ireland. The workshop was attended by distinguised professors, leading researchers and PhD students from Learning Analytics research field.\nIn this talk, I presented three technological AI tools from Tallinn University as cases of human-AI hybrid intelligence from a complementarity point of view.\nYou can read more about the presented work here:  Paper.\nPresentation slides:  Slides"
  },
  {
    "objectID": "talks/lak25.html#abstract",
    "href": "talks/lak25.html#abstract",
    "title": "Teacher-AI complementarity: From design to implementation and reflection",
    "section": "Abstract",
    "text": "Abstract\nTraditionally Artificial Intelligence (AI) was primarily focused on building adaptive tutoring systems that mimicked the role of teachers to deliver personalized instruction. Over time, AI applications have expanded to other domains, such as drop-out prediction and performance analytics, with a central goal of understanding and enhancing learning. This expansion has driven the growth of research fields like Educational data mining, Learning Analytics, and AI in Education. These fields have illustrated the potential of AI harnessing data from learning platforms and even from physical classroom spaces. Thus, AI can help teachers to efficiently observe and understand what is happening in their classrooms, augmenting the teacher‚Äôs ability to maximize positive impact on learning. One emerging approach to achieving this synergy between humans and AI is hybrid intelligence, which emphasizes the collaboration and co-evolution of humans and AI. In this paper, we present our ongoing research efforts to design and develop educational technologies with an ability to evolve and adapt from their interactions with teachers and students, and align with human values and norms."
  },
  {
    "objectID": "talks/bergen.html",
    "href": "talks/bergen.html",
    "title": "Building an automated system to detect collaboration quality using multimodal learning analytics",
    "section": "",
    "text": "This talk was given during my research stay at the SLATE research group at the University of Bergen, Norway. The talk presented research work on building automated systems to support teachers during collaborative learning activities in classrooms.\nPresentation slides:  Slides\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/ectel.html",
    "href": "talks/ectel.html",
    "title": "Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics",
    "section": "",
    "text": "This talk was given at the European conference of technology-enhanced learning organized by the University of Aveiro, Portugal. In this talk, I presented results from a research study exploring relationships between multimodal data (audio, video, and logs) and collaboration quality (and its dimensions as such argumentation). The study identified several key indicators (e.g., verticle head movement) for collaboration quality and its dimensions.\nYou can read more about the presented work here:  Paper.\nPresentation slides:  Slides"
  },
  {
    "objectID": "talks/ectel.html#abstract",
    "href": "talks/ectel.html#abstract",
    "title": "Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics",
    "section": "Abstract",
    "text": "Abstract\nMultimodal Learning Analytics researchers have explored re- lationships between collaboration quality and multimodal data. How- ever, the current state-of-art research works have scarcely investigated authentic settings and seldom used video data that can offer rich be- havioral information. In this paper, we present our findings on potential indicators for collaboration quality and its underlying dimensions such as argumentation, and mutual understanding. We collected multimodal data (namely, video and logs) from 4 Estonian classrooms during au- thentic computer-supported collaborative learning activities. Our results show that vertical head movement (looking up and down) and mouth region features could be used as potential indicators for collaboration quality and its aforementioned dimensions. Also, our results from clus- tering provide indications of the potential of video data for identifying different levels of collaboration quality (e.g., high, low, medium). The findings have implications for building collaboration quality monitoring and guiding systems for authentic classroom settings."
  },
  {
    "objectID": "talks/spain.html",
    "href": "talks/spain.html",
    "title": "Automated estimation of collaboration quality using multimodal data in classroom",
    "section": "",
    "text": "This talk was given during my research stay at GSIC-EMIC research lab at the University of Valladolid, Spain to introduce my research on Multimodal Learning Analytics. The talk was attended by most of the lab members including Prof.¬†Yannis Dimitriadis, Prof.¬†Alejandra Mart√≠nez Mon√©s, and Prof.¬†Juan Ignacio Asensio P√©rez.\nIn this talk, I presented my research work on building automated models for collaboration quality estimation using multimodal learning analytics. The objective was to identify research areas for collaboration with the lab members.\nPresentations slides:   Slides \n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The following projects focus on utilizing the power of analytics to support teaching and learning experience.\n\n\nDemo | Publication | Source code\nPython, Dash\n\nA Raspberry Pi-based prototype to capture audio data from face-to-face group activity\nAnalyzed the direction of arrival of captured audio to compute speaking time and turn-taking using python\nDeveloped an interactive dashboard for data visualization\n\n\n\n\n\nüèÜ Best Demo Award at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nDemo | Publication | Source code\nPython , jQuery, MySQL, Etherpad, plotly, Google Speech-to-Text\n\nPython Django app to collect multimodal learning data from group activities in classrooms\nVoice activity detection to compute speaking time of individual in group activities in real-time\nGoogle Speech-To-Text integration to process audio in real-time\nA real-time dashboard visualizing group‚Äôs writing behavior, speaking participation, and speech content (in the form of word cloud)\n\n\n\n\n\n\n‚≠ê Honorable mention at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nPublication | Source code\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio and log data gathered from Estonian classrooms during group activities\nExplored use of different temporal window size (30s, 60s, 90s, 120s, 180s, 240s) to process features such as speaking time, turn-taking\nDeveloped machine learning models to predict collaboration quality using audio and log data features processed with different window sizes\n\n\n\n\n\n\nPublication\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio, video and log data to develop machine learning models to predict collaboration quality in classroom settings during group activity\nUsed Random Forest algorithm to develop model and evaluated its generalizability with a different dataset collected from a different Estonian school"
  },
  {
    "objectID": "projects.html#edtech-projects",
    "href": "projects.html#edtech-projects",
    "title": "Projects",
    "section": "",
    "text": "The following projects focus on utilizing the power of analytics to support teaching and learning experience.\n\n\nDemo | Publication | Source code\nPython, Dash\n\nA Raspberry Pi-based prototype to capture audio data from face-to-face group activity\nAnalyzed the direction of arrival of captured audio to compute speaking time and turn-taking using python\nDeveloped an interactive dashboard for data visualization\n\n\n\n\n\nüèÜ Best Demo Award at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nDemo | Publication | Source code\nPython , jQuery, MySQL, Etherpad, plotly, Google Speech-to-Text\n\nPython Django app to collect multimodal learning data from group activities in classrooms\nVoice activity detection to compute speaking time of individual in group activities in real-time\nGoogle Speech-To-Text integration to process audio in real-time\nA real-time dashboard visualizing group‚Äôs writing behavior, speaking participation, and speech content (in the form of word cloud)\n\n\n\n\n\n\n‚≠ê Honorable mention at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nPublication | Source code\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio and log data gathered from Estonian classrooms during group activities\nExplored use of different temporal window size (30s, 60s, 90s, 120s, 180s, 240s) to process features such as speaking time, turn-taking\nDeveloped machine learning models to predict collaboration quality using audio and log data features processed with different window sizes\n\n\n\n\n\n\nPublication\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio, video and log data to develop machine learning models to predict collaboration quality in classroom settings during group activity\nUsed Random Forest algorithm to develop model and evaluated its generalizability with a different dataset collected from a different Estonian school"
  },
  {
    "objectID": "projects.html#web-development-projects",
    "href": "projects.html#web-development-projects",
    "title": "Projects",
    "section": "Web-Development Projects",
    "text": "Web-Development Projects\nI worked on the following projects as part of my web-developer role at Tallinn University.\n\nTruestedUx\nWebsite\nPython , Django, jQuery, Bootstrap\n\nAn application which allows assessment of trust aspect of technology usage.\nDeveloped the complete front-end part and the most of back-end of the website.\n\n\n\nTinda\nWebsite\nDrupal , jQuery, Highcharts\n\nAn application which allows assessment of digital competencies of professionals.\nAdded an module to download the responses of users to questionnaires on digital competencies following the DigiComp Framework.\nAdded a dashboard visualizing users‚Äô responses and their overall score for digital competencies.\nAdded a report download functionality.\n\n\n\nSeeds\nWebsite\nPython , Django, jQuery, Bootstrap, Plotly\n\nA map-based application to allows users to filter energy transition scenario based on their preferences.\nHandled the entire development of the project including front-end and back-end."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "PhD in Learning Analytics | Aug 2018 - May 2024  Tallinn University | Tallinn, Estonia\nMTech | Aug 2010 - June 2012  Malviya National Institute of Technology | Jaipur, India\nMCA | Aug 2007 - May 2010  Malviya National Institute of Technology | Jaipur, India\nBSc (Mathematics) | Aug 2004 - May 2007  University of Rajasthan | Jaipur, India"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "PhD in Learning Analytics | Aug 2018 - May 2024  Tallinn University | Tallinn, Estonia\nMTech | Aug 2010 - June 2012  Malviya National Institute of Technology | Jaipur, India\nMCA | Aug 2007 - May 2010  Malviya National Institute of Technology | Jaipur, India\nBSc (Mathematics) | Aug 2004 - May 2007  University of Rajasthan | Jaipur, India"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nMetrosert AS, Estonia  Data Scientist | Sep 2024 - present\n\nApplying analytical and research knoweldge in the field of bioinformatics (genome analysis)\nContributing in the preparation of research proposals together with health care industries\n\nTallinn University, Estonia  Developer/Researcher | Sep 2019 - present\n\nBuilding interactive data visualization prototypes for educational research projects\nBuilding machine learning models for detecting collaboration levels\nBayesian modeling of learning logs for building open learner models"
  },
  {
    "objectID": "about.html#other-academic-research-experience",
    "href": "about.html#other-academic-research-experience",
    "title": "About",
    "section": "Other academic research experience",
    "text": "Other academic research experience\nHarvard University, USA  Visiting Researcher | Mar 2023 - May 2023\nUniversity of Bergen, Norway  Visiting Researcher | Jun 2022 - Jul 2022\nUniversity of Valladolid, Spain  Visiting Researcher | Jan 2022 - May 2022"
  }
]