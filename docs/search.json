[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Pankaj Chejara is a junior researcher cum web developer at the Center of Education Technology at Tallinn University, Estonia. His work is primarily focused on building AI tools to support teachers in the classroom with group work monitoring. He enjoys diving into data to extract insights and is always ready to learn new things to solve the problem at hand. When not working on data analytics projects, Pankaj enjoys spending time with his son and watching animated movies."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nPhD in Learning Analytics (Thesis Submitted) | Sept 2018 - Nov 2023 Tallinn University, Tallinn | Tallinn, Estonia\nMaster in Computer Engg. (MTech) | Aug 2010 - June 2012  Malviya National Institute of Technology | Jaipur, India\nMaster in Computer Applications (MCA) | Aug 2007 - May 2010  Malviya National Institute of Technology | Jaipur, India\nBachelor in Science (Mathematics) | Aug 2004 - May 2007  University of Rajasthan | Jaipur, India"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nWeb Developer | Tallinn University, Estonia | Sep 2019 - present Assistant Professor | Sharda University, India | Sep 2012 - 2016"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pankaj Chejara",
    "section": "",
    "text": "Introduction to Python Dash Framework\n\n\n5 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nJun 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python‚Äôs Pandas API\n\n\n9 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nMay 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Principal Component Analysis under the hood\n\n\n9 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nApr 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFacial feature extraction using OpenFace\n\n\n2 min\n\n\n\n\n\n\nPankaj Chejara\n\n\nMar 28, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/dash.html",
    "href": "posts/post-with-code/dash.html",
    "title": "Introduction to Python Dash Framework",
    "section": "",
    "text": "Dashboards play a crucial role in conveying useful and actionable information from collected data regardless of context. As with economically feasible sensors, improved computation, storage facility, and IoT framework, it has become easier to collect an enormous amount of data from a variety of contexts (e.g.¬†military, health-care, education). However, finding insights from collected data remains a challenge. This post offers an introduction to Python Dash a framework that is a great option for dashboard development. I personally really like this framework. It allows me to process my data along with its visualization through a web-based application."
  },
  {
    "objectID": "posts/post-with-code/dash.html#dash-framework",
    "href": "posts/post-with-code/dash.html#dash-framework",
    "title": "Introduction to Python Dash Framework",
    "section": "Dash Framework",
    "text": "Dash Framework\ndash uses plotly graphics library for plotting graphs. In addition to their graphic feature, these graphs also have a lot of good features (e.g.¬†automatic scaling of axis for timestamp data, etc). Please refer dash documentation for details.\n\nInstallation\npip install dash-html-components==0.14.0  # HTML components\npip install dash-core-components==0.44.0  # dash core components\npip install dash-table==3.6.0  # Interactive DataTable component (new!)\n\n\nFirst dash application\nNow, we are going to develop our first dash application. For this application, we are going to plot the following data (some records are taken from here. This data has two attributes (House price and area).\nA dash application can have number of components (e.g.¬†div block, table, headings, graph). In our application, we are going to use two components - heading and a graph. Let‚Äôs begin developing it. First of all, we need to import the required packages\nimport dash_core_components as dcc\nimport dash_html_components as html\nThe first package is used to create an object of dash application. Second package dash_core_components provides graph components and the third package allows us to include html components (heading, input box) in our application.\nNext, we need to create a dash application.\napp = dash.Dash(__name__)\nname is a special variable in python which contains name of current module. For instance, when you run your python program using command prompt then it contains main.\nNow, we will create components to embed in our application. Just to have a clear idea, we want to create following structured with our application.\n\n\n First Dash Application \n\n graph here.. \n\n\nFor components, we will use dash_html_components and for graph, we will use dash_core_components.\nlist_of_data = [{\n    'y':[114300,114200,114800,94700,119800,114600],\n    'x':[1790,2030,1740,1980,2130,1780],\n    'type':'bar'\n}]\n\nbody = html.Div([\n    html.H2(\"\"First Dash Application\"\"),\n    dcc.Graph(id='first',\n        figure={'data':list_of_data})\n])\nIn the above code, we created the body of our application which is a div block. In this block, we created one H2 heading component and one Graph component. Graph has a attribute figure where we have specified the data to be plotted. The data (list_of_data) is actually a list of dictionaries (It might seems a bit confusing but you will be good after writing some codes). One important thing-&gt; we used ‚Äòtype‚Äô:‚Äòbar‚Äô which specify that we want to plot Bar chart.\nNext, we will set the layout of the application.\napp.layout = html.Div([body])\nFinally, we will start the server\nif __name__ == \"\"__main__\"\":\n    app.run_server(debug=True)\nYou can download this script from here.\nNow, we will execute our application. The execution of our application will start a server on the port 8050\n\n\n\nRunning the server\n\n\nIn order to see the output of the program, open your browser and type http://127.0.0.1:8050 in the address bar. It will show you the following screen\n\n\n\np6.3.png\n\n\nCheck for more components: dash_core_components, dash_html_components.\n\n\nAdding CSS style to the app\nThe next step towards generating a beautiful dashboard is to add a styling feature. We can use css stylesheet in our application. It can be specified at the time of creating an application.\napp = dash.app = dash.Dash(__name__,external_stylesheets=style)\nWith the above method, you can only add css which are available online. In case if you want to add your local css file, follow the given steps\n\nCreate a folder with name asset in the same directory where your dash script is stored.\nCopy your stylesheet in the asset folder.\nAdd the path in your program\n\ndash.Dash(__name__,external_stylesheets=[\"\"\\asset\\css-file-name\"\"])\n\n\nInstalling Bootstrap component for dash\ndash also supports Bootstrap which is a widely used css library. It can be added in your dash application using dash-bootstrap-component package (complete documentation). This package allows an easy integration of Bootstrap in the dash application.\nYou can install it using the following command.\npip install dash-bootstrap-components\nNow, let‚Äôs use it to add a CSS to our first example. We are going to create the following layout for our application. We will utilize Bootstrap‚Äôs grid system for structuring our components.\n\n\n\nboot.png\n\n\nFirst, we need to import dash_bootstrap_components in our previous example.\nNext, we will add bootstrap css to your program and then we will create our layout.\nimport dash-bootstrap-components as dbc\napp = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# column1 content\ncolumn_1 = [\n    html.H2(\"\"House Prices\"\"),\n    html.P(\"\"This is a demo application which shows house prices with house area.\"\"\n    ),\n    dbc.Button(\"\"Details\"\", color=\"\"secondary\"\"),\n]\n\n# column content\ncolumn_2 = [\n    html.H2(\"\"Graph\"\"),\n    dcc.Graph(id='first',\n        figure={'data':list_of_data}),\n]\n\n# Creating layout\nbody = dbc.Container(\n    [   html.H1('With Bootstrap'),\n        html.Hr(),\n        dbc.Row(\n            [\n                dbc.Col(column_1,md=4),\n                dbc.Col(column_2,md=8),\n            ]\n        )\n    ]\n)\n\n# Adding CSS\n# dash-bootstrap-components has CDN for bootstrap css in dbc.themes.BOOTSTRAP\napp = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP])\nYou can chek the above source code here."
  },
  {
    "objectID": "posts/post-with-code/pandas.html",
    "href": "posts/post-with-code/pandas.html",
    "title": "Introduction to Python‚Äôs Pandas API",
    "section": "",
    "text": "Pandas is a Python API for processing data in a easy and efficient way. This post offers an introduction to this amazing API , especially for beginners.\nThe post starts with installation instructions of the API and then introduces its functionality with the help of examples."
  },
  {
    "objectID": "posts/post-with-code/pandas.html#installation",
    "href": "posts/post-with-code/pandas.html#installation",
    "title": "Introduction to Python‚Äôs Pandas API",
    "section": "Installation",
    "text": "Installation\n\n\nThere are two options to install the Pandas API on your system. The first option is to install it through Anaconda Distribution which comes with all essential Python packages. While the second option is to install Python on the system and then install Pandas package using the following command. \n\npip install pandas"
  },
  {
    "objectID": "posts/post-with-code/pandas.html#introduction",
    "href": "posts/post-with-code/pandas.html#introduction",
    "title": "Introduction to Python‚Äôs Pandas API",
    "section": "Introduction",
    "text": "Introduction\nNow we will explore the API basics. Basically, we will cover the following topics which I think will be good enough to start using Pandas API.\n\n\n\n\n\nPandas DataFrame and Series objects\n\n\nReading data CSV files and accessing basic information\n\n\nQuerying data using loc() and iloc() function\n\n\nHandling missing data\n\n\nAdding, deleting columns or rows\n\n\n\n\n\nPandas DataFrame and Series objects\nPandas‚Äô Series is a one-dimensional array representation of values. we can understand it as an attribute in a dataset. For instance, consider a dataset with three attributes sid (student-id), name, and marks. Now, each of these attributes in pandas is represented as a Series object.\n\n\nLet‚Äôs write a program to create these three Series objects sid, name, and marks.\n\nimport pandas as pd\n\n# Creating Series using list\nid = pd.Series([101,102,103,104,105])\n\nname = pd.Series(['pradeep','manoj','kiran','pushpendra','sambit']\n\nmarks = pd.Series([20,30,40,32,28])&lt;/code&gt;&lt;/pre&gt;\nThe first line import pandas as pd imports the pandas package in wer program. Next, three lines create three Series objects with a given list.\n\nPandas‚Äô DataFrame object is a two-dimensional data structure where each attribute is a Series object. we can create a DataFrame using a dictionary of key:value pair where the key represents attribute name and the value represents the values of that attribute.\nLet‚Äôs create a dataframe using the Series objects we created in the above program.\n\ndf = pd.DataFrame({'sid':id,'name':name,'marks':marks})\n\n\nReading data CSV files and accessing basic information\nPandas have a function read__csv() to read CSV format data files. This function comes with multiple useful options which we will learn in this section. The data file used in this tutorial can be downloaded from the link. The name of the downloaded data file is iris_csv.csv. \n\nOpen and read a CSV data file\nimport pandas as pd\ndf=pd.read_csv('iris_csv.csv')\ndf.head()\n\ndf=pd.read_csv(‚Äòiris_csv.csv‚Äô) opens and reads the specified CSV file (here we can specify the name of wer data file). The third line df.head() shows first five records (we can specify the number of records) from wer data file.\n\n\n\nAssign/Rename column names\n\nIn case, if wer data file does not have column names or we want to assign a different column name then we can use the names option of read_csv() function. Example:\n\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv',names=['sep-len','sep-wid','pet-len','pet-wid','class'])\n\n\nReading data file with different seperator\n\nSometimes the data files have columns seperated by other characters (e.g.¬†spaces, colon). In such cases, in order to read the CSV file we need to specify the sep option in the read_csv() function.\n\n# reading file having data seperated by :\ndf = pd.read_csv('data_file',sep=':')\n\n\nSkipping rows while reading data\n\nIn case, if wer data file does not have data records from the first line (let‚Äôs say it contains some summary or description and data records begins from line 4), we can skip those lines by specifying skip rows option.\n\ndf = pd.read_csv('data_file',skiprows=3)\n\n\nAccessing sizes of data\nwe can check the size of wer data set (e.g.¬†number of rows, number of columns) using shape property of the DataFrame.\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv')\nprint(df.shape)\n# output\n# (150, 5)\n\nHere, 150 is the number of rows and 5 is number of columns.\n\n\n\nChecking data types of columns\n\nTo check the data types of columns in the data file, we can use &lt;a href=‚Äú‚Äúhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html‚Äù‚Äú&gt;dtypes property.\n\n\n\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv')\ndf.dtypes&lt;/code&gt;&lt;/pre&gt;\n\nOutput:\n\nsep-len    float64\nsep-wid    float64\npet-len    float64\npet-wid    float64\nclass       object\ndtype: object\n\n\n\n\n\n\n\nTip\n\n\n\nAs the data processing modules requires wer data to be in numeric data types (e.g.¬†int, float) it is best practice to check the data types before processing it.\n\n\n\n\nBasic stats of data\nIf we want to learn about our data in more depth, we can use describe() function. This function provides information about count, minimum, maximum, mean, standard deviation, quartiles for each column. An example is given below.\nimport pandas as pd\ndf = pd.read_csv('iris_csv.csv',skiprows=1,names=['sep-len','sep-wid','pet-len','pet-wid','class'])\ndf.describe()&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\nQuerying data using loc() and iloc() function\nPandas offers two different functions (there is one more ix() which is actually deprecated) for accessing data from the dataframe- .loc() and .iloc(). In these functions, we specify the labels or positions of rows and columns to access data. However, if we do not specify columns selector then by default all columns are accessed.\n\n\n\n\n\n\n\nTip\n\n\n\n: operator used for slicing purpose. It works differently in case of label and position. When applied with labels (start:end), it include end element in the result. However, in case of positions (start:end), it does not include end in the result.\n\n\n\nLet‚Äôs understand the difference between labels and positions. In the following code, we are creating a simple dataframe with two columns sid and name.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\ndf.head()\nAs shown in the figure below, the row labels are (0,1,2,3,4) and column labels are (sid, name). The position for rows and columns begins with 0 that means the first row has position 0, second row has position 1 and so on. \nIn the above example, the rows position and labels are same. To make the difference clear, let‚Äôs try to change the index of our dataframe and then see it. we can change the index of dataframe using set_index() function. In the following example, we are setting the first column sid as the index of our dataset. This function create a copy of dataframe, apply changes it it and then return the updated copy. In order to make changes to dataframe inplace=True parameter needs to be passed.\ndf.set_index('sid',inplace=True)\ndf.head()\n\nAs we can see in the figure below, rows‚Äô indices are (101,102,103,104,105) whereas rows‚Äô positions are the same as previous.\n\n\n\nSome Examples\nThe following figure shows some examples for accessing data with label and position-based selections.\n\n\nTo access a particular row, we need to specify its label or position in the row-selector (for example, we have to specify label 0 to access first row). In case, if we want to access multiple rows, we need to specify their corresponding labels or positions in a list or we can use : operator for slicing (for example, row selector for accessing first three rows can be [0,1,2] or 0:3).\n\n# import pandas package\nimport pandas as pd\n\n# create the dataframe\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# set sid as index\ndf.set_index('sid',inplace=True)\n\n# Access first row\ndf.loc[101]  # lable-based\n\ndf.iloc[0]  #  Position-based\n\n\n# Access first three rows\ndf.loc[101:103]  # label-based\n\ndf.iloc[0:3]  # position-based\n\n\nCondition-based data access\n\n\n\nFunction loc() and iloc() both support condition-based data access using boolean array. Let‚Äôs say we want to access the first row. To do that we need to specify a boolean array for rows selection. This array will contain a boolean value for each row and only one True value.\nIf we want to show a particular set-of rows, we can do that by specifying a boolean array with True values on corresponding location of those rows. Same applies for column selection.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\ndf.loc[[True,False,False,False,False],[True,True]]\n\n#Output\n#      sid     name\n#0     101     pradeep\n\n\nIn the above example, we spcified a boolean array for rows selection and another for columns selection. In the first array, True is specified at the first index (which corresponds to the first row). The second array contains all True(which corresponds to all columns). Hence, we get the values from the first row and both columns.\n\n\n\n\nHandling missing data\nPandas offers a great support to handle missing values. If the dataset has some values missing, Pandas automatically marks them as NaN values. To demonstrate it, I have prepared a CSV file with three columns- eid, name, salary.\n\nIn this file, I intentionally kept the salary field for the third record empty for the following exercises on the missing values.\nNow let‚Äôs read this file using pandas.\ndf=pd.read_csv('emp.csv')\ndf.head()\n\nOutput:\n\n\n\nWe can handle missing values in two ways: delete or replace. The following sections discusses these both ways.\n\n\nDeleting missing values\n\n\nWe can use the dropna() function to delete missing records.\nIn this function, we need to specify axis=0 if we want to delete the row/rows having NaN and for deleting column/columns having NaN specify axis=1. \n\ndf=pd.read_csv('emp.csv')\n\n#delete row\ndf1 = df.dropna(axis=0)\ndf1.head()\n\n#delete column\ndf1 = df.dropna(axis=1)\ndf1.head()&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\n\n\n\nTip\n\n\n\nTo change the original dataframe, specify inplace=True in the dropna() function.\n\n\n\n\n\nFilling missing values\n\n\n\nFunction fillna is useful in filling missing values in the dataframe.\n\ndf=pd.read_csv('emp.csv')\n\n#fill with 0\ndf1 = df.fillna(0)\ndf1.head()\n\n#fill using forward fill method\ndf2 = df.fillna(method='ffill')\ndf2.head()\n\n# fill using backward fill method\ndf3 = df.fillna(method='bfill')\ndf3.head()\n\n# fill using mean value of column\ndf4 = df.fillna(df['salary'].mean())\ndf4.head()\n\n\n\n\n\n\n\nTip\n\n\n\nffill replaces NaN with the previous value in the same column. While, bfill replaces NaN with the next value in the same column (order of values top to bottom).\n\n\n\n\n\nAdd or delete row/column\n\nThis section will show we how to add a new row or column to an already existing dataframe.\n\n\nAdding row/column\nWe can simply add a row using append() or loc()/iloc() function. We can use key:value pair in the append function, where the key is the attribute name and the value represents the value we want to add. Pandas automatically puts NaN if some attributes values are not provided.\nNow, let‚Äôs add a record with sid as 106 and name as ‚Äògaurav‚Äô.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# using the append function\ndf = df.append(append({'sid':106,'name':'gaurav'},ignore_index=True)\nprint(df)\n\n# Adding a column\ndf['marks'] = [20,30,40,32,28,50]\nThe same record can also be added using df.loc[5]=[106,‚Äògaurav‚Äô].\n\n\nDeleting row/column\n\n\n\nTo delete some columns or rows, the drop function can be used. In this function, we need to specify the label of row or column we want to delete. In case, it is a column then we also need to pass a parameter axis=1.\nThe following example illustrates the use of drop function.\n\nimport pandas as pd\ndf = pd.DataFrame({'sid':[101,102,103,104,105],'name':['pradeep','manoj','kiran','pushpendra','sambit']})\n\n# delete sid column\ndf.drop('sid',axis=1)"
  },
  {
    "objectID": "posts/post-with-code/facial-features.html",
    "href": "posts/post-with-code/facial-features.html",
    "title": "Facial feature extraction using OpenFace",
    "section": "",
    "text": "In this post, I will discuss the work I have been doing recently. I needed to extract facial features from the recorded video and for this task, I decided to use OpenFace, an open-source face recognition library. In this post, I am sharing the installation process and tutorial on detecting facial landmarks.\n\n\nInstallation\nI tried to install OpenFace on Mac OS but couldn‚Äôt succeed. There were a lot of errors and compatibility issues that I couldn‚Äôt get through. Therefore, I decided to install it on Ubuntu. For that, I installed a Virtual box on Mac and installed Ubuntu 18.04.\nTo install OpenFace, I followed the steps given here\nsudo apt-get update\nsudo apt-get install build-essential\nsudo apt-get install g++-8\n\nsudo apt-get install cmake\n\nsudo apt-get install git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\n\nsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-22-dev\n\nwget https://github.com/opencv/opencv/archive/4.1.0.zip\n\nsudo unzip 4.1.0.zip\ncd opencv-4.1.0\nmkdir build\ncd build\n\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_TIFF=ON -D WITH_TBB=ON ..\nmake -j2\nsudo make install\n\nwget http://dlib.net/files/dlib-19.13.tar.bz2\ntar xf dlib-19.13.tar.bz2\ncd dlib-19.13\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release\nsudo make install\nsudo ldconfig\ncd ../..\n\nsudo apt-get install libboost-all-dev\n\ngit clone https://github.com/TadasBaltrusaitis/OpenFace.git\n\ncd OpenFace\nmkdir build\ncd build\n\ncmake -D CMAKE_CXX_COMPILER=g++-8 -D CMAKE_C_COMPILER=gcc-8 -D CMAKE_BUILD_TYPE=RELEASE ..\nmake\nAt this point, we have installed OpenFace. Now we need to download models. You can either download it manually or use a script provided in the OpenFace library.\nManual download links.\n\nscale 0.25\nscale 0.35\nscale 0.50\nscale 1.00\n\nI ran the following command to ran the script to download the model.\ncd ..\nsh ./download_models.sh\nThe script will download models in the directory OpenFace/lib/local/LandmarkDetector/model/patch_experts.\nAfter completing this process, I ran the demo program by running the following command.\n./bin/FaceLandmarkVid -f \"\"../samples/changeLighting.wmv\"\" -f \"\"../samples/2015-10-15-15-14.avi\n\n\n\n\n\n\nExecution error\n\n\n\nI got an error CEN patch expert not found. The command was searching the models in the OpenFace/build/bin/model/patch_experts.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nI copied the files (cen_patches_0.25_of.dat,cen_patches_0.35_of.dat,cen_patches_0.50_of.dat,cen_patches_1.00_of.dat) in OpenFace/build/bin/model/patch_experts directory.\n\n\n\n\nRunning Demo\nIf you have a video with a single face, you can use FaceLandmarkVid or in case of multiple faces, you can use FaceLandmarkVidMulti\n\n\n\n\n\n\nNote\n\n\n\nThese files will be available in OpenFace/build/bin directory. Either you can specify the full path to facial landmark detector or cd to the bin directory and run the following command.\n\n\nFaceLandmarkVid -f file_name\nFollowing is the demonstration of OpenFace on a video clip with single face.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/concepts/pca.html",
    "href": "posts/concepts/pca.html",
    "title": "Understanding Principal Component Analysis under the hood",
    "section": "",
    "text": "Let‚Äôs assume that you have a dataset with a higher number of attributes and you are thinking Is there any way to compress this information into a smaller number of attributes.\nWell, dimensionality reduction methods offer that functionality. Among many dimensionality reduction methods, PCA (Principal Component Analysis) is widely used and this post introduction to PCA and its working.\nLet‚Äôs start with a simple dataset with two attributes, \\(x\\), and \\(y\\) which we need to reduce from two attributes to one. \n\n\n\n\n\n\n$x$\n$y$\n\n\n10\n5\n\n\n12\n5\n\n\n16\n6\n\n\n20\n7\n\n\n19\n5\n\n\n\n\n\n\n\n\n\nThe simplest approach is to select one of the attributes. Let‚Äôs say we are selecting \\(x\\). In other words, we are taking the projection of the data points on X-axis (Projection: simply drawing a perpendicular line from data points to the x-axis and taking that corresponding value). For example, in the following diagram, we have taken the projection of data points along the x-axis.\nSo here we have two options, we can take the projection of data points to either x-axis or y-axis. Our aim is to select one which offers more information about the data. To measure it, we can use variance which computes spread of data or in other words how the values are different from their mean value. So if you have an attribute with zero variance that means all values in that attribute are same. Variance can be computed using the following formula \\[ \\sigma^2 = \\sum\\limits_{i=1}^N (X -\\mu)^2 \\] where: \\(X\\) is the set of data points \\(\\mu\\) is the mean \\(N\\) is the number of data points in \\(X\\)\n\n\n\n\nSo, now if we look at our options (projection along x-axis or y-axis) then we find x-axis as a better option due to the larger variance shown in below figure\n\n\n\n\n\n\n\n\n\n\n\nDo we really have only these two options?\n\n\n\nNo, there are infinite number of possibilities (How: draw any line and take the projection of data points on that line in a similar manner as we did with x/y axis). In this case, if we can find a line which gives maximum variance of the data compared to other possible options then it can be used at the place of \\(x\\) and \\(y\\) attributes.\n\n\n\nPrincipal Component Analysis (PCA)\n\nAs we discussed in the above section the infinite possibilities for our two-dimensional data, PCA finds the one which offers maximum variance. Let‚Äôs go deeper into the mathematics of PCA.\n\n\n\n\nUnderstanding Mathematics behind PCA\n\n\nFirst, we are going to write the problem statement (finding a direction/vector/line which offers a maximum variance of projected data) of PCA into mathematics format. First, we need to see how to represent the projection. A projection of a data point along with a line can be computed using \\(dot\\) product. Let‚Äôs say we want to compute the projection of the first data point (10,5) on the x-axis. Let‚Äôs represent our data point as a vector \\(\\vec{x_1}\\). A vector has two properties- direction and magnitude (more info). The unit vector in the direction of x-axis and y-axis is represented by \\(\\hat{i}\\) and \\(\\hat{j}\\), respectively. Every vector then denoted by number of units in the direction of x-axis and y-axis. Our data point (10,5) can be represented as 10 units in x-axis direction and 5 units in y-axis direction. The dot product between \\(\\vec{x_1}\\) and \\(\\hat{i}\\) will give the projection over the x-axis.\n\n\n\n\n\\[= (10\\hat{i},5\\hat{j}) . (\\hat{i},0)\\]\n\n\n\n\n\\[= 10\\]\n\n\n\n\n\n\n\nIn simpler terms, if you have two vectors or list of numbers \\((a_1,a_2,a_3)\\) and\\((b_1,b_2,b_3)\\) then their dot product will be \\(a_1*b_1+a_2*b_2+a_3*b_3\\). It can be written in matrix form as following \\[\n      dot(A,B)=\\begin{bmatrix}\n      a_1 & a_2 & a_3\\\n      \\end{bmatrix}\\begin{bmatrix}\n      b_1 \\\\\n      b_2 \\\\\n      b_3 \\\\\n      \\end{bmatrix}\n     = A^TB\n  \\] here, \\(A^T\\) is transpose of \\(A\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nA dot product of \\(\\vec{A}\\) with its own gives you \\(A^2\\).\n\n\n\\[\n  A^2 = a_1^2+a_2^2+a_3^2 &lt;br/&gt;\n  = a_1*a_1+a_2*a_2+a_3*a_3\n  = A^TA\n  \\]\n\nNow coming back to PCA problem statement, let‚Äôs denote a direction (/vector/line) \\(\\vec{L}\\). So projection of our dataset \\(X\\) can be written as \\[\nX_{new}=X^TL\n\\] here, \\(X_{new}\\) represents the new values obtained after projection of \\(X\\) over \\(L\\). To ease the understanding of next step, let‚Äôs assume we transformed \\(X_{new}\\) in such a way that it has zero mean (we can do that by simply replacing every value in \\(X_{new}\\) by \\(X_{new}-Mean)\\). Next, we compute the variance of \\(X_{new}\\).\n\\[\nvar(X_{new}) = (X_{new}-0)^2 \\\\\n= X_{new}^2 \\\\\n= X_{new}^TX_{new}\n= (X^TL)^T(X^TL)\n= (L^TXX^TL)\n= (L^T  \\sum L)\n\\]\n\nhere, \\(\\sum\\) is covariance matrix of \\(X_{new}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nRule used: \\((AB)^T = B^TA^T\\) and \\((A^T)^T = A\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe add a constraint that L must be a unit vector which means \\(L^TL=1\\).\nWhy: In a single direction there can be infinite possibilities for example \\(2\\hat{i}+1\\hat{j}\\), \\(4\\hat{i}+2\\hat{j}\\), \\(\\hat{i}+.5\\hat{j}\\) all vectors are in the same direction. Therefore, to avoid it, we put a constraint that we will check only a single vector in a direction and that one with unit vector (to avoid large values).\n\n\nNow, our problem is to find \\(L\\) which maximizes \\(var(X_{new})\\) with constraint \\(L^TL=1\\) (constrain of unit vector). (also known as constrained maximization problem)\nTo formulate this problem, we will use Lagrange Multiplier. Our problem can be written as follows \\[\n\\max_{L} = (L^T\\sum L)-\\lambda(L^TL-1)\n\\]\n\nTo solve it, we will differentiate it with respect to \\(L\\) and then equate it to zero. As we seen above that \\(L^TL=L^2\\), therefore, differentiating it gives us \\(2L\\). \\[\n\\sum L - \\lambda (L) = 0\n\\]\n\n\n\n\n\\[ \\sum L = \\lambda L \\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above equation is actually stating that \\(L\\) must be eigenvector of covariance matrix of \\(X\\).\nA brief about eigen vector\n\n\n\n\n\nLet‚Äôs consider a matrix as a system of transformation. When it is multiplied with a vector that vector gets transformed into a new vector. Let‚Äôs take an example. We have a matrix \\[ M =\n    \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\] We will multiply (or apply) it on a vector \\(A = [2,3]\\). The multiplication of \\(M\\) and \\(A\\) will give \\[\n  \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\begin{bmatrix}\n      3 \\\\\n      2  \\\\\n      \\end{bmatrix}\n  = \\begin{bmatrix}\n      -2 \\\\\n      -5 \\\\\n      \\end{bmatrix}\n  \\] As it‚Äôs shown in the below figure that point (3,2) transformed into new point (-2,-5). Now, if we multiply \\(B = [1,1]\\) with M then we will get result in the same direction. \\[\n  \\begin{bmatrix}\n      2 & -4 \\\\\n      -1 &  -1 \\\\\n      \\end{bmatrix}\n  \\begin{bmatrix}\n      1 \\\\\n      1  \\\\\n      \\end{bmatrix}\n  = \\begin{bmatrix}\n      -2 \\\\\n      -1 \\\\\n      \\end{bmatrix}\n  = -2\\begin{bmatrix}\n      1 \\\\\n      1 \\\\\n      \\end{bmatrix}\n  \\]\nTherefore vector \\(B\\) is eigen vector for the matrix in above example.\n\n\n\n\nComing back to our last equation \\(\\sum L = \\lambda L\\), now the question is which eigenvector (as there can be many eigen vectors of a matrix) to use. Let‚Äôs have a look on our variance \\((L^T\\sum L)\\) which we want to maximize.\n\n\\[\n= (L^T\\sum L)\n= (L^T\\lambda L)\n= \\lambda(L^T L)\n= \\lambda\n\\] \n\n\n\n\n\n\nTip\n\n\n\nSo, to maximize the variance, we need to take eigenvector with maximum eigen value (\\(\\lambda\\)). Here is our first vector, one with highest Eigen value.\n\n\nUsing this result as a basis, we then have the following steps for PCA\n\n\n\n\n\nCompute Covariance matrix (\\(\\sum\\)) of \\(X\\)\n\n\nSubtract \\(X_\\mu\\) (mean) from \\(X\\)\n\n\nCompute eigenvector \\(\\sum\\)\n\n\nReorder the eigen vectors according to their corresponding eigen value\n\n\nProject data set on those eigenvectors beginning from the start (as the first eigenvector has the highest eigenvalue, second eigenvector with second-highest, and so on)\n\n\n\n\n\nKey points\n\n\n\n\n\nPCA is a unsupvervised machine learning algorithm.\n\n\nPCA dimensions are linear combinations of original attributes. Therefore, it is a linear DR method. However, there are variants of PCA (e.g.¬†kernelPCA) which offers non-linearity feature.\n\n\nUneven data range of attributes can influence the PCA results, therefore, standardize your data before applying PCA.\n\n\n\n\n\n\nExample\nPython‚Äôs library scikit-learn has numerous inbuilt functions for dimensionality reduction. In this example, we will see how to use that function. First, we need to import the packages\n\n\n\n# For loading iris dataset\nfrom sklearn import datasets\n\n# For standardizing our data\nfrom sklearn.preprocessing import StandardScaler\n\n# For PCA\nfrom sklearn import decomposition\n\n\n# Load your dataset here\niris = datasets.load_iris()\n\n# In the next step, we will standardize our data.&lt;/p&gt;\n\n# Creating Standard Scaler\nscaler = StandardScaler()\n\n# Fitting iris data to a scaler\nscaler.fit(iris)\n\n# Transform the data into standardised form\nnew_iris = scaler.transform(iris)\n\n# Create PCA object\npca = decomposition.PCA(n_components=3)\n\n# Fitting data to PCA\npca.fit(new_iris)\n\n# Computing new dimensions\ndr_iris = pca.transform(new_iris)\nLet‚Äôs check now how much variance offered by new dimensions. There is an attribute of PCA class in sklearn library: explained_variance_ratio_ which offer this information.\nprint(PCA.explained_variance_ratio_)\n# Output\n# array([0.72962445, 0.22850762, 0.03668922])&lt;/code&gt;&lt;/pre&gt;\nAs it can be seen that first two principal components offered in total around 94% variance o f original data.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "The following projects focus on utilizing the power of analytics to support teaching and learning experience.\n\n\nDemo | Publication | Source code\nPython, Dash\n\nA Raspberry Pi-based prototype to capture audio data from face-to-face group activity\nAnalyzed the direction of arrival of captured audio to compute speaking time and turn-taking using python\nDeveloped an interactive dashboard for data visualization\n\n\n\n\n\nüèÜ Best Demo Award at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nDemo | Publication | Source code\nPython , jQuery, MySQL, Etherpad, plotly, Google Speech-to-Text\n\nPython Django app to collect multimodal learning data from group activities in classrooms\nVoice activity detection to compute speaking time of individual in group activities in real-time\nGoogle Speech-To-Text integration to process audio in real-time\nA real-time dashboard visualizing group‚Äôs writing behavior, speaking participation, and speech content (in the form of word cloud)\n\n\n\n\n\n\nPublication | Source code\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio and log data gathered from Estonian classrooms during group activities\nExplored use of different temporal window size (30s, 60s, 90s, 120s, 180s, 240s) to process features such as speaking time, turn-taking\nDeveloped machine learning models to predict collaboration quality using audio and log data features processed with different window sizes\n\n\n\n\n\n\nPublication\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio, video and log data to develop machine learning models to predict collaboration quality in classroom settings during group activity\nUsed Random Forest algorithm to develop model and evaluated its generalizability with a different dataset collected from a different Estonian school"
  },
  {
    "objectID": "Projects.html#edtech-projects",
    "href": "Projects.html#edtech-projects",
    "title": "Projects",
    "section": "",
    "text": "The following projects focus on utilizing the power of analytics to support teaching and learning experience.\n\n\nDemo | Publication | Source code\nPython, Dash\n\nA Raspberry Pi-based prototype to capture audio data from face-to-face group activity\nAnalyzed the direction of arrival of captured audio to compute speaking time and turn-taking using python\nDeveloped an interactive dashboard for data visualization\n\n\n\n\n\nüèÜ Best Demo Award at Learning Analytics and Knowledge (LAK2023) conference, Texas, USA (Core A rank)\nDemo | Publication | Source code\nPython , jQuery, MySQL, Etherpad, plotly, Google Speech-to-Text\n\nPython Django app to collect multimodal learning data from group activities in classrooms\nVoice activity detection to compute speaking time of individual in group activities in real-time\nGoogle Speech-To-Text integration to process audio in real-time\nA real-time dashboard visualizing group‚Äôs writing behavior, speaking participation, and speech content (in the form of word cloud)\n\n\n\n\n\n\nPublication | Source code\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio and log data gathered from Estonian classrooms during group activities\nExplored use of different temporal window size (30s, 60s, 90s, 120s, 180s, 240s) to process features such as speaking time, turn-taking\nDeveloped machine learning models to predict collaboration quality using audio and log data features processed with different window sizes\n\n\n\n\n\n\nPublication\nPython , Scikit-learn, Matplotlib\n\nAnalyzed audio, video and log data to develop machine learning models to predict collaboration quality in classroom settings during group activity\nUsed Random Forest algorithm to develop model and evaluated its generalizability with a different dataset collected from a different Estonian school"
  }
]