{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5299bb77",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Basics of Natural Language Processing in Python\"\n",
    "author: \"Pankaj Chejara\"\n",
    "date: \"2023-10-23\"\n",
    "categories: [python, nlp, nltk]\n",
    "image: \"./images/nlp_basics/wall.jpg\"\n",
    "code-block-background: true\n",
    "highlight-style: \"arrow\"\n",
    "toc: true\n",
    "---\n",
    "\n",
    "\n",
    "Natural language processing is an interdisciplinary field combining computer science, artificial intelligence, and linguistics. Natural language processing focuses on the processing of [natural languages](https://www.thoughtco.com/what-is-a-natural-language-1691422) (i.e., human languages such as English, and Hindi). This processing aims at comprehending natural languages and generating text in those languages.\n",
    "\n",
    "Natural language processing, thus, enables computers to understand and process human languages and presents the enormous potential of building intelligent machines capable of understanding human input in their own languages. \n",
    "\n",
    "In this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial. The post is targeted at beginners who are just starting to gain first-hand experience with natural language processing.\n",
    "# Basics of Natural Language Processing in Python\n",
    "Natural langauge processing enables computer to human languages. It combines linguistic with statistical modeling. \n",
    "In this post, we will become familiar with basics of nlp with Python. We will use NLTK library for the tutorial.\n",
    "\n",
    "Front image by [Andrea De Santis](https://unsplash.com/@santesson89?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash) on Unsplash.\n",
    "\n",
    "\n",
    "## Installation\n",
    "First we need to install nltk library. The following command can be used to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a02da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7bf4b",
   "metadata": {},
   "source": [
    "## Basic concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f238e6c",
   "metadata": {},
   "source": [
    "Now, we will get familiar with the basic concepts of natural language processing. This processing takes place through multiple steps. With each step, a higher level of abstraction is achieved. Let's understand some basic steps first. \n",
    "\n",
    "When a text is preseted to a computer, it only sees the text as a sequence of characters; So, the first step focuses on breaking the character's sequence into sentences, and then each sentence into words. This step is known as **Tokenization**. \n",
    "\n",
    "Next, the sentence structure is understood from a grammatical point of view. This involves identifying nouns, verbs, objects, etc. This step is known as **Parts-of-Speech Tagging**. \n",
    "\n",
    "Once the grammatical relavant tags are identified for each word in the sentence, the next step tries to apply a transformation which replaces words with their base form. For example, transforming `running` into `run`. This step is known as **Stemming/Lemmatization**. We will later discuss the differences between these two.\n",
    "\n",
    "The final step converts the text data into numbers for the computer's usage. This step is known as **Vectorization**. \n",
    "\n",
    "Now we will cover these topics one by one. The list of topics is provided below as well.\n",
    "\n",
    "* Tokenization\n",
    "* Parts of Speech Tagging\n",
    "* Stemming/Lemmatization\n",
    "* Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049729b6",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Let's start with tokenization, the first step in the process. The tokenization step simply breaks down text data into smaller units for analysis purposes such as sentences, words, numbers, etc. These units are also known as tokens. \n",
    "\n",
    "The following program performs tokenization, first breaking the text into a group of sentences, and second, breaking each sentence into a group of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76b2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      " ['This post offers basics of natural langauge processing (NLP) in Python.', 'NLP enables computer to human languages.', 'It combines linguistic with statistical modeling.']\n",
      "Words:\n",
      " [['This', 'post', 'offers', 'basics', 'of', 'natural', 'langauge', 'processing', '(', 'NLP', ')', 'in', 'Python', '.'], ['NLP', 'enables', 'computer', 'to', 'human', 'languages', '.'], ['It', 'combines', 'linguistic', 'with', 'statistical', 'modeling', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"\"\"This post offers basics of natural langauge processing (NLP) in Python. \n",
    "    NLP enables computer to human languages. It combines linguistic with statistical modeling.\n",
    "    \"\"\"\n",
    "\n",
    "# Breaking the text data into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print('Sentences:\\n',sentences)\n",
    "\n",
    "# Breaking each sentece into words\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "print('Words:\\n',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3b5f4",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "    \n",
    "    You need to run nltk.download('punkt') only once.\n",
    "    \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ca4a6",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "Let's move to now understanding grammatical structure of sentences. The step involves identifying whether a word in the sentence is noun, verb, adverb, etc. It is known as **Parts-of-Speech** or **POS tagging**. \n",
    "\n",
    "Let's see our first example of tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69f53a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Estonia', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'JJ'), ('country', 'NN'), ('in', 'IN'), ('digital', 'JJ'), ('space', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "# Uncomment the below statement if you get an error of tagger not found\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# using only word tokenization because there is only one sentence in the text data.\n",
    "words = word_tokenize('Estonia is a leading country in digital space.')\n",
    "\n",
    "# applying parts-of-speech tagging\n",
    "tags = pos_tag(words)\n",
    "\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f53267",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "    You can check the meaning of each tag using ntlk.help.upenn_tagset() function.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9cbb359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# checking the meaning of NN\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb238b",
   "metadata": {},
   "source": [
    "The result is provided in a form of list of tuples. Each tuple contains a word and corresponding word category (e.g., VBZ for verb). This tuple is also known as **tagged token**. You can read about it in more details [here](https://www.nltk.org/book/ch05.html).\n",
    "\n",
    "\n",
    "The complete list is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60907261",
   "metadata": {},
   "source": [
    "|Tag  |Description                            |\n",
    "|-----|---------------------------------------|\n",
    "|CC   |coordinating conjunction               |\n",
    "|CD   | cardinal digit                        |\n",
    "|DT   | determiner                            |\n",
    "|EX   | existential there (like: \"there is\" . think of it like \"there exists\")|\n",
    "|FW   | foreign word                          |\n",
    "|IN   | preposition/subordinating conjunction |\n",
    "|JJ   | adjective 'big'                       |\n",
    "|JJR  | adjective, comparative 'bigger'       |\n",
    "|JJS  | adjective, superlative 'biggest'      |\n",
    "|LS   | list marker                           |\n",
    "|MD   | modal could, will                     |\n",
    "|NN   | noun, singular 'desk'                 |\n",
    "|NNS  | noun plural 'desks'                   |\n",
    "|NNP  | proper noun, singular 'Harrison'      |\n",
    "|NNPS | proper noun, plural 'Americans'       |\n",
    "|PDT  | predeterminer 'all the kids'          |\n",
    "|POS  | possessive ending parent              |\n",
    "|PRP  | personal pronoun I, he, she           |\n",
    "|PRP\\$ | possessive pronoun my, his, hers      |\n",
    "|RB   | adverb very, silently,                |\n",
    "|RBR  | adverb, comparative better            |\n",
    "|RBS  | adverb, superlative best              |\n",
    "|RP   | particle give up                      |\n",
    "|TO,  | to go 'to' the store.                 |\n",
    "|UH   | interjection, errrrrrrrm              |\n",
    "|VB   | verb, base form take                  |\n",
    "|VBD  | verb, past tense took                 |\n",
    "|VBG  | verb, gerund/present participle taking|\n",
    "|VBN  | verb, past participle taken           |\n",
    "|VBP  | verb, sing. present     |\n",
    "|VBZ  | verb, third person sing. present takes  |\n",
    "|WDT  | wh-determiner which                   |\n",
    "|WP   | wh-pronoun who, what                  |\n",
    "|WP\\$  | possessive wh-pronoun whose           |\n",
    "|WRB  | wh-abverb where, when                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3d371",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "This step transforms a word into its root word or base form. For example, car, cars, car's all share a common root word `car`. In the linguistic field, such words are known as words with inflection endings or derivationally related words. There is a [nice post](https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx) which you can refer to understand more about it and also about morphological analysis.\n",
    "\n",
    "Here we are briefly discussing inflection and derivational forms of words.\n",
    "\n",
    "**Inflection forms** \n",
    "These word forms are used to distinct tenses, person, gender, etc. For example, words like go, going, gone, goes. If you notice these words have different endings. These all are called inflection endings in [**linguistic**](https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx) field. \n",
    "\n",
    ":::{.callout-tip}\n",
    "The words with inflection endings do not have a separate entry in the dictionary. You will find all words with inflection endings under a single entry i.e., go.\n",
    ":::\n",
    "\n",
    "**Derivational form**\n",
    "\n",
    "These word forms are derived from the rood words and create a new meaning. For example, re`act` and `act`or  both are derived from the word `act`.\n",
    "\n",
    ":::{.callout-tip}\n",
    "The words with derivational forms have a separate entry in the dictionary. You will find a separate entry in the dictionary for act, react, and actor.\n",
    ":::\n",
    "\n",
    "Now, as we have a preliminary understanding of different forms of words, we next move to the **stemming and lemmatization**. These are two techniques to transform a word from its inflection form (and sometimes derivational form) to the base form.\n",
    "\n",
    "\n",
    "### Stemming\n",
    "**Stemming** is the technique which simply chops off the ending of a word to obtain its base form. For example, removing `ing` from `eating` to obain the base form i.e., `eat`. By default NLTK uses a rule-based stemmer (i.e., Porter Stemmer). There are other stemmer as well. You can check [this page](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) for more information on different stemmers.\n",
    "\n",
    "Let's take a look at the following code which perform stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8d21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:Estonia    Stem:estonia\n",
      "Word:is         Stem:is\n",
      "Word:a          Stem:a\n",
      "Word:leading    Stem:lead\n",
      "Word:country    Stem:countri\n",
      "Word:in         Stem:in\n",
      "Word:digital    Stem:digit\n",
      "Word:space      Stem:space\n",
      "Word:,          Stem:,\n",
      "Word:and        Stem:and\n",
      "Word:on         Stem:on\n",
      "Word:its        Stem:it\n",
      "Word:way        Stem:way\n",
      "Word:to         Stem:to\n",
      "Word:become     Stem:becom\n",
      "Word:the        Stem:the\n",
      "Word:leader     Stem:leader\n",
      "Word:.          Stem:.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Uncomment the following statement if running it first time\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize the text data\n",
    "words = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n",
    "\n",
    "# Printing the data with its base form after stemming operation\n",
    "for word in words:\n",
    "    print('Word:{:10} Stem:{}'.format(word,ps.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3698bff",
   "metadata": {},
   "source": [
    "In the output, we can see that words `leading`, `country`, `digital` are transformed into `lead`, `countri`, and `digit`, respectively. Now, we will move to Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631378b",
   "metadata": {},
   "source": [
    "### Lemmatization \n",
    "Lemmatization is an another technique which also perform a similar task as Stemming i.e., transforming words into its base forms. However, it differs in its approach. Lemmatization uses [morphological analysis](https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx) to achieve the goal. The morphological analysis means understanding of words and their parts. \n",
    "\n",
    "You can refer to this post to gain more information on different [lemmatization approaches](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/?utm_content=cmp-true) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd15c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:Estonia    Lemma:Estonia\n",
      "Word:is         Lemma:is\n",
      "Word:a          Lemma:a\n",
      "Word:leading    Lemma:leading\n",
      "Word:country    Lemma:country\n",
      "Word:in         Lemma:in\n",
      "Word:digital    Lemma:digital\n",
      "Word:space      Lemma:space\n",
      "Word:,          Lemma:,\n",
      "Word:and        Lemma:and\n",
      "Word:on         Lemma:on\n",
      "Word:its        Lemma:it\n",
      "Word:way        Lemma:way\n",
      "Word:to         Lemma:to\n",
      "Word:become     Lemma:become\n",
      "Word:the        Lemma:the\n",
      "Word:leader     Lemma:leader\n",
      "Word:.          Lemma:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/htk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n",
    "\n",
    "for word in words:\n",
    "    print('Word:{:10} Lemma:{} Tag:{}'.format(word,wordnet_lemmatizer.lemmatize(word),wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a4efe",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "# No change after lemmatization\n",
    "If you notice in the results there are no changes after applying lemmatization. The reason is that\n",
    "if the word can not be found in WordNet (publicly awailable English dataset) then the word remain unchanged. It can be corrected by providing the pos tag of the word when calling `lemmatize` function.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbec696",
   "metadata": {},
   "source": [
    "Now, we will supply pos tag of each word when calling `lemmatize` function. However, the function only takes a single character for the pos tag. For example, `n` for nouns, `v` for verbs, `a` for adjectives, and `r` for adverbs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7339ae",
   "metadata": {},
   "source": [
    "So, we need to prepare a mapping which translates pos tags, obtained from `nltk.pos_tag()` function, into `a`, `r`, `n`, `v` (depending on the tag).\n",
    "\n",
    "We know from the POS tags table above that tags for adjectives starts from 'J'. So what we can do is, we can take the first character of pos-tag and determine which tag to supply in lemmatize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35c9555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def get_pos(word):\n",
    "    # the function returns a list with one tagged tuple, e.g., [('riding','VBD')]\n",
    "    tagged_tuple_list = pos_tag([word])\n",
    "    \n",
    "    # fetching the item in the list and then the tag in tuple\n",
    "    tagged_tuple = tagged_tuple_list[0]  # the first index will fetch the fist item in the list\n",
    "    \n",
    "    # fetching the tag from the tagged tuple\n",
    "    tag = tagged_tuple[1]   # the index will fetch the tag (e.g., 'VBD')\n",
    "    \n",
    "    # extracting the first character\n",
    "    tag_char = tag[0]\n",
    "    \n",
    "    # all these three statement can be combined into a single statement given below\n",
    "    # tag_char = pos_tag([word])[0][1][0]\n",
    "    \n",
    "    # Now we will create a mapping\n",
    "    pos_to_lemma_tag = {\n",
    "        'J': 'a',\n",
    "        'N': 'n',\n",
    "        'R': 'r',\n",
    "        'V': 'v'\n",
    "    }\n",
    "    \n",
    "    # we will return the tag for usage in lemmatize function\n",
    "    return pos_to_lemma_tag.get(tag_char,'n')   # get function will return n if the tag is something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d247a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:Estonia    Lemma:Estonia\n",
      "Word:is         Lemma:be\n",
      "Word:a          Lemma:a\n",
      "Word:leading    Lemma:lead\n",
      "Word:country    Lemma:country\n",
      "Word:in         Lemma:in\n",
      "Word:digital    Lemma:digital\n",
      "Word:space      Lemma:space\n",
      "Word:,          Lemma:,\n",
      "Word:and        Lemma:and\n",
      "Word:on         Lemma:on\n",
      "Word:its        Lemma:it\n",
      "Word:way        Lemma:way\n",
      "Word:to         Lemma:to\n",
      "Word:become     Lemma:become\n",
      "Word:the        Lemma:the\n",
      "Word:leader     Lemma:leader\n",
      "Word:.          Lemma:.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n",
    "\n",
    "for word in words:\n",
    "    print('Word:{:10} Lemma:{}'.format(word,\n",
    "                                       wordnet_lemmatizer.lemmatize(word,get_pos(word))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cfdb0",
   "metadata": {},
   "source": [
    "> It worked now like a charm :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92478ad",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Now, we will move to the final step in the aforementioned list of preprocessing steps in natural language processing. This final step is vectorization step. This step translates the text into numbers so that computers can use them for further anlaysis.  \n",
    "\n",
    "There are multiple techinques of vectorization. In this post, we are going to discuss two basics techniques techniques: count vectorization, and tf-idf vectorization.\n",
    "\n",
    "\n",
    "#### Count Vectorization\n",
    "In this technique, the input text is first broken down into a set of unique words, and next, each word is assigned a number which represents the frequency of that word.\n",
    "\n",
    "Let's see a working example. For the example, we will use [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5302a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1, 20) \n",
      " Vector: [[1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# input data\n",
    "input_text = [\"In this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial.\"] \n",
    "\n",
    "# initialization\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# applying count vectorization on the text\n",
    "result = vect.fit_transform(input_text)\n",
    "\n",
    "# printing vocubulary with frequency\n",
    "print('Shape:',result.shape, '\\n Vector:',result.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d836b",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "The result is a single vector of length 20. In out input, we only had a single sentence, therefore, one got a single vector. In case of multiple sentences, we get one vector for each sentence.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3516fac",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "The next vectorization technique is TF-IDF (Term Frequency- Inverse Document Frequency). Let's understand their meaning.\n",
    "<br/>\n",
    "\n",
    "**Term Frequency (TF)**\n",
    "This frequency counts the number of times a word occurs in the document. \n",
    "\n",
    "**Inverse Document Frequency**\n",
    "This is the inverse of how many documents contains the specified term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e4118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 25) \n",
      " Vector: [[0.         0.30520733 0.30520733 0.         0.30520733 0.30520733\n",
      "  0.30520733 0.         0.30520733 0.30520733 0.         0.23211804\n",
      "  0.23211804 0.         0.30520733 0.         0.         0.\n",
      "  0.23211804 0.         0.         0.         0.         0.30520733\n",
      "  0.        ]\n",
      " [0.35955412 0.         0.         0.35955412 0.         0.\n",
      "  0.         0.         0.         0.         0.27345018 0.27345018\n",
      "  0.         0.35955412 0.         0.         0.         0.\n",
      "  0.         0.35955412 0.35955412 0.35955412 0.         0.\n",
      "  0.27345018]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36977238 0.         0.         0.28122142 0.\n",
      "  0.28122142 0.         0.         0.36977238 0.36977238 0.36977238\n",
      "  0.28122142 0.         0.         0.         0.36977238 0.\n",
      "  0.28122142]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# input data\n",
    "input_text = [\"This is an amazing field with huge potential of building intelligent machine.\",\n",
    "              \"Those machine would be able to transform people's life.\",\n",
    "              \"This transformation would significantly improve the quality of life.\"]\n",
    "# TF-IDF intialization\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# applying vectorizer\n",
    "result = tf.fit_transform(input_text)\n",
    "\n",
    "# print results\n",
    "print('Shape:',result.shape, '\\n Vector:',result.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0060585",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can check [this blog post](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp) on further information on vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "References\n",
    "1. https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n",
    "2. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "3. https://www.nltk.org/book/\n",
    "4. Morphological analysis: https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx\n",
    "5. https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "6. https://pianalytix.com/countvectorizer-in-nlp/#:~:text=CountVectorizer%20means%20breaking%20down%20a,data%20needs%20to%20be%20vectorized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
