{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5299bb77",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "---\n",
    "title: \"Basics of Natural Language Processing in Python\"\n",
    "author: \"Pankaj Chejara\"\n",
    "date: \"2023-01-23\"\n",
    "categories: [python, nlp, nltk]\n",
    "image: \"./images/map_vis/map.png\"\n",
    "code-block-background: true\n",
    "highlight-style: \"arrow\"\n",
    "toc: true\n",
    "draft: true\n",
    "---\n",
    "\n",
    "Natural language processing (NLP) is an interdisciplinary field combining computer science, artificial intelligence, and linguistics. Natural language processing focuses on the processing of [natural languages](https://www.thoughtco.com/what-is-a-natural-language-1691422) (i.e., human languages such as English, and Hindi). This processing aims at comprehending natural languages and generating text in those languages.\n",
    "\n",
    "Natural language processing, thus, enables computers to understand and process human languages and presents the enormous potential of building intelligent machines capable of understanding human input in their own languages. \n",
    "\n",
    "In this post, we will become familiar with the basics of natural language processing with Python. We will use the NLTK library for the tutorial. The post is targeted at beginners who are just starting to gain first-hand experience with natural language processing.\n",
=======
    "# Basics of Natural Language Processing in Python\n",
    "Natural langauge processing enables computer to human languages. It combines linguistic with statistical modeling. \n",
    "In this post, we will become familiar with basics of nlp with Python. We will use NLTK library for the tutorial.\n",
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
    "\n",
    "## Installation\n",
    "First we need to install nltk library. The following command can be used to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a02da0e",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "pip install nltk"
=======
    "! pip install nltk"
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7bf4b",
   "metadata": {},
   "source": [
    "## Basic concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f238e6c",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Now, we will get familiar with the basic concepts of natural language processing. This processing takes place through multiple steps. With each step, a higher level of abstraction is achieved. Let's understand some basic steps first. \n",
    "\n",
    "When a text is preseted to a computer, it only sees the text as a sequence of characters; So, the first step focuses on breaking the character's sequence into sentences, and then each sentence into words. This step is known as **Tokenization**. \n",
    "\n",
    "Next, the sentence structure is understood from a grammatical point of view. This involves identifying nouns, verbs, objects, etc. This step is known as **Parts-of-Speech Tagging**. \n",
    "\n",
    "Once the grammatical relavant tags are identified for each word in the sentence, the next step tries to apply a transformation which replaces words with their base form. For example, transforming `running` into `run`. This step is known as **Stemming/Lemmatization**. We will later discuss the differences between these two.\n",
    "\n",
    "The final step converts the text data into numbers for the computer's usage. This step is known as **Vectorization**. \n",
    "\n",
    "Now we will cover these topics one by one. The list of topics is provided below as well.\n",
    "\n",
    "* Tokenization\n",
    "* Parts of Speech Tagging\n",
    "* Stemming/Lemmatization\n",
    "* Vectorization"
=======
    "Now, we will get familiar with basic concepts of nlp. Every nlp application usually takes a large amount of text data to train. This text data, however, is not understandable directly by the computer. To allow a computer to understand text data, the data needs to be converted into numbers. This process is broken down into the following steps\n",
    "\n",
    "* Tokenization\n",
    "* Parts of Speech Tagging\n",
    "* 1-hot Encoding\n",
    "* Word Embeddings"
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049729b6",
   "metadata": {},
   "source": [
    "### Tokenization\n",
<<<<<<< HEAD
    "Let's start with tokenization, the first step in the process. The tokenization step simply breaks down text data into smaller units for analysis purposes such as sentences, words, numbers, etc. These units are also known as tokens. \n",
    "\n",
    "The following program performs tokenization, first breaking the text into a group of sentences, and second, breaking each sentence into a group of words."
=======
    "Let's start with tokenization, the first step in the process. Tokenization step simply breaks down a text data into smaller units, also known as tokens. These tokens could be words, punctuation marks, numbers, etc.\n",
    "\n",
    "\n",
    "Let's take an example to understand this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34dd9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pankaj/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76b2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      " ['This post offers basics of natural langauge processing (NLP) in Python.', 'NLP enables computer to human languages.', 'It combines linguistic with statistical modeling.']\n",
      "Words:\n",
      " [['This', 'post', 'offers', 'basics', 'of', 'natural', 'langauge', 'processing', '(', 'NLP', ')', 'in', 'Python', '.'], ['NLP', 'enables', 'computer', 'to', 'human', 'languages', '.'], ['It', 'combines', 'linguistic', 'with', 'statistical', 'modeling', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
<<<<<<< HEAD
    "# uncomment the following statement if you are running the program first time\n",
    "# nltk.download('punkt')\n",
    "\n",
    "#text data\n",
=======
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
    "text = \"\"\"This post offers basics of natural langauge processing (NLP) in Python. \n",
    "    NLP enables computer to human languages. It combines linguistic with statistical modeling.\n",
    "    \"\"\"\n",
    "\n",
    "# Breaking the text data into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print('Sentences:\\n',sentences)\n",
    "\n",
    "# Breaking each sentece into words\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "print('Words:\\n',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3b5f4",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "    \n",
<<<<<<< HEAD
    "    You need to run nltk.download('punkt') only once.\n",
    "    \n",
=======
    "    If you get an error while running the above code then run the following statement.\n",
    "    \n",
    "    ```\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    ```\n",
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
    ":::"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "cd5ca4a6",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "Let's move to now understanding grammatical structure of sentences. The step involves identifying whether a word in the sentence is noun, verb, adverb, etc. It is known as **Parts-of-Speech** or **POS tagging**. \n",
    "\n",
    "Let's see our first example of tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69f53a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Estonia', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'JJ'), ('country', 'NN'), ('in', 'IN'), ('digital', 'JJ'), ('space', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "# Uncomment the below statement if you get an error of tagger not found\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# using only word tokenization because there is only one sentence in the text data.\n",
    "words = word_tokenize('Estonia is a leading country in digital space.')\n",
    "\n",
    "# applying parts-of-speech tagging\n",
    "tags = pos_tag(words)\n",
    "\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f53267",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "    You can check the meaning of each tag using ntlk.help.upenn_tagset() function.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9cbb359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# checking the meaning of NN\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb238b",
   "metadata": {},
   "source": [
    "The result is provided in a form of list of tuples. Each tuple contains a word and corresponding word category (e.g., VBZ for verb). This tuple is also known as **tagged token**. You can read about it in more details [here](https://www.nltk.org/book/ch05.html).\n",
    "\n",
    "\n",
    "The complete list is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60907261",
   "metadata": {},
   "source": [
    "|Tag  |Description                            |\n",
    "|-----|---------------------------------------|\n",
    "|CC   |coordinating conjunction               |\n",
    "|CD   | cardinal digit                        |\n",
    "|DT   | determiner                            |\n",
    "|EX   | existential there (like: \"there is\" . think of it like \"there exists\")|\n",
    "|FW   | foreign word                          |\n",
    "|IN   | preposition/subordinating conjunction |\n",
    "|JJ   | adjective 'big'                       |\n",
    "|JJR  | adjective, comparative 'bigger'       |\n",
    "|JJS  | adjective, superlative 'biggest'      |\n",
    "|LS   | list marker                           |\n",
    "|MD   | modal could, will                     |\n",
    "|NN   | noun, singular 'desk'                 |\n",
    "|NNS  | noun plural 'desks'                   |\n",
    "|NNP  | proper noun, singular 'Harrison'      |\n",
    "|NNPS | proper noun, plural 'Americans'       |\n",
    "|PDT  | predeterminer 'all the kids'          |\n",
    "|POS  | possessive ending parent              |\n",
    "|PRP  | personal pronoun I, he, she           |\n",
    "|PRP\\$ | possessive pronoun my, his, hers      |\n",
    "|RB   | adverb very, silently,                |\n",
    "|RBR  | adverb, comparative better            |\n",
    "|RBS  | adverb, superlative best              |\n",
    "|RP   | particle give up                      |\n",
    "|TO,  | to go 'to' the store.                 |\n",
    "|UH   | interjection, errrrrrrrm              |\n",
    "|VB   | verb, base form take                  |\n",
    "|VBD  | verb, past tense took                 |\n",
    "|VBG  | verb, gerund/present participle taking|\n",
    "|VBN  | verb, past participle taken           |\n",
    "|VBP  | verb, sing. present     |\n",
    "|VBZ  | verb, third person sing. present takes  |\n",
    "|WDT  | wh-determiner which                   |\n",
    "|WP   | wh-pronoun who, what                  |\n",
    "|WP\\$  | possessive wh-pronoun whose           |\n",
    "|WRB  | wh-abverb where, when                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3d371",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "This step transforms a word into its root word or base form. For example, car, cars, car's all share a common root word `car`. In the linguistic field, such words are known as words with inflection endings or derivationally related words. There is a [nice post](https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx) which you can refer to understand more about it and also about morphological analysis.\n",
    "\n",
    "Here we are briefly discussing inflection and derivational forms of words.\n",
    "\n",
    "**Inflection forms** \n",
    "These word forms are used to distinct tenses, person, gender, etc. For example, words like go, going, gone, goes. If you notice these words have different endings. These all are called inflection endings in [**linguistic**](https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx) field. \n",
    "\n",
    ":::{.callout-tip}\n",
    "The words with inflection endings do not have a separate entry in the dictionary. You will find all words with inflection endings under a single entry i.e., go.\n",
    ":::\n",
    "\n",
    "**Derivational form**\n",
    "\n",
    "These word forms are derived from the rood words and create a new meaning. For example, re`act` and `act`or  both are derived from the word `act`.\n",
    "\n",
    ":::{.callout-tip}\n",
    "The words with derivational forms have a separate entry in the dictionary. You will find a separate entry in the dictionary for act, react, and actor.\n",
    ":::\n",
    "\n",
    "Now, as we have a preliminary understanding of different forms of words, we next move to the **stemming and lemmatization**. These are two techniques to transform a word from its inflection form (and sometimes derivational form) to the base form.\n",
    "\n",
    "\n",
    "### Stemming\n",
    "**Stemming** is the technique which simply chops off the ending of a word to obtain its base form. For example, removing `ing` from `eating` to obain the base form i.e., `eat`. By default NLTK uses a rule-based stemmer (i.e., Porter Stemmer). There are other stemmer as well. You can check [this page](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) for more information on different stemmers.\n",
    "\n",
    "Let's take a look at the following code which perform stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8d21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:Estonia    Stem:estonia\n",
      "Word:is         Stem:is\n",
      "Word:a          Stem:a\n",
      "Word:leading    Stem:lead\n",
      "Word:country    Stem:countri\n",
      "Word:in         Stem:in\n",
      "Word:digital    Stem:digit\n",
      "Word:space      Stem:space\n",
      "Word:,          Stem:,\n",
      "Word:and        Stem:and\n",
      "Word:on         Stem:on\n",
      "Word:its        Stem:it\n",
      "Word:way        Stem:way\n",
      "Word:to         Stem:to\n",
      "Word:become     Stem:becom\n",
      "Word:the        Stem:the\n",
      "Word:leader     Stem:leader\n",
      "Word:.          Stem:.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Uncomment the following statement if running it first time\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize the text data\n",
    "words = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n",
    "\n",
    "# Printing the data with its base form after stemming operation\n",
    "for word in words:\n",
    "    print('Word:{:10} Stem:{}'.format(word,ps.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3698bff",
   "metadata": {},
   "source": [
    "In the output, we can see that words `leading`, `country`, `digital` are transformed into `lead`, `countri`, and `digit`, respectively. Now, we will move to Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631378b",
   "metadata": {},
   "source": [
    "### Lemmatization \n",
    "Lemmatization is an another technique which also perform a similar task as Stemming i.e., transoforming words into its base forms. However, it is differs in its approach. Lemmatization uses [morphological analysis](https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx) to achieve the goal. The morphological analysis means understanding of words and their parts. \n",
    "\n",
    "You can refer to this post to gain more information on different [lemmatization approaches](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/?utm_content=cmp-true) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd15c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:Estonia    Lemma:Estonia\n",
      "Word:is         Lemma:is\n",
      "Word:a          Lemma:a\n",
      "Word:leading    Lemma:leading\n",
      "Word:country    Lemma:country\n",
      "Word:in         Lemma:in\n",
      "Word:digital    Lemma:digital\n",
      "Word:space      Lemma:space\n",
      "Word:,          Lemma:,\n",
      "Word:and        Lemma:and\n",
      "Word:on         Lemma:on\n",
      "Word:its        Lemma:it\n",
      "Word:way        Lemma:way\n",
      "Word:to         Lemma:to\n",
      "Word:become     Lemma:become\n",
      "Word:the        Lemma:the\n",
      "Word:leader     Lemma:leader\n",
      "Word:.          Lemma:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/htk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n",
    "\n",
    "for word in words:\n",
    "    print('Word:{:10} Lemma:{} Tag:{}'.format(word,wordnet_lemmatizer.lemmatize(word),wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a4efe",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "# No change after lemmatization\n",
    "If you notice in the results there are no changes after applying lemmatization. The reason is that\n",
    "if the word can not be found in WordNet (publicly awailable English dataset) then the word remain unchanged. It can be corrected by providing the pos tag of the word when calling `lemmatize` function.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbec696",
   "metadata": {},
   "source": [
    "Now, we will supply pos tag of each word when calling `lemmatize` function. However, the function only takes a single character for the pos tag. For example, `n` for nouns, `v` for verbs, `a` for adjectives, and `r` for adverbs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7339ae",
   "metadata": {},
   "source": [
    "So, we need to prepare a mapping which translates pos tags, obtained from `nltk.pos_tag()` function, into `a`, `r`, `n`, `v` (depending on the tag).\n",
    "\n",
    "We know from the POS tags table above that tags for adjectives starts from 'J'. So what we can do is, we can take the first character of pos-tag and determine which tag to supply in lemmatize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35c9555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def get_pos(word):\n",
    "    # the function returns a list with one tagged tuple, e.g., [('riding','VBD')]\n",
    "    tagged_tuple_list = pos_tag([word])\n",
    "    \n",
    "    # fetching the item in the list and then the tag in tuple\n",
    "    tagged_tuple = tagged_tuple_list[0]  # the first index will fetch the fist itme in the list\n",
    "    \n",
    "    # fetching the tag from the tagged tuple\n",
    "    tag = tagged_tuple[1]   # the index will fetch the tag (e.g., 'VBD')\n",
    "    \n",
    "    # extracting the first character\n",
    "    tag_char = tag[0]\n",
    "    \n",
    "    # all these three statement can be combined into a single statement given below\n",
    "    # tag_char = pos_tag([word])[0][1][0]\n",
    "    \n",
    "    # Now we will create a mapping\n",
    "    pos_to_lemma_tag = {\n",
    "        'J': 'a',\n",
    "        'N': 'n',\n",
    "        'R': 'r',\n",
    "        'V': 'v'\n",
    "    }\n",
    "    \n",
    "    # we will return the tag for usage in lemmatize function\n",
    "    return pos_to_lemma_tag.get(tag_char,'n')   # get function will return n if the tag is something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d247a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:Estonia    Lemma:Estonia\n",
      "Word:is         Lemma:be\n",
      "Word:a          Lemma:a\n",
      "Word:leading    Lemma:lead\n",
      "Word:country    Lemma:country\n",
      "Word:in         Lemma:in\n",
      "Word:digital    Lemma:digital\n",
      "Word:space      Lemma:space\n",
      "Word:,          Lemma:,\n",
      "Word:and        Lemma:and\n",
      "Word:on         Lemma:on\n",
      "Word:its        Lemma:it\n",
      "Word:way        Lemma:way\n",
      "Word:to         Lemma:to\n",
      "Word:become     Lemma:become\n",
      "Word:the        Lemma:the\n",
      "Word:leader     Lemma:leader\n",
      "Word:.          Lemma:.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize('Estonia is a leading country in digital space, and on its way to become the leader.')\n",
    "\n",
    "for word in words:\n",
    "    print('Word:{:10} Lemma:{}'.format(word,\n",
    "                                       wordnet_lemmatizer.lemmatize(word,get_pos(word))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cfdb0",
   "metadata": {},
   "source": [
    "> It worked now like a charm :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92478ad",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Now, we will move to the final step in the aforementioned list of steps in natural language processing. This step translates the text into numbers so that computers can use them for further anlaysis. This is known as vectorization. \n",
    "\n",
    "There are multiple techinques of vectorization. In this post, however, we are going to discuss three main of those techniques: count vectorization, tf-idf, and word embeddings.\n",
    "\n",
    "Let's say we have a piece of text which we like to classify into some categories. These categories are positive and negative. Our first question would be how we are going to transform the text into numbers so that it can be used with machine learning algorithms.\n",
    "\n",
    "One way could be counting of words' frequency and using it as a feature to the algorithm.\n",
    "Another way, which we are going to discuss now, is to use one-hot encoding. This encoding transform each word into a vector. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "References\n",
    "1. https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n",
    "2. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "3. https://www.nltk.org/book/\n",
    "4. Morphological analysis: https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/litfocuswordmorph.aspx\n",
    "5. https://www.datacamp.com/tutorial/stemming-lemmatization-python"
   ]
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aea85",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.16"
=======
   "version": "3.11.5"
>>>>>>> 972e74c67588d0222ab83f334a4c1386fa26f572
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
